{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slsabilaAura/Tugas-Akhir/blob/main/Preprocessing_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VreatOn5R-2D",
        "outputId": "26e537c3-94f9-4eb8-9e0f-e7008d7caf93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting indoNLP\n",
            "  Downloading indoNLP-0.3.4-py3-none-any.whl.metadata (3.4 kB)\n",
            "Downloading indoNLP-0.3.4-py3-none-any.whl (121 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/121.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m112.6/121.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: indoNLP\n",
            "Successfully installed indoNLP-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install indoNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QREF1mQTSA8K",
        "outputId": "15a83fd9-2225-4773-8734-fb0211a84879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/209.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xTMZZLE4gRv",
        "outputId": "28cd1e1a-534b-4f27-d2f4-7cf4456c4c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75yCniaTKtOp",
        "outputId": "442ac7d0-a80d-4b6f-ea0b-a1edc0d84a4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import string\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from indoNLP.preprocessing import remove_stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACrd1eSDR3rA",
        "outputId": "47c83afb-1d72-4586-a0b3-af1ec395de5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqbPt4ZWT6wR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "file_path_expert = \"/content/drive/MyDrive/Skripsi2/Dataset/expert.csv\"\n",
        "expert_df = pd.read_csv(file_path_expert)\n",
        "file_path_proposal = \"/content/drive/MyDrive/Skripsi2/Dataset/proposal.csv\"\n",
        "proposal_df = pd.read_csv(file_path_proposal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uu4Irbv04-HC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.stem import PorterStemmer\n",
        "from langdetect import detect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "\n",
        "# Inisialisasi spaCy dan stemmer\n",
        "nlp = spacy.blank(\"id\")\n",
        "factory = StemmerFactory()\n",
        "stemmer_id = factory.create_stemmer()\n",
        "stemmer_en = PorterStemmer()\n",
        "\n",
        "# Stopword gabungan: akademik, huruf, angka, bahasa Inggris\n",
        "custom_stopwords = set([\n",
        "    # Akademik\n",
        "    \"penelitian\", \"data\", \"model\", \"hasil\", \"sistem\", \"metode\", \"menggunakan\",\n",
        "    \"dapat\", \"dalam\", \"dengan\", \"tujuan\", \"analisis\", \"penggunaan\", \"studi\",\n",
        "    \"proses\", \"berdasarkan\", \"et\", \"al\", \"digunakan\", \"nilai\", \"informasi\",\n",
        "    \"menunjukkan\", \"hingga\", \"oleh\", \"terhadap\", \"memiliki\", \"algoritma\", \"akurasi\", \"salah\",\n",
        "    \"adalah\", \"di\", \"ke\", \"dari\", \"yaitu\", \"atau\", \"lalu\", \"kemudian\", \"dkk\", \"akan\", \"serta\", \"ini\", \"itu\",\n",
        "    \"seperti\", \"telah\", \"sama\", \"dan\", \"yang\",\"dapat\",\"tentu\",\"mana\",\"banyak\",\"ada\",\"buat\",\"oleh\",\"tiap\",\"setiap\",\n",
        "    \"lain\",\"selain\",\"amtara\",\"suatu\",\"contoh\",\"beberapa\",\"sehingga\",\"lalu\",\"juga\",\"pilih\",\"pilihan\",\"karena\",\"tahu\",\"diketahui\",\n",
        "\n",
        "    # Huruf a-z dan angka 0-9\n",
        "    *list(\"abcdefghijklmnopqrstuvwxyz\"),\n",
        "    *list(\"0123456789\"),\n",
        "\n",
        "    # Bahasa Inggris umum\n",
        "    \"the\", \"is\", \"a\", \"an\", \"and\", \"or\", \"of\", \"to\", \"in\", \"for\", \"on\", \"with\",\n",
        "    \"this\", \"that\", \"by\", \"at\", \"from\", \"it\", \"as\", \"are\", \"be\", \"was\", \"were\",\n",
        "    \"has\", \"have\", \"had\", \"not\", \"but\", \"we\", \"they\", \"their\", \"our\", \"its\"\n",
        "])\n",
        "\n",
        "# Fungsi untuk deteksi bahasa per kata\n",
        "def detect_language_word(word):\n",
        "    try:\n",
        "        return detect(word)\n",
        "    except LangDetectException:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Fungsi cleaning text\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\((?:[A-Z][a-z]+(?:\\s+(?:dan|and|&)\\s+[A-Z][a-z]+)?)(?:,\\s*\\d{4}[a-z]?)?\\)\", \"\", text)\n",
        "    text = re.sub(r\"\\b[A-Z][a-z]+(?:\\s+(?:and|dan|&)\\s+[A-Z][a-z]+)?\\s*,?\\s*\\d{4}[a-z]?\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b[A-Z][a-z]+(?:\\s+et\\.?\\s*al\\.?|dkk)\\s*,?\\s*\\d{4}[a-z]?\", \"\", text)\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    text = re.sub(r\"\\b(\\w+)-\\1\\b\", r\"\\1 \\1\", text)\n",
        "    text = re.sub(r\"\\b(dasar|landasan)\\s+teori\\b\", \"\", text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "# Preprocessing langsung: cleaning → tokenizing → stopword removal → stemming (ID & EN)\n",
        "def preprocess_text_detail(text):\n",
        "    original = text\n",
        "    tokens = [token.text.lower() for token in nlp(original)]\n",
        "    tokens_no_stopwords = [t for t in tokens if t not in custom_stopwords and t.strip() != \"\"]\n",
        "\n",
        "    stemmed = []\n",
        "    for token in tokens_no_stopwords:\n",
        "        lang = detect_language_word(token)\n",
        "        if lang == 'en':\n",
        "            stemmed_token = stemmer_en.stem(token)\n",
        "        elif lang == 'id':\n",
        "            stemmed_token = stemmer_id.stem(token)\n",
        "        else:\n",
        "            stemmed_token = stemmer_id.stem(token)  # default ID\n",
        "        stemmed.append(stemmed_token)\n",
        "\n",
        "    return {\n",
        "        \"original_text\": original,\n",
        "        \"tokens\": tokens,\n",
        "        \"no_stopwords\": tokens_no_stopwords,\n",
        "        \"stemmed\": stemmed,\n",
        "        \"final_text\": \" \".join(stemmed)\n",
        "    }\n",
        "\n",
        "# Pipeline preprocessing lengkap\n",
        "def preprocess_pipeline(file_path_proposal, file_path_expert):\n",
        "    proposal_df = pd.read_csv(file_path_proposal)\n",
        "    expert_df = pd.read_csv(file_path_expert)\n",
        "\n",
        "    proposal_df[\"full_text\"] = proposal_df[\"proposal_introduction\"].astype(str) + \" \" + proposal_df[\"proposal_theory\"].astype(str)\n",
        "    expert_df[\"full_text\"] = expert_df[\"research_abstract\"].astype(str)\n",
        "\n",
        "    print(\"🚀 Preprocessing proposal...\")\n",
        "    proposal_results = proposal_df[\"full_text\"].apply(preprocess_text_detail)\n",
        "    proposal_df[\"original_text\"] = proposal_results.apply(lambda x: x[\"original_text\"])\n",
        "    proposal_df[\"tokens\"] = proposal_results.apply(lambda x: x[\"tokens\"])\n",
        "    proposal_df[\"no_stopwords\"] = proposal_results.apply(lambda x: x[\"no_stopwords\"])\n",
        "    proposal_df[\"stemmed\"] = proposal_results.apply(lambda x: x[\"stemmed\"])\n",
        "    proposal_df[\"processed_text\"] = proposal_results.apply(lambda x: x[\"final_text\"])\n",
        "\n",
        "    print(\"🚀 Preprocessing publikasi dosen...\")\n",
        "    expert_results = expert_df[\"full_text\"].apply(preprocess_text_detail)\n",
        "    expert_df[\"original_text\"] = expert_results.apply(lambda x: x[\"original_text\"])\n",
        "    expert_df[\"tokens\"] = expert_results.apply(lambda x: x[\"tokens\"])\n",
        "    expert_df[\"no_stopwords\"] = expert_results.apply(lambda x: x[\"no_stopwords\"])\n",
        "    expert_df[\"stemmed\"] = expert_results.apply(lambda x: x[\"stemmed\"])\n",
        "    expert_df[\"processed_text\"] = expert_results.apply(lambda x: x[\"final_text\"])\n",
        "\n",
        "    print(\"✅ Preprocessing selesai!\")\n",
        "    return proposal_df, expert_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmDRzkfnZhJ1",
        "outputId": "2a6402ed-c40d-4100-b13a-1a436a8fa945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Preprocessing proposal...\n",
            "🚀 Preprocessing publikasi dosen...\n",
            "✅ Preprocessing selesai!\n"
          ]
        }
      ],
      "source": [
        "proposal_df, expert_df = preprocess_pipeline(file_path_proposal, file_path_expert)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk7-FxVfMb8T",
        "outputId": "783d3f37-5664-4314-cc7d-0b7b5e90e76c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    proposal_id student_id                                     proposal_title  \\\n",
            "0            P1         S1  Analisa dan Implementasi Steganografi dengan M...   \n",
            "1           P10        S10  Clustering Penyalahguna Narkotika Menggunakan ...   \n",
            "2           P11        S11  CNN Berbasis Blok Xception Menggunakan Fitur K...   \n",
            "3           P12        S12  COLOR-EMBEDDED-GRAYSCALE IMAGE MENGGUNAKAN RES...   \n",
            "4           P13        S13  COMPREHENSIVE MULTIPLE SECRET SHARING MENGGUNA...   \n",
            "..          ...        ...                                                ...   \n",
            "137        P138       S138  Pengaruh Double Learning Logistic Regression p...   \n",
            "138        P139       S139  Residual Model pada Detail Retaining Convoluti...   \n",
            "139        P140       S140  Analisa Sentimen Komentar terhadap Kebijakan P...   \n",
            "140        P141       S141  Klasifikasi Relevansi Trending Topic Dengan Ko...   \n",
            "141        P142       S142  Analisa Perbandingan Pemodelan TF-IDF dan WORD...   \n",
            "\n",
            "                                     proposal_abstract  \\\n",
            "0    seiring perkembangan internet kebutuhan akan k...   \n",
            "1    narkotika adalah zat atau obat yang dapat meni...   \n",
            "2    penelitian ini menguji performa berbagai modul...   \n",
            "3    citra grayscale yang direstorasi menjadi citra...   \n",
            "4    multiple secret sharing mss sering digunakan u...   \n",
            "..                                                 ...   \n",
            "137  data biomedis sangat rentan terhadap noisy mis...   \n",
            "138  llposed inverse imaging merupakan sebuah perma...   \n",
            "139  kebijakan pemerintah indonesia untuk penangana...   \n",
            "140  twitter adalah media sosial yang sangat berpen...   \n",
            "141  peranan angkutan umum menjadi salah satu aspek...   \n",
            "\n",
            "                                 proposal_introduction  \\\n",
            "0    kriptografi dna adalah sebuah cabang kriptogra...   \n",
            "1    di era globalisasi sekarang ini kemajuan dalam...   \n",
            "2    rekognisi emosiemotion recognition merupakan t...   \n",
            "3    citra berwarna yang diubah menjadi grayscale d...   \n",
            "4    pada era sekarang konten multimedia sering kal...   \n",
            "..                                                 ...   \n",
            "137  data mining adalah proses menemukan sebuah pol...   \n",
            "138  illposed merupakan sebuah permasalahan yang ti...   \n",
            "139  virus covid telah menyebar keseluruh dunia mul...   \n",
            "140  twitter menjadi sumber informasi yang utama da...   \n",
            "141  jakarta adalah salah satu kota dengan tingkat ...   \n",
            "\n",
            "                                       proposal_theory  proposal_year  \\\n",
            "0    kriptografi kriptografi adalah studi tentang t...           2021   \n",
            "1    data mining data mining adalah proses yang men...           2021   \n",
            "2    emotion recognition emotion recognition merupa...           2023   \n",
            "3    discrete wavelet transform discrete wavelet tr...           2023   \n",
            "4    digital image digital image merupakan citra ya...           2023   \n",
            "..                                                 ...            ...   \n",
            "137  data mining data mining adalah suatu proses ya...           2021   \n",
            "138  iiiposed problem illposed problems secara sede...           2022   \n",
            "139  pandemi covid pandemi covid mulai muncul dari ...           2021   \n",
            "140  trending topic twitter twitter adalah media so...           2021   \n",
            "141  yang digunakan sebagai acuan penelitian ini ad...           2021   \n",
            "\n",
            "                                             full_text  \\\n",
            "0    kriptografi dna adalah sebuah cabang kriptogra...   \n",
            "1    di era globalisasi sekarang ini kemajuan dalam...   \n",
            "2    rekognisi emosiemotion recognition merupakan t...   \n",
            "3    citra berwarna yang diubah menjadi grayscale d...   \n",
            "4    pada era sekarang konten multimedia sering kal...   \n",
            "..                                                 ...   \n",
            "137  data mining adalah proses menemukan sebuah pol...   \n",
            "138  illposed merupakan sebuah permasalahan yang ti...   \n",
            "139  virus covid telah menyebar keseluruh dunia mul...   \n",
            "140  twitter menjadi sumber informasi yang utama da...   \n",
            "141  jakarta adalah salah satu kota dengan tingkat ...   \n",
            "\n",
            "                                         original_text  \\\n",
            "0    kriptografi dna adalah sebuah cabang kriptogra...   \n",
            "1    di era globalisasi sekarang ini kemajuan dalam...   \n",
            "2    rekognisi emosiemotion recognition merupakan t...   \n",
            "3    citra berwarna yang diubah menjadi grayscale d...   \n",
            "4    pada era sekarang konten multimedia sering kal...   \n",
            "..                                                 ...   \n",
            "137  data mining adalah proses menemukan sebuah pol...   \n",
            "138  illposed merupakan sebuah permasalahan yang ti...   \n",
            "139  virus covid telah menyebar keseluruh dunia mul...   \n",
            "140  twitter menjadi sumber informasi yang utama da...   \n",
            "141  jakarta adalah salah satu kota dengan tingkat ...   \n",
            "\n",
            "                                                tokens  \\\n",
            "0    [kriptografi, dna, adalah, sebuah, cabang, kri...   \n",
            "1    [di, era, globalisasi, sekarang, ini, kemajuan...   \n",
            "2    [rekognisi, emosiemotion, recognition, merupak...   \n",
            "3    [citra, berwarna, yang, diubah, menjadi, grays...   \n",
            "4    [pada, era, sekarang, konten, multimedia, seri...   \n",
            "..                                                 ...   \n",
            "137  [data, mining, adalah, proses, menemukan, sebu...   \n",
            "138  [illposed, merupakan, sebuah, permasalahan, ya...   \n",
            "139  [virus, covid, telah, menyebar, keseluruh, dun...   \n",
            "140  [twitter, menjadi, sumber, informasi, yang, ut...   \n",
            "141  [jakarta, adalah, salah, satu, kota, dengan, t...   \n",
            "\n",
            "                                          no_stopwords  \\\n",
            "0    [kriptografi, dna, adalah, sebuah, cabang, kri...   \n",
            "1    [di, era, globalisasi, sekarang, ini, kemajuan...   \n",
            "2    [rekognisi, emosiemotion, recognition, merupak...   \n",
            "3    [citra, berwarna, yang, diubah, menjadi, grays...   \n",
            "4    [pada, era, sekarang, konten, multimedia, seri...   \n",
            "..                                                 ...   \n",
            "137  [mining, adalah, menemukan, sebuah, pola, dan,...   \n",
            "138  [illposed, merupakan, sebuah, permasalahan, ya...   \n",
            "139  [virus, covid, telah, menyebar, keseluruh, dun...   \n",
            "140  [twitter, menjadi, sumber, yang, utama, bebera...   \n",
            "141  [jakarta, adalah, satu, kota, tingkat, kemacet...   \n",
            "\n",
            "                                               stemmed  \\\n",
            "0    [kriptografi, dna, adalah, buah, cabang, kript...   \n",
            "1    [di, era, globalisasi, sekarang, ini, maju, se...   \n",
            "2    [rekognisi, emosiemotion, recognition, rupa, t...   \n",
            "3    [citra, warna, yang, ubah, jadi, grayscale, ru...   \n",
            "4    [pada, era, sekarang, konten, multimedia, seri...   \n",
            "..                                                 ...   \n",
            "137  [mining, adalah, temu, buah, pola, dan, tahu, ...   \n",
            "138  [illposed, rupa, buah, masalah, yang, tidak, s...   \n",
            "139  [virus, covid, telah, sebar, seluruh, dunia, m...   \n",
            "140  [twitter, jadi, sumber, yang, utama, beberapa,...   \n",
            "141  [jakarta, adalah, satu, kota, tingkat, macet, ...   \n",
            "\n",
            "                                        processed_text  \n",
            "0    kriptografi dna adalah buah cabang kriptografi...  \n",
            "1    di era globalisasi sekarang ini maju segala bi...  \n",
            "2    rekognisi emosiemotion recognition rupa tugas ...  \n",
            "3    citra warna yang ubah jadi grayscale rubah war...  \n",
            "4    pada era sekarang konten multimedia sering kal...  \n",
            "..                                                 ...  \n",
            "137  mining adalah temu buah pola dan tahu tarik da...  \n",
            "138  illposed rupa buah masalah yang tidak solusi y...  \n",
            "139  virus covid telah sebar seluruh dunia mulai da...  \n",
            "140  twitter jadi sumber yang utama beberapa tahun ...  \n",
            "141  jakarta adalah satu kota tingkat macet lalu li...  \n",
            "\n",
            "[142 rows x 13 columns]\n"
          ]
        }
      ],
      "source": [
        "print(proposal_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbI9YInAjvN5",
        "outputId": "9ff22f66-413c-44cb-e9fb-38d43b1ce8b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed dataframes saved to CSV in Google Drive.\n"
          ]
        }
      ],
      "source": [
        "# prompt: how to export to csv\n",
        "\n",
        "# Save the preprocessed DataFrames to new CSV files\n",
        "proposal_df.to_csv('/content/drive/MyDrive/Skripsi2/Dataset/processed_proposalC.csv', index=False)\n",
        "expert_df.to_csv('/content/drive/MyDrive/Skripsi2/Dataset/processed_expertBC.csv', index=False)\n",
        "\n",
        "print(\"Processed dataframes saved to CSV in Google Drive.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRvaSbPMVnkBoATSUHXxHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}