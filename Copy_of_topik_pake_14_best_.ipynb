{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aca851f",
        "outputId": "41afc13d-99a2-4fd0-d1ec-682a652621f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQIdw0eelk43",
        "outputId": "e0307461-1764-48fb-daea-6976b11458d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# prompt: import file form drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0HTbrGd6ePM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Save the preprocessed DataFrames to new CSV files\n",
        "proposal_df = pd.read_csv('/content/drive/MyDrive/Skripsi3/Dataset/processed_proposalC.csv')\n",
        "expert_df = pd.read_csv('/content/drive/MyDrive/Skripsi3/Dataset/processed_expertC.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpxYRgx768uU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import ast\n",
        "from gensim.models import Phrases\n",
        "from gensim.models.phrases import Phraser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brLAWvZi8CQh"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G42J8DDSOq0f"
      },
      "outputs": [],
      "source": [
        "def convert_to_list(text):\n",
        "    try:\n",
        "        return ast.literal_eval(text) if isinstance(text, str) else text\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "\n",
        "proposal_df[\"processed\"] = proposal_df[\"stemmed\"].apply(convert_to_list)\n",
        "expert_df[\"processed\"] = expert_df[\"stemmed\"].apply(convert_to_list)\n",
        "\n",
        "# Gabungkan semua dokumen untuk membuat satu kamus bersama\n",
        "documents_all = proposal_df[\"processed\"].tolist() + expert_df[\"processed\"].tolist()\n",
        "dictionary_all = Dictionary(documents_all)\n",
        "\n",
        "dictionary_all.filter_extremes(no_below=5, no_above=0.5)\n",
        "\n",
        "proposal_corpus = [dictionary_all.doc2bow(doc) for doc in proposal_df[\"processed\"]]\n",
        "expert_corpus = [dictionary_all.doc2bow(doc) for doc in expert_df[\"processed\"]]\n",
        "\n",
        "corpus_all= proposal_corpus + expert_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx-2C63p6Ma_",
        "outputId": "da488af5-0b1e-47de-c8ea-d5a2b66b967b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "902\n",
            "902\n",
            "3406\n"
          ]
        }
      ],
      "source": [
        "print(len(documents_all))\n",
        "print(len(corpus_all))\n",
        "print(len(dictionary_all))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NeYydlZKA0W3"
      },
      "outputs": [],
      "source": [
        "# # Misalnya kita ambil satu dokumen dari corpus_all\n",
        "# doc_bow = corpus_all[0]\n",
        "\n",
        "# # # Tampilkan representasi kata dari dokumen tersebut\n",
        "# for word_id, freq in doc_bow:\n",
        "#     word = dictionary_all[word_id]  # ambil kata dari kamus berdasarkan indeks\n",
        "#     print(f\"('{word}', {freq})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me5xyf5Z8OwR"
      },
      "source": [
        "# Model LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJDKKZUd8Kqz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2080fb29-4ed5-4b35-c757-a16032d7a702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score: 0.43131366590542786\n",
            "Diversity Score: 0.0628\n"
          ]
        }
      ],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "num_topics = 14  # atau optimalisasi via coherence\n",
        "\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus_all,\n",
        "    id2word=dictionary_all,\n",
        "    num_topics=num_topics,\n",
        "    passes=50,\n",
        "    random_state=42,\n",
        "    iterations= 400,\n",
        "    alpha=0.5,\n",
        "    eta=0.01\n",
        "\n",
        ")\n",
        "\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=proposal_df['processed'].tolist() + expert_df['processed'].tolist(), dictionary=dictionary_all, coherence='c_v')\n",
        "coherence_score = coherence_model_lda.get_coherence()\n",
        "print(f\"Coherence Score: {coherence_score}\")\n",
        "\n",
        "# Hitung topic diversity\n",
        "def compute_topic_diversity(lda_model, topn=10):\n",
        "    topic_words = [set([word for word, _ in lda_model.show_topic(topicid, topn=topn)])\n",
        "                   for topicid in range(lda_model.num_topics)]\n",
        "    pairwise_intersections = [\n",
        "        len(t1 & t2) / len(t1 | t2)\n",
        "        for t1, t2 in combinations(topic_words, 2)\n",
        "    ]\n",
        "    avg_jaccard_similarity = sum(pairwise_intersections) / len(pairwise_intersections)\n",
        "    # diversity_score = 1 - avg_jaccard_similarity  # semakin besar, semakin beragam\n",
        "    return round(avg_jaccard_similarity, 4)\n",
        "\n",
        "diversity_score = compute_topic_diversity(lda_model, topn=10)\n",
        "print(f\"Diversity Score: {diversity_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FQqm9YGikDW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "\n",
        "for topic_id in range(num_topics):\n",
        "    top_words = lda_model.show_topic(topic_id, topn=25)\n",
        "    words = [word for word, prob in top_words]\n",
        "    probs = [round(prob, 4) for word, prob in top_words]\n",
        "\n",
        "    data.append({\n",
        "        'Topik': topic_id + 1,\n",
        "        'Kata Kunci': ', '.join(words),\n",
        "        'Probabilitas': ', '.join(map(str, probs))\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Tampilkan hasil\n",
        "# print(df)\n",
        "\n",
        "# Simpan ke CSV jika diinginkan\n",
        "# df.to_csv('/content/drive/MyDrive/Skripsi4/dictionary/topik_kata_dan_probabilitas_14_baru_top25word.csv', index=False, encoding='utf-8-sig')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5k7jvUSj35x"
      },
      "outputs": [],
      "source": [
        "from gensim.matutils import sparse2full\n",
        "\n",
        "# Fungsi untuk mendapatkan dense topic vector\n",
        "def get_topic_vector(lda_model, dictionary, document, num_topics):\n",
        "    bow = dictionary.doc2bow(document)\n",
        "    topic_dist = lda_model.get_document_topics(bow, minimum_probability=0.00001)\n",
        "    return sparse2full(topic_dist, num_topics)\n",
        "\n",
        "# Ambil jumlah topik dari model LDA\n",
        "num_topics = lda_model.num_topics\n",
        "\n",
        "# Hitung topik vektor dalam bentuk dense array (untuk expert)\n",
        "expert_df[\"topic_vector\"] = [\n",
        "    get_topic_vector(lda_model, dictionary_all, doc, num_topics)\n",
        "    for doc in expert_df[\"processed\"]\n",
        "]\n",
        "\n",
        "# Hitung topik vektor dalam bentuk dense array (untuk proposal)\n",
        "proposal_df[\"topic_vector\"] = [\n",
        "    get_topic_vector(lda_model, dictionary_all, doc, num_topics)\n",
        "    for doc in proposal_df[\"processed\"]\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WOc8HgLHAzGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "113987bc-1117-4dde-db82-7985c75de26a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
            "research_id                                                               \n",
            "R1           0.005264  0.044919  0.005749  0.005661  0.159443  0.048705   \n",
            "R1           0.005264  0.044919  0.005749  0.005661  0.159443  0.048705   \n",
            "R2           0.006275  0.006679  0.344286  0.005781  0.006000  0.274839   \n",
            "R3           0.004788  0.004560  0.382559  0.004312  0.010355  0.004713   \n",
            "R3           0.004788  0.004560  0.382559  0.004312  0.010355  0.004713   \n",
            "\n",
            "              topic_7   topic_8   topic_9  topic_10  topic_11  topic_12  \\\n",
            "research_id                                                               \n",
            "R1           0.376329  0.005356  0.005144  0.297490  0.005534  0.005274   \n",
            "R1           0.376329  0.005356  0.005144  0.297490  0.005534  0.005274   \n",
            "R2           0.041040  0.134105  0.006468  0.006391  0.007006  0.006133   \n",
            "R3           0.546399  0.004910  0.005227  0.005206  0.005002  0.011567   \n",
            "R3           0.546399  0.004910  0.005227  0.005206  0.005002  0.011567   \n",
            "\n",
            "             topic_13  topic_14             name expert_id  \n",
            "research_id                                                 \n",
            "R1           0.005611  0.029520          Wiharto       D12  \n",
            "R1           0.005611  0.029520       Abdul Aziz        D5  \n",
            "R2           0.006814  0.148182       Abdul Aziz        D5  \n",
            "R3           0.005654  0.004749       Abdul Aziz        D5  \n",
            "R3           0.005654  0.004749  Bambang Harjito        D1  \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "topic_matrix_expert = np.vstack(expert_df[\"topic_vector\"])\n",
        "topic_df = pd.DataFrame(\n",
        "    topic_matrix_expert,\n",
        "    columns=[f\"topic_{i+1}\" for i in range(lda_model.num_topics)]\n",
        ")\n",
        "\n",
        "topic_df[\"research_id\"] = expert_df[\"research_id\"].values\n",
        "topic_df[\"name\"] = expert_df[\"name\"].values\n",
        "topic_df[\"expert_id\"] = expert_df[\"expert_id\"].values\n",
        "\n",
        "# Set kolom id_dosen sebagai index\n",
        "topic_df.set_index(\"research_id\", inplace=True)\n",
        "# topic_df = topic_df[~topic_df.index.duplicated(keep=\"first\")]\n",
        "\n",
        "# (Opsional) Tampilkan 5 baris pertama untuk verifikasi\n",
        "print(topic_df.head())\n",
        "\n",
        "# print(topic_df)\n",
        "# topic_df.to_csv('/content/drive/MyDrive/Skripsi4/dictionary/vektorExpert_14_baru.csv', index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat dataframe dari vektor topik dosen\n",
        "topic_matrix_expert = np.vstack(expert_df[\"topic_vector\"])\n",
        "topic_df = pd.DataFrame(topic_matrix_expert, columns=[f\"topic_{i+1}\" for i in range(topic_matrix_expert.shape[1])])\n",
        "\n",
        "# Gabungkan dengan nama dosen\n",
        "topic_df[\"name\"] = expert_df[\"name\"].values\n",
        "topic_df[\"research_id\"] = expert_df[\"research_id\"].values\n",
        "topic_df[\"expert_id\"] = expert_df[\"expert_id\"].values\n",
        "topic_df[\"author_position\"] = expert_df[\"author_position\"].values\n",
        "topic_df[\"author_weight\"] = expert_df[\"author_weight\"].values\n",
        "topic_df[\"pub_year\"] = expert_df[\"pub_year\"].values\n",
        "\n",
        "# Tambahkan kolom topik dominan\n",
        "topic_df[\"topik_dominan\"] = topic_df[[f\"topic_{i+1}\" for i in range(topic_matrix_expert.shape[1])]].idxmax(axis=1)\n",
        "topic_df[\"nilai_topik_dominan\"] = topic_df[[f\"topic_{i+1}\" for i in range(topic_matrix_expert.shape[1])]].max(axis=1)\n",
        "\n",
        "# Tampilkan contoh\n",
        "print(topic_df[[\"research_id\",\"pub_year\", \"expert_id\",\"name\",\"author_position\", \"author_weight\", \"topik_dominan\", \"nilai_topik_dominan\"]].head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmiy9Qyn-iPe",
        "outputId": "84544ffc-9670-4a90-b298-e1e27d85ba07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  research_id  pub_year expert_id             name  author_position  \\\n",
            "0          R1      2015       D12          Wiharto                1   \n",
            "1          R1      2015        D5       Abdul Aziz                2   \n",
            "2          R2      2015        D5       Abdul Aziz                1   \n",
            "3          R3      2016        D5       Abdul Aziz                1   \n",
            "4          R3      2016        D1  Bambang Harjito                2   \n",
            "5          R4      2017        D5       Abdul Aziz                1   \n",
            "6          R4      2017        D8     Esti Suryani                2   \n",
            "7          R5      2018        D5       Abdul Aziz                1   \n",
            "8          R6      2018        D5       Abdul Aziz                1   \n",
            "9          R7      2018        D2          Wiranto                1   \n",
            "\n",
            "   author_weight topik_dominan  nilai_topik_dominan  \n",
            "0            0.6       topic_7             0.376329  \n",
            "1            0.4       topic_7             0.376329  \n",
            "2            1.0       topic_3             0.344286  \n",
            "3            0.6       topic_7             0.546399  \n",
            "4            0.4       topic_7             0.546399  \n",
            "5            0.6       topic_8             0.664380  \n",
            "6            0.4       topic_8             0.664380  \n",
            "7            1.0       topic_3             0.844152  \n",
            "8            1.0       topic_8             0.302608  \n",
            "9            0.6      topic_11             0.409660  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topik_label = {\n",
        "    \"topic_1\":  \"Pengolahan Citra dan Transformasi Digital\",\n",
        "    \"topic_2\":  \"Clustering Citra dan Ekstraksi Fitur\",\n",
        "    \"topic_3\":  \"Data Mining untuk E-Commerce dan Rekomendasi\",\n",
        "    \"topic_4\":  \"Deteksi dan Klasifikasi Penyakit dari Citra Medis\",\n",
        "    \"topic_5\":  \"Deep Learning CNN untuk Citra dan Segmentasi\",\n",
        "    \"topic_6\":  \"IoT dan Pemantauan Lingkungan Berbasis Sensor\",\n",
        "    \"topic_7\":  \"Diagnosis Penyakit Menggunakan Machine Learning\",\n",
        "    \"topic_8\":  \"Sistem Pendukung Keputusan dalam Dunia Pendidikan\",\n",
        "    \"topic_9\":  \"Analisis Sentimen dan Teks di Media Sosial\",\n",
        "    \"topic_10\": \"Klasifikasi Data Akademik dengan Metode Machine Learning\",\n",
        "    \"topic_11\": \"Pemrosesan Bahasa Alami dan Representasi Teks\",\n",
        "    \"topic_12\": \"Restorasi Citra dan Denoising dengan Deep Learning\",\n",
        "    \"topic_13\": \"Sistem Rekomendasi dan Klasifikasi Naive Bayes\",\n",
        "    \"topic_14\": \"Kriptografi, Keamanan Data, dan Enkripsi\"\n",
        "}\n",
        "\n",
        "topic_df[\"label_topik_dominan\"] = topic_df[\"topik_dominan\"].map(topik_label)\n",
        "print(topic_df[[\"research_id\",\"pub_year\", \"expert_id\",\"name\",\"author_position\", \"author_weight\", \"topik_dominan\", \"nilai_topik_dominan\"]].head(10))\n",
        "\n",
        "topic_df.to_csv('/content/drive/MyDrive/Skripsi4/dictionary/labeltopik14.csv', index=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MrMYhYt-6ib",
        "outputId": "f8033301-5d24-457a-e2c9-41e477cae7ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  research_id  pub_year expert_id             name  author_position  \\\n",
            "0          R1      2015       D12          Wiharto                1   \n",
            "1          R1      2015        D5       Abdul Aziz                2   \n",
            "2          R2      2015        D5       Abdul Aziz                1   \n",
            "3          R3      2016        D5       Abdul Aziz                1   \n",
            "4          R3      2016        D1  Bambang Harjito                2   \n",
            "5          R4      2017        D5       Abdul Aziz                1   \n",
            "6          R4      2017        D8     Esti Suryani                2   \n",
            "7          R5      2018        D5       Abdul Aziz                1   \n",
            "8          R6      2018        D5       Abdul Aziz                1   \n",
            "9          R7      2018        D2          Wiranto                1   \n",
            "\n",
            "   author_weight topik_dominan  nilai_topik_dominan  \n",
            "0            0.6       topic_7             0.376329  \n",
            "1            0.4       topic_7             0.376329  \n",
            "2            1.0       topic_3             0.344286  \n",
            "3            0.6       topic_7             0.546399  \n",
            "4            0.4       topic_7             0.546399  \n",
            "5            0.6       topic_8             0.664380  \n",
            "6            0.4       topic_8             0.664380  \n",
            "7            1.0       topic_3             0.844152  \n",
            "8            1.0       topic_8             0.302608  \n",
            "9            0.6      topic_11             0.409660  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat list nama kolom topik\n",
        "topic_cols = [f\"topic_{i+1}\" for i in range(topic_matrix_expert.shape[1])]\n",
        "\n",
        "# Ambil top-3 topik per baris\n",
        "top3_topics = topic_df[topic_cols].apply(lambda row: row.sort_values(ascending=False).index[:3].tolist(), axis=1)\n",
        "top3_values = topic_df[topic_cols].apply(lambda row: row.sort_values(ascending=False).values[:3].tolist(), axis=1)\n",
        "\n",
        "# Masukkan ke dataframe\n",
        "topic_df[\"topik_1\"] = top3_topics.str[0]\n",
        "topic_df[\"topik_2\"] = top3_topics.str[1]\n",
        "topic_df[\"topik_3\"] = top3_topics.str[2]\n",
        "\n",
        "topic_df[\"nilai_1\"] = top3_values.str[0]\n",
        "topic_df[\"nilai_2\"] = top3_values.str[1]\n",
        "topic_df[\"nilai_3\"] = top3_values.str[2]\n",
        "\n",
        "# Tambahkan labelnya dari topik_label\n",
        "topic_df[\"label_1\"] = topic_df[\"topik_1\"].map(topik_label)\n",
        "topic_df[\"label_2\"] = topic_df[\"topik_2\"].map(topik_label)\n",
        "topic_df[\"label_3\"] = topic_df[\"topik_3\"].map(topik_label)\n",
        "\n",
        "print(topic_df[[\n",
        "    \"name\", \"expert_id\",\n",
        "    \"topik_1\", \"nilai_1\", \"label_1\",\n",
        "    \"topik_2\", \"nilai_2\", \"label_2\",\n",
        "    \"topik_3\", \"nilai_3\", \"label_3\"\n",
        "]].head(10))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqI8CDAoMS0Y",
        "outputId": "f751255e-feba-46e0-b554-f9e3958477ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              name expert_id   topik_1   nilai_1  \\\n",
            "0          Wiharto       D12   topic_7  0.376329   \n",
            "1       Abdul Aziz        D5   topic_7  0.376329   \n",
            "2       Abdul Aziz        D5   topic_3  0.344286   \n",
            "3       Abdul Aziz        D5   topic_7  0.546399   \n",
            "4  Bambang Harjito        D1   topic_7  0.546399   \n",
            "5       Abdul Aziz        D5   topic_8  0.664380   \n",
            "6     Esti Suryani        D8   topic_8  0.664380   \n",
            "7       Abdul Aziz        D5   topic_3  0.844152   \n",
            "8       Abdul Aziz        D5   topic_8  0.302608   \n",
            "9          Wiranto        D2  topic_11  0.409660   \n",
            "\n",
            "                                             label_1   topik_2   nilai_2  \\\n",
            "0    Diagnosis Penyakit Menggunakan Machine Learning  topic_10  0.297490   \n",
            "1    Diagnosis Penyakit Menggunakan Machine Learning  topic_10  0.297490   \n",
            "2       Data Mining untuk E-Commerce dan Rekomendasi   topic_6  0.274839   \n",
            "3    Diagnosis Penyakit Menggunakan Machine Learning   topic_3  0.382559   \n",
            "4    Diagnosis Penyakit Menggunakan Machine Learning   topic_3  0.382559   \n",
            "5  Sistem Pendukung Keputusan dalam Dunia Pendidikan  topic_14  0.126645   \n",
            "6  Sistem Pendukung Keputusan dalam Dunia Pendidikan  topic_14  0.126645   \n",
            "7       Data Mining untuk E-Commerce dan Rekomendasi   topic_7  0.071210   \n",
            "8  Sistem Pendukung Keputusan dalam Dunia Pendidikan   topic_3  0.246787   \n",
            "9      Pemrosesan Bahasa Alami dan Representasi Teks   topic_3  0.210144   \n",
            "\n",
            "                                             label_2   topik_3   nilai_3  \\\n",
            "0  Klasifikasi Data Akademik dengan Metode Machin...   topic_5  0.159443   \n",
            "1  Klasifikasi Data Akademik dengan Metode Machin...   topic_5  0.159443   \n",
            "2      IoT dan Pemantauan Lingkungan Berbasis Sensor  topic_14  0.148182   \n",
            "3       Data Mining untuk E-Commerce dan Rekomendasi  topic_12  0.011567   \n",
            "4       Data Mining untuk E-Commerce dan Rekomendasi  topic_12  0.011567   \n",
            "5           Kriptografi, Keamanan Data, dan Enkripsi   topic_6  0.085052   \n",
            "6           Kriptografi, Keamanan Data, dan Enkripsi   topic_6  0.085052   \n",
            "7    Diagnosis Penyakit Menggunakan Machine Learning   topic_9  0.026365   \n",
            "8       Data Mining untuk E-Commerce dan Rekomendasi  topic_14  0.238001   \n",
            "9       Data Mining untuk E-Commerce dan Rekomendasi   topic_2  0.190508   \n",
            "\n",
            "                                             label_3  \n",
            "0       Deep Learning CNN untuk Citra dan Segmentasi  \n",
            "1       Deep Learning CNN untuk Citra dan Segmentasi  \n",
            "2           Kriptografi, Keamanan Data, dan Enkripsi  \n",
            "3  Restorasi Citra dan Denoising dengan Deep Lear...  \n",
            "4  Restorasi Citra dan Denoising dengan Deep Lear...  \n",
            "5      IoT dan Pemantauan Lingkungan Berbasis Sensor  \n",
            "6      IoT dan Pemantauan Lingkungan Berbasis Sensor  \n",
            "7         Analisis Sentimen dan Teks di Media Sosial  \n",
            "8           Kriptografi, Keamanan Data, dan Enkripsi  \n",
            "9               Clustering Citra dan Ekstraksi Fitur  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ubah ke format long (1 dosen x 3 topik = 3 baris)\n",
        "top3_long = pd.wide_to_long(\n",
        "    topic_df,\n",
        "    stubnames=[\"topik\", \"nilai\", \"label\"],\n",
        "    i=[\"name\", \"expert_id\", \"research_id\"], # Added research_id here\n",
        "    j=\"rank\",\n",
        "    sep=\"_\",\n",
        "    suffix=\"\\d\"\n",
        ").reset_index()\n",
        "\n",
        "# Urutkan jika perlu\n",
        "top3_long = top3_long.sort_values([\"name\", \"rank\"])\n",
        "print(top3_long.head(10))\n",
        "top3_long.to_csv('/content/drive/MyDrive/Skripsi4/dictionary/labeltopik14baru_rank.csv', index=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cj9X2jTNB5m",
        "outputId": "f4cb9d05-6609-4bd9-e07a-57c19a5680e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          name expert_id research_id  rank  \\\n",
            "3   Abdul Aziz        D5          R1     1   \n",
            "6   Abdul Aziz        D5          R2     1   \n",
            "9   Abdul Aziz        D5          R3     1   \n",
            "15  Abdul Aziz        D5          R4     1   \n",
            "21  Abdul Aziz        D5          R5     1   \n",
            "24  Abdul Aziz        D5          R6     1   \n",
            "30  Abdul Aziz        D5          R7     1   \n",
            "33  Abdul Aziz        D5          R8     1   \n",
            "45  Abdul Aziz        D5          R9     1   \n",
            "48  Abdul Aziz        D5         R10     1   \n",
            "\n",
            "                                  label_topik_dominan  nilai_topik_dominan  \\\n",
            "3     Diagnosis Penyakit Menggunakan Machine Learning             0.376329   \n",
            "6        Data Mining untuk E-Commerce dan Rekomendasi             0.344286   \n",
            "9     Diagnosis Penyakit Menggunakan Machine Learning             0.546399   \n",
            "15  Sistem Pendukung Keputusan dalam Dunia Pendidikan             0.664380   \n",
            "21       Data Mining untuk E-Commerce dan Rekomendasi             0.844152   \n",
            "24  Sistem Pendukung Keputusan dalam Dunia Pendidikan             0.302608   \n",
            "30      Pemrosesan Bahasa Alami dan Representasi Teks             0.409660   \n",
            "33      IoT dan Pemantauan Lingkungan Berbasis Sensor             0.923255   \n",
            "45         Analisis Sentimen dan Teks di Media Sosial             0.428187   \n",
            "48       Data Mining untuk E-Commerce dan Rekomendasi             0.252049   \n",
            "\n",
            "     topic_1  topic_10  topic_11  topic_12  ...   topic_4   topic_5   topic_6  \\\n",
            "3   0.005264  0.297490  0.005534  0.005274  ...  0.005661  0.159443  0.048705   \n",
            "6   0.006275  0.006391  0.007006  0.006133  ...  0.005781  0.006000  0.274839   \n",
            "9   0.004788  0.005206  0.005002  0.011567  ...  0.004312  0.010355  0.004713   \n",
            "15  0.005033  0.004811  0.006146  0.004593  ...  0.004378  0.030542  0.085052   \n",
            "21  0.004482  0.008062  0.004866  0.004160  ...  0.004089  0.004315  0.005256   \n",
            "24  0.002940  0.003269  0.075013  0.010571  ...  0.003056  0.002768  0.100651   \n",
            "30  0.006207  0.009331  0.409660  0.004236  ...  0.004149  0.004189  0.007640   \n",
            "33  0.005515  0.005431  0.005477  0.005236  ...  0.006714  0.005395  0.923255   \n",
            "45  0.006429  0.035345  0.008149  0.006764  ...  0.014879  0.011859  0.039163   \n",
            "48  0.007500  0.006912  0.013072  0.023228  ...  0.225863  0.007157  0.142481   \n",
            "\n",
            "     topic_7   topic_8   topic_9  topik_dominan     topik     nilai  \\\n",
            "3   0.376329  0.005356  0.005144        topic_7   topic_7  0.376329   \n",
            "6   0.041040  0.134105  0.006468        topic_3   topic_3  0.344286   \n",
            "9   0.546399  0.004910  0.005227        topic_7   topic_7  0.546399   \n",
            "15  0.018316  0.664380  0.004556        topic_8   topic_8  0.664380   \n",
            "21  0.071210  0.007624  0.026365        topic_3   topic_3  0.844152   \n",
            "24  0.003155  0.302608  0.003479        topic_8   topic_8  0.302608   \n",
            "30  0.006396  0.126118  0.011591       topic_11  topic_11  0.409660   \n",
            "33  0.007500  0.005629  0.005135        topic_6   topic_6  0.923255   \n",
            "45  0.048340  0.008118  0.428187        topic_9   topic_9  0.428187   \n",
            "48  0.197147  0.007674  0.007009        topic_3   topic_3  0.252049   \n",
            "\n",
            "                                                label  \n",
            "3     Diagnosis Penyakit Menggunakan Machine Learning  \n",
            "6        Data Mining untuk E-Commerce dan Rekomendasi  \n",
            "9     Diagnosis Penyakit Menggunakan Machine Learning  \n",
            "15  Sistem Pendukung Keputusan dalam Dunia Pendidikan  \n",
            "21       Data Mining untuk E-Commerce dan Rekomendasi  \n",
            "24  Sistem Pendukung Keputusan dalam Dunia Pendidikan  \n",
            "30      Pemrosesan Bahasa Alami dan Representasi Teks  \n",
            "33      IoT dan Pemantauan Lingkungan Berbasis Sensor  \n",
            "45         Analisis Sentimen dan Teks di Media Sosial  \n",
            "48       Data Mining untuk E-Commerce dan Rekomendasi  \n",
            "\n",
            "[10 rows x 24 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zhzx4doA_MN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5e0eb7-5647-4a00-e744-17ae2c13879c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
            "proposal_id                                                               \n",
            "P1           0.000580  0.000521  0.000403  0.001298  0.000521  0.000451   \n",
            "P10          0.000347  0.703296  0.089274  0.000321  0.000346  0.001877   \n",
            "P11          0.751460  0.000321  0.005268  0.000265  0.160344  0.000319   \n",
            "P12          0.037477  0.000466  0.000384  0.000409  0.069484  0.002810   \n",
            "P13          0.001201  0.000586  0.000490  0.000477  0.000512  0.000465   \n",
            "\n",
            "              topic_7   topic_8   topic_9  topic_10  topic_11  topic_12  \\\n",
            "proposal_id                                                               \n",
            "P1           0.000380  0.000501  0.000876  0.000609  0.010455  0.000578   \n",
            "P10          0.000502  0.130369  0.000378  0.000491  0.000416  0.000290   \n",
            "P11          0.000353  0.003129  0.000395  0.004616  0.000436  0.072385   \n",
            "P12          0.000346  0.000331  0.000339  0.000364  0.000427  0.886446   \n",
            "P13          0.000497  0.004814  0.000442  0.000476  0.261691  0.465200   \n",
            "\n",
            "             topic_13  topic_14  \n",
            "proposal_id                      \n",
            "P1           0.000542  0.982284  \n",
            "P10          0.071745  0.000349  \n",
            "P11          0.000367  0.000342  \n",
            "P12          0.000395  0.000321  \n",
            "P13          0.000488  0.262661  \n"
          ]
        }
      ],
      "source": [
        "topic_matrix_proposal = np.vstack(proposal_df[\"topic_vector\"])\n",
        "topic_proposal_df = pd.DataFrame(topic_matrix_proposal, columns=[f\"topic_{i+1}\" for i in range(lda_model.num_topics)])\n",
        "topic_proposal_df[\"proposal_id\"] = proposal_df[\"proposal_id\"].values\n",
        "\n",
        "# Set kolom id_dosen sebagai index\n",
        "topic_proposal_df.set_index(\"proposal_id\", inplace=True)\n",
        "print(topic_proposal_df.head())\n",
        "\n",
        "# print(topic_proposal_df)\n",
        "# topic_proposal_df.to_csv('/content/drive/MyDrive/Skripsi4/dictionary/vektorProposal_14_baru.csv', index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat kolom topik dominan dan nilainya\n",
        "topic_proposal_df[\"topik_dominan\"] = topic_proposal_df[[f\"topic_{i+1}\" for i in range(lda_model.num_topics)]].idxmax(axis=1)\n",
        "topic_proposal_df[\"nilai_topik_dominan\"] = topic_proposal_df[[f\"topic_{i+1}\" for i in range(lda_model.num_topics)]].max(axis=1)\n",
        "topic_proposal_df[\"label_topik_dominan\"] = topic_proposal_df[\"topik_dominan\"].map(topik_label)\n",
        "print(topic_proposal_df.head())\n",
        "topic_proposal_df.to_csv('/content/drive/MyDrive/Skripsi4/dictionary/vektorProposal_topikdominan_14_baru.csv', index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_NGyNpCZ2Jg",
        "outputId": "6c83dc7b-4c8e-4df5-8484-7569ede4e1c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
            "proposal_id                                                               \n",
            "P1           0.000580  0.000521  0.000403  0.001298  0.000521  0.000451   \n",
            "P10          0.000347  0.703296  0.089274  0.000321  0.000346  0.001877   \n",
            "P11          0.751460  0.000321  0.005268  0.000265  0.160344  0.000319   \n",
            "P12          0.037477  0.000466  0.000384  0.000409  0.069484  0.002810   \n",
            "P13          0.001201  0.000586  0.000490  0.000477  0.000512  0.000465   \n",
            "\n",
            "              topic_7   topic_8   topic_9  topic_10  topic_11  topic_12  \\\n",
            "proposal_id                                                               \n",
            "P1           0.000380  0.000501  0.000876  0.000609  0.010455  0.000578   \n",
            "P10          0.000502  0.130369  0.000378  0.000491  0.000416  0.000290   \n",
            "P11          0.000353  0.003129  0.000395  0.004616  0.000436  0.072385   \n",
            "P12          0.000346  0.000331  0.000339  0.000364  0.000427  0.886446   \n",
            "P13          0.000497  0.004814  0.000442  0.000476  0.261691  0.465200   \n",
            "\n",
            "             topic_13  topic_14 topik_dominan  nilai_topik_dominan  \\\n",
            "proposal_id                                                          \n",
            "P1           0.000542  0.982284      topic_14             0.982284   \n",
            "P10          0.071745  0.000349       topic_2             0.703296   \n",
            "P11          0.000367  0.000342       topic_1             0.751460   \n",
            "P12          0.000395  0.000321      topic_12             0.886446   \n",
            "P13          0.000488  0.262661      topic_12             0.465200   \n",
            "\n",
            "                                           label_topik_dominan  \n",
            "proposal_id                                                     \n",
            "P1                    Kriptografi, Keamanan Data, dan Enkripsi  \n",
            "P10                       Clustering Citra dan Ekstraksi Fitur  \n",
            "P11                  Pengolahan Citra dan Transformasi Digital  \n",
            "P12          Restorasi Citra dan Denoising dengan Deep Lear...  \n",
            "P13          Restorasi Citra dan Denoising dengan Deep Lear...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kktZnb3TBCsE"
      },
      "source": [
        "# Kemiripan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZFE5rntA7F4"
      },
      "outputs": [],
      "source": [
        "proposal_df[\"tahun\"] = proposal_df[\"proposal_year\"].astype(int)\n",
        "expert_df[\"pub_year\"] = expert_df[\"research_pub_year\"].astype(int)\n",
        "\n",
        "\n",
        "def similarity_m_to_d(proposal_vector, expert_vector):\n",
        "    numerator = np.dot(proposal_vector, expert_vector)\n",
        "    denominator = (np.linalg.norm(proposal_vector) + 1e-10)\n",
        "    return numerator / (denominator)\n",
        "\n",
        "\n",
        "def similarity_d_to_m(expert_vector, proposal_vector):\n",
        "    numerator = np.dot(expert_vector, proposal_vector)\n",
        "    denominator = (np.linalg.norm(expert_vector) + 1e-10)\n",
        "    return numerator / (denominator)\n",
        "\n",
        " # denominator = np.linalg.norm(expert_vector)\n",
        "\n",
        "def time_decay(year_prop, year_ex, t=1, gamma=0.1):\n",
        "    decay = 1 - ((year_prop - year_ex) / t) * gamma\n",
        "    return max(decay, 0.0)  # tidak boleh negatif\n",
        "\n",
        "# mapping Dosen dengan ID Dosen\n",
        "\n",
        "dosen_id_map = pd.read_csv(\"/content/drive/MyDrive/Skripsi3/Dataset/mapping.csv\")  # pastikan kolom: expert_id, expert_name\n",
        "dosen_id_map[\"expert_name\"] = dosen_id_map[\"expert_name\"].str.strip().str.lower()\n",
        "\n",
        "\n",
        "def explode_authors_with_weights(df, dosen_id_map):\n",
        "    rows = []\n",
        "\n",
        "    # Normalisasi nama dosen agar cocok\n",
        "    dosen_id_map = dosen_id_map.copy()\n",
        "    dosen_id_map[\"expert_name\"] = dosen_id_map[\"expert_name\"].str.strip().str.lower()\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        authors = row.get(\"authors\", [])\n",
        "        # Bersihkan nama kosong atau NaN\n",
        "        authors = [a for a in authors if isinstance(a, str) and a.strip() != \"\"]\n",
        "\n",
        "        num_authors = len(authors)\n",
        "        for idx, name in enumerate(authors):\n",
        "            name_clean = name.strip().lower()\n",
        "\n",
        "            if num_authors == 1:\n",
        "                weight = 1.0\n",
        "            else:\n",
        "                weight = 0.6 if idx == 0 else 0.4 / (num_authors - 1)\n",
        "\n",
        "            new_row = row.to_dict()\n",
        "            new_row[\"name\"] = name\n",
        "            new_row[\"author_position\"] = idx + 1\n",
        "            new_row[\"num_authors\"] = num_authors\n",
        "            new_row[\"author_weight\"] = round(weight, 4)\n",
        "\n",
        "            matched = dosen_id_map[dosen_id_map[\"expert_name\"] == name_clean]\n",
        "            new_row[\"expert_id\"] = matched[\"expert_id\"].values[0] if not matched.empty else None\n",
        "\n",
        "            rows.append(new_row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# Gabungkan author_1 sampai author_6 jadi list\n",
        "author_cols = [\"author_1\", \"author_2\", \"author_3\", \"author_4\", \"author_5\", \"author_6\"]\n",
        "expert_df[\"authors\"] = expert_df[author_cols].values.tolist()\n",
        "\n",
        "# Hapus duplikat berdasarkan research_id\n",
        "expert_df = expert_df.drop_duplicates(subset=[\"research_id\"]).copy()\n",
        "\n",
        "# Jalankan explode\n",
        "expert_df = explode_authors_with_weights(expert_df, dosen_id_map)\n",
        "\n",
        "# Opsional: hanya simpan baris dengan expert_id valid\n",
        "expert_df = expert_df[expert_df[\"expert_id\"].notna()]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pAGdpruELTI"
      },
      "source": [
        "## Directed M->D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BwQYxgFDHL6"
      },
      "outputs": [],
      "source": [
        "def compute_od_m_to_d(proposals_df, experts_df):\n",
        "    all_results = []\n",
        "\n",
        "    for _, proposal in proposals_df.iterrows():\n",
        "        proposal_vector = proposal[\"topic_vector\"]\n",
        "        mahasiswa = proposal[\"student_id\"]\n",
        "        id_proposal = proposal[\"proposal_id\"]\n",
        "        tahun_proposal = proposal[\"tahun\"]\n",
        "\n",
        "        for _, expert in expert_df.iterrows():\n",
        "            expert_vector = expert[\"topic_vector\"]\n",
        "            dosen = expert[\"name\"]\n",
        "            id_dosen = expert[\"expert_id\"]\n",
        "            id_penelitian = expert[\"research_id\"]\n",
        "            tahun_penelitian = expert[\"pub_year\"]\n",
        "            weight = expert.get(\"author_weight\")\n",
        "            position= expert[\"author_position\"]\n",
        "\n",
        "            sim_mahasiswa = similarity_m_to_d(proposal_vector, expert_vector)\n",
        "            score = sim_mahasiswa * weight\n",
        "\n",
        "            all_results.append({\n",
        "                \"id_proposal\": id_proposal,\n",
        "                \"id_penelitian\": id_penelitian,\n",
        "                \"mahasiswa\": mahasiswa,\n",
        "                \"dosen\": dosen,\n",
        "                \"id_dosen\": id_dosen,\n",
        "                \"posisi_author\": position,\n",
        "                \"author_weight\":weight,\n",
        "                \"tahun_proposal\": tahun_proposal,\n",
        "                \"tahun_penelitian\": tahun_penelitian,\n",
        "                \"OD(M→D)\": round(score, 4)\n",
        "            })\n",
        "    df_scores = pd.DataFrame(all_results)\n",
        "    # df_scores = df_scores.loc[df_scores.groupby([\"id_proposal\", \"id_dosen\"])[\"OD(M→D)\"].idxmax()].reset_index(drop=True)\n",
        "    df_scores = df_scores.sort_values(by=[\"id_proposal\", \"OD(M→D)\"], ascending=[True, False])\n",
        "    return df_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tc1RIoXLFNaR"
      },
      "outputs": [],
      "source": [
        "od_m2d_df = compute_od_m_to_d(proposal_df, expert_df)\n",
        "# print(od_m2d_df[[\"id_proposal\", \"mahasiswa\",\"id_penelitian\",\"dosen\",\"id_dosen\",\"posisi_author\",\"author_weight\", \"OD(M→D)\"]].head(30))\n",
        "\n",
        "# od_m2d_df.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/14_baru/od_m2d_df_topik.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5FkdQdcHvI2"
      },
      "source": [
        "## TOD M->D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUx5KCOsFSFm"
      },
      "outputs": [],
      "source": [
        "# Hitung TOD(M→D)\n",
        "def compute_tod_m_to_d(od_df, t=1, gamma=0.1, max_year_diff=5):\n",
        "    od_df = od_df.copy()\n",
        "\n",
        "    # Hitung selisih tahun\n",
        "    od_df[\"selisih_tahun\"] = od_df[\"tahun_proposal\"] - od_df[\"tahun_penelitian\"]\n",
        "\n",
        "    # Filter: proposal harus lebih baru dari publikasi, dan selisih maksimal 5 tahun\n",
        "    od_df = od_df[(od_df[\"selisih_tahun\"] >= 0) & (od_df[\"selisih_tahun\"] <= max_year_diff)].copy()\n",
        "\n",
        "    # Hitung time decay factor\n",
        "    od_df[\"time_decay_factor\"] = od_df.apply(\n",
        "        lambda row: time_decay(row[\"tahun_proposal\"], row[\"tahun_penelitian\"], t=t, gamma=gamma),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Hitung TOD(M→D)\n",
        "    od_df[\"TOD(M→D)\"] = (od_df[\"OD(M→D)\"] * od_df[\"time_decay_factor\"]).round(4)\n",
        "\n",
        "    return od_df\n",
        "\n",
        "\n",
        "\n",
        "# # Hitung TOD(M→D) sebelum di sortir\n",
        "# tod_m2d_before_sorted= compute_tod_m_to_d(od_m2d_df, t=1, gamma=0.1)\n",
        "\n",
        "# # Lihat contoh hasil\n",
        "# print(tod_m2d_before_sorted[[\"id_proposal\", \"id_penelitian\",\"dosen\",\"id_dosen\", \"OD(M→D)\", \"tahun_proposal\",\"tahun_penelitian\",\"selisih_tahun\", \"time_decay_factor\", \"TOD(M→D)\"]].head(5))\n",
        "\n",
        "# tod_m2d_before_sorted.to_csv(\"/kaggle/working/tod_m2d_before_sorted.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMgz9n1VHzFg"
      },
      "outputs": [],
      "source": [
        "# Hitung TOD(M→D) Disortir\n",
        "tod_m2d_df= compute_tod_m_to_d(od_m2d_df, t=1, gamma=0.1, max_year_diff=5)\n",
        "\n",
        "tod_m2d_df = (\n",
        "    tod_m2d_df\n",
        "    .loc[tod_m2d_df.groupby([\"id_proposal\", \"id_dosen\"])[\"TOD(M→D)\"].idxmax()]\n",
        "    .reset_index(drop=True)\n",
        "    .sort_values(by=[\"id_proposal\", \"TOD(M→D)\"], ascending=[True, False])\n",
        ")\n",
        "\n",
        "\n",
        "# Lihat contoh hasil\n",
        "\n",
        "# print(tod_m2d_df[[\"id_proposal\",\"mahasiswa\", \"id_penelitian\",\"dosen\",\"id_dosen\", \"OD(M→D)\", \"tahun_proposal\",\"tahun_penelitian\",\"selisih_tahun\", \"time_decay_factor\", \"TOD(M→D)\"]].head(5))\n",
        "\n",
        "# tod_m2d_df.to_csv(\"/kaggle/working/tod_m2d_df.csv\", index=False)\n",
        "# tod_m2d_df.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/14_baru/tod_m2d_df_topik.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GGdS8VRaUtG"
      },
      "outputs": [],
      "source": [
        "# dupes_todm_d = tod_m2d_df[tod_m2d_df.duplicated(subset=[\"id_proposal\",\"dosen\", \"TOD(M→D)\"], keep=False)]\n",
        "# print(dupes_todm_d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXJRYVfCIx5Z"
      },
      "source": [
        "# OD D->M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql8l9N2zYCoS"
      },
      "outputs": [],
      "source": [
        "def compute_od_d_to_m(proposals_df, experts_df):\n",
        "    results = []\n",
        "\n",
        "    for _, expert in experts_df.iterrows():\n",
        "        expert_vector = expert[\"topic_vector\"]\n",
        "        dosen = expert[\"name\"]\n",
        "        id_dosen = expert[\"expert_id\"]\n",
        "        id_penelitian = expert[\"research_id\"]\n",
        "        weight = expert[\"author_weight\"]\n",
        "        position = expert[\"author_position\"]\n",
        "        tahun_penelitian = expert[\"pub_year\"]\n",
        "\n",
        "        for _, proposal in proposals_df.iterrows():\n",
        "            proposal_vector = proposal[\"topic_vector\"]\n",
        "            mahasiswa = proposal[\"student_id\"]\n",
        "            id_proposal = proposal[\"proposal_id\"]\n",
        "            tahun_proposal = proposal[\"tahun\"]\n",
        "\n",
        "            # # Tambahkan filter tahun\n",
        "            selisih_tahun = tahun_proposal - tahun_penelitian\n",
        "            if 0 < selisih_tahun <= 5 and tahun_penelitian <= tahun_proposal:\n",
        "               sim_dosen = similarity_d_to_m(expert_vector, proposal_vector)\n",
        "               score = sim_dosen * weight\n",
        "               results.append({\n",
        "                    \"id_proposal\": id_proposal,\n",
        "                    \"id_penelitian\": id_penelitian,\n",
        "                    \"mahasiswa\": mahasiswa,\n",
        "                    \"dosen\": dosen,\n",
        "                    \"id_dosen\": id_dosen,\n",
        "                    \"posisi_author\": position,\n",
        "                    \"author_weight\": weight,\n",
        "                    \"tahun_proposal\": tahun_proposal,\n",
        "                    \"tahun_penelitian\": tahun_penelitian,\n",
        "                    \"OD(D→M)\": round(score, 4),\n",
        "                })\n",
        "\n",
        "    df_scores = pd.DataFrame(results)\n",
        "    df_scores = df_scores.loc[df_scores.groupby([\"id_proposal\",\"id_dosen\"])[\"OD(D→M)\"].idxmax()].reset_index(drop=True)\n",
        "    df_scores = df_scores.sort_values(by=[\"id_dosen\", \"OD(D→M)\"], ascending=[True, False])\n",
        "\n",
        "    return df_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SJKghECMbCs"
      },
      "outputs": [],
      "source": [
        "#Setelah di Sorted\n",
        "# od_d2m_df = (\n",
        "#     od_d2m_before_sorted\n",
        "#     .sort_values(by=[\"id_proposal\", \"OD(D→M)\"], ascending=[True, False])\n",
        "#     # .drop_duplicates(subset=[\"id_proposal\"], keep=\"first\")  #nnti comment ini\n",
        "#     .reset_index(drop=True)\n",
        "# )\n",
        "\n",
        "# Filter nilai di atas 0.3\n",
        "# od_d2m_df = od_d2m_df.loc[od_d2m_df.groupby([\"id_proposal\", \"id_dosen\"])[\"OD(D→M)\"].idxmax()].reset_index(drop=True).sort_values(by=[\"id_proposal\", \"OD(D→M)\"], ascending=[True, False])\n",
        "\n",
        "od_d2m_df = compute_od_d_to_m(proposal_df, expert_df)\n",
        "# print(od_d2m_df[[\"id_proposal\", \"mahasiswa\",\"id_penelitian\", \"dosen\", \"id_dosen\",\"posisi_author\",\"author_weight\",\"OD(D→M)\"]].head(5))\n",
        "\n",
        "# od_d2m_df.to_csv(\"/kaggle/working/od_d2m_df.csv\", index=False)\n",
        "# od_d2m_df.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/14_baru/od_d2m_df_topik.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLs6cYz8NWTv"
      },
      "source": [
        "# Overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QaOzHpwzMwpf"
      },
      "outputs": [],
      "source": [
        "# def combine_overlap_scores(df_m2d, df_d2m):\n",
        "#     # Ambil Top-N dari masing-masing arah\n",
        "#     top_m2d = df_m2d.groupby(\"id_proposal\")[[\"id_proposal\",\"mahasiswa\", \"id_dosen\", \"dosen\", \"TOD(M→D)\"]]\n",
        "#     top_d2m = df_d2m.groupby(\"id_proposal\")[[\"id_proposal\",\"mahasiswa\", \"id_dosen\", \"dosen\", \"OD(D→M)\"]]\n",
        "\n",
        "#     # Outer join untuk semua pasangan top-n\n",
        "#     merged = pd.merge(top_m2d, top_d2m, on=[\"id_proposal\", \"id_dosen\"], how=\"outer\")\n",
        "\n",
        "#     # Tambahkan kolom nama dosen jika hilang (dari M→D arah saja)\n",
        "#     if \"dosen_x\" in merged.columns:\n",
        "#         merged[\"dosen\"] = merged[\"dosen_x\"].combine_first(merged.get(\"dosen_y\"))\n",
        "#     elif \"dosen\" not in merged.columns:\n",
        "#         merged[\"dosen\"] = None\n",
        "\n",
        "#     if \"mahasiswa_x\" in merged.columns:\n",
        "#         merged[\"mahasiswa\"] = merged[\"mahasiswa_x\"].combine_first(merged.get(\"mahasiswa_y\"))\n",
        "#     elif \"mahasiswa\" not in merged.columns:\n",
        "#         merged[\"mahasiswa\"] = None\n",
        "\n",
        "\n",
        "#     # Ganti NaN skor dengan 0 agar bisa dihitung rata-ratanya\n",
        "#     merged[\"TOD(M→D)\"] = merged[\"TOD(M→D)\"].fillna(0)\n",
        "#     merged[\"OD(D→M)\"] = merged[\"OD(D→M)\"].fillna(0)\n",
        "\n",
        "#     # Tandai overlap jika dosen muncul di kedua arah\n",
        "#     merged[\"overlap\"] = (merged[\"TOD(M→D)\"] > 0) & (merged[\"OD(D→M)\"] > 0)\n",
        "\n",
        "#     # Hitung skor rata-rata (hanya jika overlap)\n",
        "#     merged[\"skor_rata2\"] = merged.apply(\n",
        "#         lambda row: (row[\"TOD(M→D)\"] + row[\"OD(D→M)\"]) / 2 if row[\"overlap\"] else 0, axis=1\n",
        "#     )\n",
        "\n",
        "#     # Ambil skor tertinggi per proposal dan dosen\n",
        "#     final_scores = merged.sort_values(by=[\"id_proposal\", \"skor_rata2\"], ascending=[True, False])\n",
        "#     final_scores = final_scores[[\"id_proposal\", \"mahasiswa\",\"dosen\", \"id_dosen\", \"TOD(M→D)\",\"OD(D→M)\", \"skor_rata2\", \"overlap\"]]\n",
        "\n",
        "#     return final_scores\n",
        "\n",
        "# df_final = combine_overlap_scores(tod_m2d_df, od_d2m_df)\n",
        "# # df_final = combine_overlap_scores(tod_m2d_before_sorted, od_d2m_before_sorted, top_n=10)\n",
        "\n",
        "# # idx = df_final.groupby([\"id_proposal\", \"id_dosen\"])[\"skor_rata2\"].idxmax()\n",
        "# # df_final = df_final.loc[idx]\n",
        "\n",
        "# # print(df_final.head(5))\n",
        "\n",
        "# df_final.to_csv(\"/content/drive/MyDrive/Skripsi3/topik/directedbaru/30/overlap_directed_topik.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCttVVnEsjV7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def combine_overlap_scores(df_m2d, df_d2m):\n",
        "    # Ambil semua kolom yang dibutuhkan dari masing-masing arah\n",
        "    top_m2d = df_m2d[[\"id_proposal\", \"mahasiswa\", \"id_dosen\", \"dosen\", \"TOD(M→D)\"]]\n",
        "    top_d2m = df_d2m[[\"id_proposal\", \"mahasiswa\", \"id_dosen\", \"dosen\", \"OD(D→M)\"]]\n",
        "\n",
        "    # Gabungkan kedua dataframe berdasarkan id_proposal dan id_dosen\n",
        "    merged = pd.merge(top_m2d, top_d2m, on=[\"id_proposal\", \"id_dosen\"], how=\"outer\", suffixes=('_m2d', '_d2m'))\n",
        "\n",
        "    # Gabungkan kolom nama mahasiswa dan dosen (dari salah satu sisi)\n",
        "    merged[\"mahasiswa\"] = merged[\"mahasiswa_m2d\"].combine_first(merged[\"mahasiswa_d2m\"])\n",
        "    merged[\"dosen\"] = merged[\"dosen_m2d\"].combine_first(merged[\"dosen_d2m\"])\n",
        "\n",
        "    # Ganti NaN skor dengan 0\n",
        "    merged[\"TOD(M→D)\"] = merged[\"TOD(M→D)\"].fillna(0)\n",
        "    merged[\"OD(D→M)\"] = merged[\"OD(D→M)\"].fillna(0)\n",
        "\n",
        "    # Tandai overlap jika skor dari dua arah ada (lebih dari 0)\n",
        "    merged[\"overlap\"] = (merged[\"TOD(M→D)\"] > 0) & (merged[\"OD(D→M)\"] > 0)\n",
        "\n",
        "    # Hitung skor rata-rata hanya jika overlap, jika tidak maka 0\n",
        "    merged[\"skor_rata2\"] = merged.apply(\n",
        "        lambda row: (row[\"TOD(M→D)\"] + row[\"OD(D→M)\"]) / 2 if row[\"overlap\"] else 0, axis=1\n",
        "    )\n",
        "\n",
        "    # Urutkan berdasarkan skor rata-rata tertinggi untuk setiap proposal\n",
        "    final_scores = merged.sort_values(by=[\"id_proposal\", \"skor_rata2\"], ascending=[True, False])\n",
        "\n",
        "    # Pilih kolom akhir yang relevan\n",
        "    final_scores = final_scores[[\n",
        "        \"id_proposal\", \"mahasiswa\", \"dosen\", \"id_dosen\",\n",
        "        \"TOD(M→D)\", \"OD(D→M)\", \"skor_rata2\", \"overlap\"\n",
        "    ]]\n",
        "\n",
        "    return final_scores\n",
        "\n",
        "# Contoh penggunaan:\n",
        "df_final = combine_overlap_scores(tod_m2d_df, od_d2m_df)\n",
        "# df_final.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/14_baru/overlap_directed_topik.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65wj7aCgNlf3"
      },
      "source": [
        "# Ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsIetvPuNaPV"
      },
      "outputs": [],
      "source": [
        "def combine_overlap_scores_with_ranking(df_m2d, df_d2m):\n",
        "    # Ambil semua skor dari kedua arah\n",
        "    m2d = df_m2d[[\"id_proposal\", \"id_dosen\",\"mahasiswa\", \"dosen\", \"TOD(M→D)\"]]\n",
        "    d2m = df_d2m[[\"id_proposal\",\"id_dosen\",\"mahasiswa\",\"dosen\", \"OD(D→M)\"]]\n",
        "\n",
        "    # Outer join agar semua kombinasi muncul\n",
        "    merged = pd.merge(m2d, d2m, on=[\"id_proposal\", \"id_dosen\"], how=\"outer\")\n",
        "\n",
        "    # Tambahkan kolom nama dosen jika hilang (dari M→D arah saja)\n",
        "    if \"dosen_x\" in merged.columns:\n",
        "        merged[\"dosen\"] = merged[\"dosen_x\"].combine_first(merged.get(\"dosen_y\"))\n",
        "    elif \"dosen\" not in merged.columns:\n",
        "        merged[\"dosen\"] = None\n",
        "\n",
        "    if \"mahasiswa_x\" in merged.columns:\n",
        "        merged[\"mahasiswa\"] = merged[\"mahasiswa_x\"].combine_first(merged.get(\"mahasiswa_y\"))\n",
        "    elif \"mahasiswa\" not in merged.columns:\n",
        "        merged[\"mahasiswa\"] = None\n",
        "\n",
        "    # Isi nilai NaN dengan 0 untuk penggabungan skor\n",
        "    merged[\"TOD(M→D)\"] = merged[\"TOD(M→D)\"].fillna(0)\n",
        "    merged[\"OD(D→M)\"] = merged[\"OD(D→M)\"].fillna(0)\n",
        "\n",
        "    # Overlap = muncul di kedua arah\n",
        "    merged[\"overlap\"] = (merged[\"TOD(M→D)\"] > 0) & (merged[\"OD(D→M)\"] > 0)\n",
        "\n",
        "    # Skor rata-rata jika overlap\n",
        "    merged[\"skor_rata2\"] = merged.apply(\n",
        "        lambda row: (row[\"TOD(M→D)\"] + row[\"OD(D→M)\"]) / 2 if row[\"overlap\"] else 0, axis=1\n",
        "    )\n",
        "\n",
        "       # Hitung ranking per proposal berdasarkan skor rata-rata (tanpa groupby + agg)\n",
        "    merged[\"rank\"] = merged.groupby(\"id_proposal\")[\"skor_rata2\"]\\\n",
        "                           .rank(ascending=False, method=\"dense\")\\\n",
        "                           .astype(int)\n",
        "\n",
        "    # Ambil kolom yang diinginkan dan urutkan\n",
        "    result = merged.sort_values(by=[\"id_proposal\", \"rank\"])[\n",
        "        [\"id_proposal\",\"mahasiswa\", \"dosen\", \"id_dosen\", \"TOD(M→D)\", \"OD(D→M)\", \"skor_rata2\", \"overlap\", \"rank\"]\n",
        "    ]\n",
        "\n",
        "\n",
        "    return result.sort_values(by=[\"id_proposal\", \"rank\"])[\n",
        "        [\"id_proposal\",\"mahasiswa\", \"dosen\", \"id_dosen\", \"TOD(M→D)\", \"OD(D→M)\", \"skor_rata2\", \"overlap\", \"rank\"]\n",
        "    ]\n",
        "\n",
        "\n",
        "df_peringkat = combine_overlap_scores_with_ranking(tod_m2d_df, od_d2m_df)\n",
        "# Filter hanya yang overlap == True\n",
        "df_overlap_true = df_peringkat[df_peringkat[\"overlap\"] == True]\n",
        "# print(df_overlap_true.head(20))\n",
        "\n",
        "# df_overlap_true.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/14_baru/rank_overlap_true_kata.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CyWBkN1Nohe"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# 1. Inisialisasi count untuk rank 1\n",
        "rank1_count_directed = defaultdict(int)\n",
        "final_assignment_directed = []\n",
        "\n",
        "# 2. Tetapkan rank 1 dengan batas 15 kali per dosen\n",
        "for pid in df_overlap_true[\"id_proposal\"].unique():\n",
        "    candidates = df_overlap_true[df_overlap_true[\"id_proposal\"] == pid]\n",
        "    candidates = candidates.sort_values(by=\"skor_rata2\", ascending=False)\n",
        "\n",
        "    assigned_rank1 = False\n",
        "    for _, row in candidates.iterrows():\n",
        "        dosen_id = row[\"id_dosen\"]\n",
        "        if rank1_count_directed[dosen_id] < 15:\n",
        "            rank1_count_directed[dosen_id] += 1\n",
        "            row_data = row.to_dict()\n",
        "            row_data[\"rank\"] = 1\n",
        "            row_data[\"beban\"] = rank1_count_directed[dosen_id]\n",
        "            final_assignment_directed.append(row_data)\n",
        "            assigned_rank1 = True\n",
        "            break\n",
        "\n",
        "    if not assigned_rank1:\n",
        "        row = candidates.iloc[0].to_dict()\n",
        "        dosen_id = row[\"id_dosen\"]\n",
        "        rank1_count_directed[dosen_id] += 1\n",
        "        row[\"rank\"] = 1\n",
        "        row[\"beban\"] = rank1_count_directed[dosen_id]\n",
        "        final_assignment_directed.append(row)\n",
        "\n",
        "# 3. Buat dataframe dari rank 1\n",
        "rank1_directed = pd.DataFrame(final_assignment_directed)\n",
        "\n",
        "# 4. Tambahkan rank 2–17 berdasarkan similarity, excl. dosen yang sudah dipakai di rank 1 untuk proposal yang sama\n",
        "other_ranks = []\n",
        "\n",
        "for pid in df_overlap_true[\"id_proposal\"].unique():\n",
        "    # Dapatkan dosen yang sudah dipakai sebagai rank 1\n",
        "    used_dosen = rank1_directed[rank1_directed[\"id_proposal\"] == pid][\"id_dosen\"].tolist()\n",
        "\n",
        "    # Ambil kandidat lain untuk proposal ini\n",
        "    candidates = df_overlap_true[(df_overlap_true[\"id_proposal\"] == pid) & (~df_overlap_true[\"id_dosen\"].isin(used_dosen))]\n",
        "    candidates = candidates.sort_values(by=\"skor_rata2\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    for idx, (_, row) in enumerate(candidates.iterrows(), start=2):\n",
        "        if idx > 17:\n",
        "            break\n",
        "        row_data = row.to_dict()\n",
        "        row_data[\"rank\"] = idx\n",
        "        row_data[\"beban\"] = rank1_count_directed[row_data[\"id_dosen\"]]  # Beban hanya dihitung dari rank 1\n",
        "        other_ranks.append(row_data)\n",
        "\n",
        "# 5. Gabungkan rank1 dan other ranks\n",
        "df_ranked_filtered = pd.concat([rank1_directed, pd.DataFrame(other_ranks)], ignore_index=True)\n",
        "df_ranked_filtered = df_ranked_filtered.sort_values(by=[\"id_proposal\", \"rank\"])\n",
        "\n",
        "# 🔟 Filter hanya Top 10 dosen per proposal\n",
        "df_ranked_filtered = df_ranked_filtered[df_ranked_filtered[\"rank\"] <= 17]\n",
        "\n",
        "# Print the DataFrame\n",
        "# print(df_ranked_filtered)\n",
        "\n",
        "# df_ranked_filtered.to_csv(\"/kaggle/working/df_ranked_filtered_coba.csv\", index=False)\n",
        "# df_ranked_filtered.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/14_baru/beban_directed_topik.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY5LgnXwOAoi"
      },
      "source": [
        "# Evaluasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWMQJ-jIOM6q"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# Load the true labels DataFrame\n",
        "true_label_df = pd.read_csv(\"/content/drive/MyDrive/Skripsi3/Dataset/true_labels.csv\")\n",
        "\n",
        "# Gabungkan label benar menjadi list\n",
        "# true_label_df[\"true_dosens\"] = true_label_df.apply(lambda row: [\n",
        "#     row[\"author1\"], row[\"author2\"], row[\"author3\"]\n",
        "# ], axis=1)\n",
        "\n",
        "\n",
        "# Ubah kolom author, author2, author3 menjadi lowercase\n",
        "true_label_df[\"author1\"] = true_label_df[\"examiner_1\"].astype(str).str.strip()\n",
        "true_label_df[\"author2\"] = true_label_df[\"examiner_2\"].astype(str).str.strip()\n",
        "true_label_df[\"author3\"] = true_label_df[\"examiner_3\"].astype(str).str.strip()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISliac_wWuZb"
      },
      "source": [
        "coba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skG7yYjbKUlO"
      },
      "source": [
        "## Evaluasi Baru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks2XbNJ-Wt_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429f85c3-a76a-4a49-8f9e-3f9acc128781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Top-N  Mean_Recall_Existence  Recall_Pos_1_Ordered  Recall_Pos_2_Ordered  \\\n",
            "0      3               0.302817               0.34507              0.056338   \n",
            "1      5               0.443662               0.34507              0.056338   \n",
            "2      7               0.605634               0.34507              0.056338   \n",
            "3     10               0.774648               0.34507              0.056338   \n",
            "\n",
            "   Recall_Pos_3_Ordered  Avg_Normalized_Euclidean  \n",
            "0              0.091549                  0.842481  \n",
            "1              0.091549                  0.757989  \n",
            "2              0.091549                  0.668262  \n",
            "3              0.091549                  0.551565  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Contoh struktur dummy data untuk demonstrasi perbaikan fungsi (tidak dijalankan secara nyata di sini)\n",
        "# similarity_cosine_df = pd.read_csv(...)  # Format: id_proposal, rank, similarity_score_akhir, dosen\n",
        "# true_label_df = pd.read_csv(...)  # Format: proposal_id, author1, author2, author3\n",
        "\n",
        "def evaluate_ordered_recommendation_directed(rank_all_df, true_label_df, top_ns=[3, 5, 7, 10]):\n",
        "    summary = []\n",
        "\n",
        "    for TOP_N in top_ns:\n",
        "        # Filter Top-N dan urutkan\n",
        "        top_n_df = rank_all_df[rank_all_df[\"rank\"] <= TOP_N]\n",
        "        rec_df = top_n_df.sort_values(by=[\"id_proposal\", \"rank\", \"skor_rata2\"], ascending=[True, True, False])\n",
        "        rec_df = rec_df.drop_duplicates(subset=[\"id_proposal\", \"rank\",\"skor_rata2\"])\n",
        "        rec_pivot = rec_df.pivot(index=\"id_proposal\", columns=\"rank\", values=\"dosen\").reset_index()\n",
        "        rec_pivot.columns.name = None\n",
        "        rec_pivot.columns = [\"id_proposal\"] + [f\"rec_{i}\" for i in range(1, len(rec_pivot.columns))]\n",
        "\n",
        "        # Gabungkan dengan ground truth\n",
        "        merged_df = pd.merge(\n",
        "            rec_pivot,\n",
        "            true_label_df.rename(columns={\"proposal_id\": \"id_proposal\"}),\n",
        "            on=\"id_proposal\",\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Recall keberadaan (tidak memperhatikan urutan)\n",
        "        def recall_of_existence(row):\n",
        "            true_set = {row.get(\"author1\"), row.get(\"author2\"), row.get(\"author3\")}\n",
        "            pred_set = {row.get(f\"rec_{i}\") for i in range(1, TOP_N + 1) if row.get(f\"rec_{i}\")}\n",
        "            return len(true_set.intersection(pred_set)) / 3\n",
        "\n",
        "        merged_df[f'recall_of_existence@{TOP_N}'] = merged_df.apply(recall_of_existence, axis=1)\n",
        "\n",
        "        # Recall per posisi dengan urutan diperhatikan (rec_i harus sama dengan author_i)\n",
        "        recall_pos = {1: [], 2: [], 3: []}\n",
        "        for _, row in merged_df.iterrows():\n",
        "            for pos in [1, 2, 3]:\n",
        "                examiner = row.get(f'author{pos}')\n",
        "                rec = row.get(f'rec_{pos}') if pos <= TOP_N else None\n",
        "                hit = int(pd.notna(examiner) and pd.notna(rec) and examiner == rec)\n",
        "                recall_pos[pos].append(hit)\n",
        "\n",
        "        # Tambahkan recall ke DataFrame\n",
        "        for pos in [1, 2, 3]:\n",
        "            merged_df[f'recall_pos{pos}_ordered@{TOP_N}'] = recall_pos[pos]\n",
        "\n",
        "        recall_pos_mean = {pos: np.mean(recall_pos[pos]) for pos in [1, 2, 3]}\n",
        "\n",
        "        # Euclidean distance antar posisi (penalti posisi meleset)\n",
        "        distances = []\n",
        "        for _, row in merged_df.iterrows():\n",
        "            true_authors = [row.get(f'author{i}') for i in [1, 2, 3]]\n",
        "            pred_authors = [row.get(f'rec_{i}', None) for i in range(1, TOP_N + 1)]\n",
        "            distance = 0\n",
        "            max_penalty = TOP_N\n",
        "            for i, true_author in enumerate(true_authors):\n",
        "                if pd.isna(true_author) or true_author == '':\n",
        "                    continue\n",
        "                try:\n",
        "                    pred_pos = pred_authors.index(true_author)\n",
        "                    pos_diff = pred_pos - i\n",
        "                    distance += pos_diff ** 2\n",
        "                except ValueError:\n",
        "                    distance += max_penalty ** 2\n",
        "            distances.append(np.sqrt(distance))\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        norm_dists = scaler.fit_transform(np.array(distances).reshape(-1, 1)).flatten()\n",
        "        merged_df[f'norm_euclidean@{TOP_N}'] = norm_dists\n",
        "\n",
        "        # Ringkasan metrik\n",
        "        summary.append({\n",
        "            'Top-N': TOP_N,\n",
        "            'Mean_Recall_Existence': merged_df[f'recall_of_existence@{TOP_N}'].mean(),\n",
        "            'Recall_Pos_1_Ordered': recall_pos_mean[1],\n",
        "            'Recall_Pos_2_Ordered': recall_pos_mean[2],\n",
        "            'Recall_Pos_3_Ordered': recall_pos_mean[3],\n",
        "            'Avg_Normalized_Euclidean': np.mean(norm_dists)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(summary)\n",
        "\n",
        "result_df_directed = evaluate_ordered_recommendation_directed(df_overlap_true, true_label_df)\n",
        "# result_df_directed = evaluate_ordered_recommendation_directed(df_ranked_filtered, true_label_df)\n",
        "print(result_df_directed)\n",
        "\n",
        "\n",
        "# result_df_directed.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/14_baru/hasil_directed.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YrQO7Qbasqb"
      },
      "source": [
        "## Evaluasi per proposal Baru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7h0LFgnasJe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def evaluate_per_proposal_directed(rank_all_df, true_label_df, top_n=3):\n",
        "    # Filter dan urutkan\n",
        "    top_n_df = rank_all_df[rank_all_df[\"rank\"] <= top_n]\n",
        "    rec_df = top_n_df.sort_values(by=[\"id_proposal\", \"rank\", \"skor_rata2\"], ascending=[True, True, False])\n",
        "    rec_df = rec_df.drop_duplicates(subset=[\"id_proposal\", \"rank\",\"skor_rata2\"])\n",
        "    rec_pivot = rec_df.pivot(index=\"id_proposal\", columns=\"rank\", values=\"dosen\").reset_index()\n",
        "    rec_pivot.columns.name = None\n",
        "    rec_pivot.columns = [\"id_proposal\"] + [f\"rec_{i}\" for i in range(1, len(rec_pivot.columns))]\n",
        "\n",
        "    # Gabung dengan label kebenaran\n",
        "    merged_df = pd.merge(\n",
        "        rec_pivot,\n",
        "        true_label_df.rename(columns={\"proposal_id\": \"id_proposal\"}),\n",
        "        on=\"id_proposal\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    # Recall of existence (abaikan urutan)\n",
        "    def recall_of_existence(row):\n",
        "        true_set = {row.get(\"author1\"), row.get(\"author2\"), row.get(\"author3\")}\n",
        "        pred_set = {row.get(f\"rec_{i}\") for i in range(1, top_n + 1) if row.get(f\"rec_{i}\")}\n",
        "        return len(true_set.intersection(pred_set)) / 3\n",
        "\n",
        "    merged_df[f'recall_of_existence@{top_n}'] = merged_df.apply(recall_of_existence, axis=1)\n",
        "\n",
        "    # Recall berdasarkan posisi (urutan harus sama)\n",
        "    for pos in [1, 2, 3]:\n",
        "        merged_df[f'recall_pos{pos}_ordered@{top_n}'] = merged_df.apply(\n",
        "            lambda row: int(\n",
        "                pd.notna(row.get(f'author{pos}')) and\n",
        "                pd.notna(row.get(f'rec_{pos}')) and\n",
        "                row.get(f'author{pos}') == row.get(f'rec_{pos}')\n",
        "            ) if pos <= top_n else 0,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    # Euclidean distance penalti posisi\n",
        "    distances = []\n",
        "    for _, row in merged_df.iterrows():\n",
        "        true_authors = [row.get(f'author{i}') for i in [1, 2, 3]]\n",
        "        pred_authors = [row.get(f'rec_{i}', None) for i in range(1, top_n + 1)]\n",
        "        distance = 0\n",
        "        max_penalty = top_n\n",
        "        for i, true_author in enumerate(true_authors):\n",
        "            if pd.isna(true_author) or true_author == '':\n",
        "                continue\n",
        "            try:\n",
        "                pred_pos = pred_authors.index(true_author)\n",
        "                pos_diff = pred_pos - i\n",
        "                distance += pos_diff ** 2\n",
        "            except ValueError:\n",
        "                distance += max_penalty ** 2\n",
        "        distances.append(np.sqrt(distance))\n",
        "\n",
        "    # Normalisasi jarak\n",
        "    scaler = MinMaxScaler()\n",
        "    norm_dists = scaler.fit_transform(np.array(distances).reshape(-1, 1)).flatten()\n",
        "    merged_df[f'norm_euclidean@{top_n}'] = norm_dists\n",
        "\n",
        "    # Ambil kolom evaluasi\n",
        "    result_df = merged_df[[\"id_proposal\",\n",
        "                           f'recall_of_existence@{top_n}',\n",
        "                           f'recall_pos1_ordered@{top_n}',\n",
        "                           f'recall_pos2_ordered@{top_n}',\n",
        "                           f'recall_pos3_ordered@{top_n}',\n",
        "                           f'norm_euclidean@{top_n}']].copy()\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "eval_per_proposal_directed_3 = evaluate_per_proposal_directed(df_overlap_true, true_label_df, top_n=3)\n",
        "eval_per_proposal_directed_5 = evaluate_per_proposal_directed(df_overlap_true, true_label_df, top_n=5)\n",
        "eval_per_proposal_directed_7 = evaluate_per_proposal_directed(df_overlap_true, true_label_df, top_n=7)\n",
        "eval_per_proposal_directed_10 = evaluate_per_proposal_directed(df_overlap_true, true_label_df, top_n=10)\n",
        "\n",
        "# eval_per_proposal_directed_3.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/14_baru/hasil_eval_3_directed.csv\", index=False)\n",
        "# eval_per_proposal_directed_5.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/14_baru/hasil_eval_5_directed.csv\", index=False)\n",
        "# eval_per_proposal_directed_7.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/14_baru/hasil_eval_7_directed.csv\", index=False)\n",
        "# eval_per_proposal_directed_10.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/14_baru/hasil_eval_10_directed.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yc463fO8YsFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e67cbe-9115-408b-cfea-93cf0c40d038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_proposal mahasiswa            dosen id_dosen  TOD(M→D)  OD(D→M)  \\\n",
            "383          P12       S12  Ardhi Wijayanto      D16    0.0177   0.0451   \n",
            "388          P12       S12    Ristu Saptono       D6    0.0224   0.0404   \n",
            "715         P139      S139          Winarno      D11    0.0142   0.0387   \n",
            "721         P139      S139          Wiranto       D2    0.0142   0.0387   \n",
            "731          P14       S14          Winarno      D11    0.0140   0.0399   \n",
            "737          P14       S14          Wiranto       D2    0.0140   0.0399   \n",
            "843          P18       S18          Winarno      D11    0.0141   0.0382   \n",
            "849          P18       S18          Wiranto       D2    0.0141   0.0382   \n",
            "1243         P40       S40          Winarno      D11    0.0169   0.0392   \n",
            "1249         P40       S40          Wiranto       D2    0.0169   0.0392   \n",
            "1307         P44       S44          Winarno      D11    0.0173   0.0406   \n",
            "1313         P44       S44          Wiranto       D2    0.0173   0.0406   \n",
            "1499         P55       S55          Winarno      D11    0.0142   0.0402   \n",
            "1505         P55       S55          Wiranto       D2    0.0142   0.0402   \n",
            "1515         P56       S56          Winarno      D11    0.0140   0.0401   \n",
            "1521         P56       S56          Wiranto       D2    0.0140   0.0401   \n",
            "\n",
            "      skor_rata2  overlap  rank  \n",
            "383      0.03140     True    10  \n",
            "388      0.03140     True    10  \n",
            "715      0.02645     True     9  \n",
            "721      0.02645     True     9  \n",
            "731      0.02695     True     9  \n",
            "737      0.02695     True     9  \n",
            "843      0.02615     True     9  \n",
            "849      0.02615     True     9  \n",
            "1243     0.02805     True    10  \n",
            "1249     0.02805     True    10  \n",
            "1307     0.02895     True    10  \n",
            "1313     0.02895     True    10  \n",
            "1499     0.02720     True     9  \n",
            "1505     0.02720     True     9  \n",
            "1515     0.02705     True     9  \n",
            "1521     0.02705     True     9  \n"
          ]
        }
      ],
      "source": [
        "dupes = df_overlap_true[df_overlap_true.duplicated(subset=[\"id_proposal\", \"rank\"], keep=False)]\n",
        "print(dupes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vzt-RHMSsRw"
      },
      "source": [
        "# Cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t6VT0-ROwjb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Ubah list topic_vector menjadi array 2D\n",
        "expert_vectors = np.vstack(expert_df[\"topic_vector\"].values)\n",
        "proposal_vectors = np.vstack(proposal_df[\"topic_vector\"].values)\n",
        "\n",
        "# Hitung similarity\n",
        "similarity_matrix = cosine_similarity(expert_vectors, proposal_vectors)\n",
        "\n",
        "# Buat label baris dari dosen: gabungkan name, expert_id, research_id\n",
        "expert_labels = expert_df.apply(\n",
        "    lambda row: f\"{row['name']} ({row['expert_id']}, {row['research_id']})\", axis=1\n",
        ")\n",
        "\n",
        "# Buat label kolom dari proposal\n",
        "proposal_labels = proposal_df[\"proposal_id\"].values\n",
        "\n",
        "# Buat DataFrame dari similarity matrix\n",
        "similarity_matrix_df = pd.DataFrame(\n",
        "    similarity_matrix,\n",
        "    index=expert_labels,\n",
        "    columns=proposal_labels\n",
        ")\n",
        "\n",
        "# Simpan ke CSV\n",
        "similarity_matrix_df.to_csv(\"/content/drive/MyDrive/Skripsi4/dictionary/hasil_cosine_matriks_14.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd3oic5wS5KM"
      },
      "outputs": [],
      "source": [
        "similarity_cosine_df = []\n",
        "\n",
        "for i, (_, mahasiswa) in enumerate(proposal_df.iterrows()):\n",
        "    for j, (_, dosen) in enumerate(expert_df.iterrows()):\n",
        "        # weight = dosen.get(\"author_weight\", 1.0)\n",
        "        # score_akhir = similarity_matrix[j, i] * weight\n",
        "        score_akhir = similarity_matrix[j, i]\n",
        "        similarity_cosine_df.append({\n",
        "            \"id_proposal\": mahasiswa[\"proposal_id\"],\n",
        "            \"mahasiswa\": mahasiswa[\"student_id\"],\n",
        "            \"id_dosen\": dosen[\"expert_id\"],\n",
        "            \"id_penelitian\": dosen[\"research_id\"],\n",
        "            \"tahun_proposal\": mahasiswa[\"tahun\"],\n",
        "            \"tahun_penelitian\": dosen[\"pub_year\"],\n",
        "            \"selisih_tahun\": mahasiswa[\"tahun\"] - dosen[\"pub_year\"],\n",
        "            \"dosen\": dosen[\"name\"],\n",
        "           \"author_position\": dosen[\"author_position\"],\n",
        "            # \"weight\": weight,\n",
        "            \"similarity_score\" : similarity_matrix[j, i],\n",
        "            \"similarity_score_akhir\": score_akhir,   # baris dosen, kolom mahasiswa\n",
        "        })\n",
        "\n",
        "similarity_cosine_df = pd.DataFrame(similarity_cosine_df)\n",
        "\n",
        "# Filter sesuai kondisi\n",
        "similarity_cosine_df = similarity_cosine_df[\n",
        "    (similarity_cosine_df[\"tahun_proposal\"] > similarity_cosine_df[\"tahun_penelitian\"]) &\n",
        "    (similarity_cosine_df[\"selisih_tahun\"] <= 5)\n",
        "].copy()\n",
        "\n",
        "\n",
        "# Tampilkan hasil\n",
        "# print(similarity_cosine_df[[\"id_proposal\",\"mahasiswa\",\"id_penelitian\", \"id_dosen\",\"selisih_tahun\", \"dosen\",\"weight\", \"similarity_score\" ,\"similarity_score_akhir\",]])\n",
        "# similarity_cosine_df.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/cosine/14_baru/hasil_cosine_topik.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGX5xEtNUpcw"
      },
      "source": [
        "# Pemeringkatan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7shoY9CES9Tz"
      },
      "outputs": [],
      "source": [
        "# Ambil baris dengan similarity tertinggi untuk kombinasi unik id_proposal dan id_dosen\n",
        "idx = similarity_cosine_df.groupby([\"id_proposal\", \"id_dosen\"])[\"similarity_score_akhir\"].idxmax()\n",
        "similarity_cosine_df = similarity_cosine_df.loc[idx]\n",
        "\n",
        "# Ranking ulang berdasarkan proposal\n",
        "similarity_cosine_df[\"rank\"] = similarity_cosine_df.groupby(\"id_proposal\")[\"similarity_score_akhir\"] \\\n",
        "                                     .rank(method=\"first\", ascending=False).astype(int)\n",
        "\n",
        "# Urutkan\n",
        "similarity_cosine_df = similarity_cosine_df.sort_values([\"id_proposal\", \"rank\"])\n",
        "similarity_cosine_df = similarity_cosine_df[similarity_cosine_df[\"rank\"] <= 17]\n",
        "\n",
        "\n",
        "# Tampilkan\n",
        "# print(similarity_cosine_df[[\"id_proposal\", \"mahasiswa\", \"id_dosen\", \"dosen\", \"similarity_score\",\"similarity_score_akhir\", \"rank\"]])\n",
        "# similarity_cosine_df.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/cosine/14_baru/hasil_rank_cosine_topik.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EFjJdmrVV34"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# 1. Inisialisasi count untuk rank 1\n",
        "rank1_count = defaultdict(int)\n",
        "final_assignments = []\n",
        "\n",
        "# 2. Tetapkan rank 1 dengan batas 15 kali per dosen\n",
        "for pid in similarity_cosine_df[\"id_proposal\"].unique():\n",
        "    candidates = similarity_cosine_df[similarity_cosine_df[\"id_proposal\"] == pid]\n",
        "    candidates = candidates.sort_values(by=\"similarity_score_akhir\", ascending=False)\n",
        "\n",
        "    assigned_rank1 = False\n",
        "    for _, row in candidates.iterrows():\n",
        "        dosen_id = row[\"id_dosen\"]\n",
        "        if rank1_count[dosen_id] < 15:\n",
        "            rank1_count[dosen_id] += 1\n",
        "            row_data = row.to_dict()\n",
        "            row_data[\"rank\"] = 1\n",
        "            row_data[\"beban\"] = rank1_count[dosen_id]\n",
        "            final_assignments.append(row_data)\n",
        "            assigned_rank1 = True\n",
        "            break\n",
        "\n",
        "    if not assigned_rank1:\n",
        "        row = candidates.iloc[0].to_dict()\n",
        "        dosen_id = row[\"id_dosen\"]\n",
        "        rank1_count[dosen_id] += 1\n",
        "        row[\"rank\"] = 1\n",
        "        row[\"beban\"] = rank1_count[dosen_id]\n",
        "        final_assignments.append(row)\n",
        "\n",
        "# 3. Buat dataframe dari rank 1\n",
        "rank1_df = pd.DataFrame(final_assignments)\n",
        "\n",
        "# 4. Tambahkan rank 2–17 berdasarkan similarity, excl. dosen yang sudah dipakai di rank 1 untuk proposal yang sama\n",
        "other_ranks = []\n",
        "\n",
        "for pid in similarity_cosine_df[\"id_proposal\"].unique():\n",
        "    # Dapatkan dosen yang sudah dipakai sebagai rank 1\n",
        "    used_dosen = rank1_df[rank1_df[\"id_proposal\"] == pid][\"id_dosen\"].tolist()\n",
        "\n",
        "    # Ambil kandidat lain untuk proposal ini\n",
        "    candidates = similarity_cosine_df[(similarity_cosine_df[\"id_proposal\"] == pid) & (~similarity_cosine_df[\"id_dosen\"].isin(used_dosen))]\n",
        "    candidates = candidates.sort_values(by=\"similarity_score_akhir\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    for idx, (_, row) in enumerate(candidates.iterrows(), start=2):\n",
        "        if idx > 17:\n",
        "            break\n",
        "        row_data = row.to_dict()\n",
        "        row_data[\"rank\"] = idx\n",
        "        row_data[\"beban\"] = rank1_count[row_data[\"id_dosen\"]]  # Beban hanya dihitung dari rank 1\n",
        "        other_ranks.append(row_data)\n",
        "\n",
        "# 5. Gabungkan rank1 dan other ranks\n",
        "rank_all_df = pd.concat([rank1_df, pd.DataFrame(other_ranks)], ignore_index=True)\n",
        "rank_all_df = rank_all_df.sort_values(by=[\"id_proposal\", \"rank\"])\n",
        "\n",
        "# 6. Tampilkan hasil\n",
        "# print(rank_all_df[[\"id_proposal\", \"mahasiswa\", \"id_dosen\", \"dosen\", \"similarity_score\",\"similarity_score_akhir\", \"rank\", \"beban\"]])\n",
        "# rank_all_df.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/cosine/14_baru/beban_cosine_topik.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Urutkan berdasarkan id_proposal, similarity_score_akhir (descending), dan author_position (ascending)\n",
        "similarity_cosine_df_baru = similarity_cosine_df.sort_values(\n",
        "    by=[\"id_proposal\", \"similarity_score_akhir\", \"author_position\"],\n",
        "    ascending=[True, False, True]\n",
        ")\n",
        "\n",
        "# Tambahkan kolom rank untuk setiap id_proposal\n",
        "similarity_cosine_df_baru[\"rank\"] = similarity_cosine_df_baru.groupby(\"id_proposal\").cumcount() + 1\n",
        "\n",
        "# Simpan ke CSV\n",
        "# similarity_cosine_df_baru.to_csv(\n",
        "#     \"/content/drive/MyDrive/Skripsi4/topik/cosine/14_baru/beban_cosine_topik_sort.csv\",\n",
        "#     index=False\n",
        "# )\n",
        "\n"
      ],
      "metadata": {
        "id": "pCDHrPFiPzuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDpaTPt7VlSU"
      },
      "source": [
        "# Evaluasi Cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjpOqOQ2VdV7"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# # Fungsi evaluasi rekomendasi dosen penguji\n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# # Fungsi evaluasi rekomendasi dosen penguji\n",
        "# def evaluate_ordered_recommendation_cosine(rank_all_df, true_label_df, top_ns=[3, 5, 7, 10]):\n",
        "#     summary = []\n",
        "\n",
        "#     for TOP_N in top_ns:\n",
        "#         # Ambil Top-N dosen per proposal\n",
        "#         top_n_df = rank_all_df[rank_all_df[\"rank\"] <= TOP_N]\n",
        "\n",
        "#         # Ubah menjadi format pivot untuk gabung dengan true_label_df\n",
        "#         rec_df = top_n_df.sort_values(by=[\"id_proposal\", \"rank\", \"similarity_score_akhir\"], ascending=[True, True, False])\n",
        "#         rec_df = rec_df.drop_duplicates(subset=[\"id_proposal\", \"rank\"])\n",
        "#         rec_pivot = rec_df.pivot(index=\"id_proposal\", columns=\"rank\", values=\"dosen\").reset_index()\n",
        "#         rec_pivot.columns.name = None\n",
        "#         rec_pivot.columns = [\"id_proposal\"] + [f\"rec_{i}\" for i in range(1, len(rec_pivot.columns))]\n",
        "\n",
        "#         # Gabungkan dengan label ground truth\n",
        "#         merged_df = pd.merge(\n",
        "#             rec_pivot,\n",
        "#             true_label_df.rename(columns={\"proposal_id\": \"id_proposal\"}),\n",
        "#             on=\"id_proposal\",\n",
        "#             how=\"left\"\n",
        "#         )\n",
        "\n",
        "#         # Recall keberadaan (tanpa urutan)\n",
        "#         def recall_of_existence(row):\n",
        "#             true_set = {row.get(\"author1\"), row.get(\"author2\"), row.get(\"author3\")}\n",
        "#             pred_set = {row.get(f\"rec_{i}\") for i in range(1, TOP_N+1) if row.get(f\"rec_{i}\")}\n",
        "#             return len(true_set.intersection(pred_set)) / 3\n",
        "\n",
        "#         merged_df[f'recall_of_existence@{TOP_N}'] = merged_df.apply(recall_of_existence, axis=1)\n",
        "\n",
        "#         # Recall posisi urut (ordered position recall)\n",
        "#         recall_pos = {1: [], 2: [], 3: []}\n",
        "#         for _, row in merged_df.iterrows():\n",
        "#             gt = [row.get(f'author{i}') for i in range(1, 4)]\n",
        "#             pred = [row.get(f'rec_{i}') for i in range(1, TOP_N + 1)]\n",
        "\n",
        "#             # Posisi kemunculan tiap GT di pred\n",
        "#             positions = {}\n",
        "#             for i, g in enumerate(gt):\n",
        "#                 try:\n",
        "#                     positions[i + 1] = pred.index(g)\n",
        "#                 except ValueError:\n",
        "#                     positions[i + 1] = None\n",
        "\n",
        "#             # Cek urutan valid (misalnya: pos(author1) < pos(author2) < pos(author3))\n",
        "#             valid_order = True\n",
        "#             for i in range(1, 3):\n",
        "#                 if positions.get(i) is not None and positions.get(i + 1) is not None:\n",
        "#                     if positions[i] >= positions[i + 1]:\n",
        "#                         valid_order = False\n",
        "#                         break\n",
        "\n",
        "#             for pos in [1, 2, 3]:\n",
        "#                 hit = int(positions.get(pos) is not None and valid_order)\n",
        "#                 recall_pos[pos].append(hit)\n",
        "\n",
        "#         for pos in [1, 2, 3]:\n",
        "#             merged_df[f'recall_pos{pos}_ordered@{TOP_N}'] = recall_pos[pos]\n",
        "\n",
        "#         recall_pos_mean = {\n",
        "#             pos: np.mean(recall_pos[pos]) if recall_pos[pos] else None\n",
        "#             for pos in [1, 2, 3]\n",
        "#         }\n",
        "\n",
        "#         # Euclidean distance antar posisi\n",
        "#         distances = []\n",
        "#         for _, row in merged_df.iterrows():\n",
        "#             true_authors = [row.get(f'author{i}') for i in [1, 2, 3]]\n",
        "#             pred_authors = [row.get(f'rec_{i}', None) for i in range(1, TOP_N + 1)]\n",
        "#             distance = 0\n",
        "#             max_penalty = TOP_N\n",
        "#             for i, true_author in enumerate(true_authors):\n",
        "#                 if pd.isna(true_author) or true_author == '':\n",
        "#                     continue\n",
        "#                 try:\n",
        "#                     pred_pos = pred_authors.index(true_author)\n",
        "#                     pos_diff = pred_pos - i\n",
        "#                     distance += pos_diff ** 2\n",
        "#                 except ValueError:\n",
        "#                     distance += max_penalty ** 2\n",
        "#             distances.append(np.sqrt(distance))\n",
        "\n",
        "#         scaler = MinMaxScaler()\n",
        "#         norm_dists = scaler.fit_transform(np.array(distances).reshape(-1, 1)).flatten()\n",
        "#         merged_df[f'norm_euclidean@{TOP_N}'] = norm_dists\n",
        "#         avg_dist = np.mean(norm_dists)\n",
        "#         avg_recall_exist = merged_df[f'recall_of_existence@{TOP_N}'].mean()\n",
        "\n",
        "#         summary.append({\n",
        "#             'Top-N': TOP_N,\n",
        "#             'Mean_Recall_Existence': avg_recall_exist,\n",
        "#             'Recall_Pos_1_Ordered': recall_pos_mean[1],\n",
        "#             'Recall_Pos_2_Ordered': recall_pos_mean[2],\n",
        "#             'Recall_Pos_3_Ordered': recall_pos_mean[3],\n",
        "#             'Avg_Normalized_Euclidean': avg_dist\n",
        "#         })\n",
        "\n",
        "#     return pd.DataFrame(summary)\n",
        "\n",
        "\n",
        "# result_eval_cosine = evaluate_ordered_recommendation_cosine(similarity_cosine_df, true_label_df)\n",
        "# print(result_eval_cosine)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ11PrKrWfQX"
      },
      "source": [
        "coba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-1fxi4gKcPw"
      },
      "source": [
        "## Evaluasi Baru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyEdj1o1Szkf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ff53fb-ba87-4097-b536-7e3784218079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Top-N  Mean_Recall_Existence  Recall_Pos_1_Ordered  Recall_Pos_2_Ordered  \\\n",
            "0      3               0.345070              0.260563              0.105634   \n",
            "1      5               0.460094              0.260563              0.105634   \n",
            "2      7               0.593897              0.260563              0.105634   \n",
            "3     10               0.753521              0.260563              0.105634   \n",
            "\n",
            "   Recall_Pos_3_Ordered  Avg_Normalized_Euclidean  \n",
            "0              0.077465                  0.807783  \n",
            "1              0.077465                  0.734154  \n",
            "2              0.077465                  0.659702  \n",
            "3              0.077465                  0.554564  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def evaluate_ordered_recommendation_cosine(rank_all_df, true_label_df, top_ns=[3, 5, 7, 10]):\n",
        "    summary = []\n",
        "\n",
        "    for TOP_N in top_ns:\n",
        "        # Filter Top-N dan urutkan\n",
        "        top_n_df = rank_all_df[rank_all_df[\"rank\"] <= TOP_N]\n",
        "        rec_df = top_n_df.sort_values(by=[\"id_proposal\", \"rank\", \"similarity_score_akhir\"], ascending=[True, True, False])\n",
        "        rec_pivot = rec_df.pivot(index=\"id_proposal\", columns=\"rank\", values=\"dosen\").reset_index()\n",
        "        rec_pivot.columns.name = None\n",
        "        rec_pivot.columns = [\"id_proposal\"] + [f\"rec_{i}\" for i in range(1, len(rec_pivot.columns))]\n",
        "\n",
        "        # Gabungkan dengan ground truth\n",
        "        merged_df = pd.merge(\n",
        "            rec_pivot,\n",
        "            true_label_df.rename(columns={\"proposal_id\": \"id_proposal\"}),\n",
        "            on=\"id_proposal\",\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Recall keberadaan (tidak memperhatikan urutan)\n",
        "        def recall_of_existence(row):\n",
        "            true_set = {row.get(\"author1\"), row.get(\"author2\"), row.get(\"author3\")}\n",
        "            pred_set = {row.get(f\"rec_{i}\") for i in range(1, TOP_N + 1) if row.get(f\"rec_{i}\")}\n",
        "            return len(true_set.intersection(pred_set)) / 3\n",
        "\n",
        "        merged_df[f'recall_of_existence@{TOP_N}'] = merged_df.apply(recall_of_existence, axis=1)\n",
        "\n",
        "        # Recall per posisi dengan urutan diperhatikan (rec_i harus sama dengan author_i)\n",
        "        recall_pos = {1: [], 2: [], 3: []}\n",
        "        for _, row in merged_df.iterrows():\n",
        "            for pos in [1, 2, 3]:\n",
        "                examiner = row.get(f'author{pos}')\n",
        "                rec = row.get(f'rec_{pos}') if pos <= TOP_N else None\n",
        "                hit = int(pd.notna(examiner) and pd.notna(rec) and examiner == rec)\n",
        "                recall_pos[pos].append(hit)\n",
        "\n",
        "        # Tambahkan recall ke DataFrame\n",
        "        for pos in [1, 2, 3]:\n",
        "            merged_df[f'recall_pos{pos}_ordered@{TOP_N}'] = recall_pos[pos]\n",
        "\n",
        "        recall_pos_mean = {pos: np.mean(recall_pos[pos]) for pos in [1, 2, 3]}\n",
        "\n",
        "        # Euclidean distance antar posisi (penalti posisi meleset)\n",
        "        distances = []\n",
        "        for _, row in merged_df.iterrows():\n",
        "            true_authors = [row.get(f'author{i}') for i in [1, 2, 3]]\n",
        "            pred_authors = [row.get(f'rec_{i}', None) for i in range(1, TOP_N + 1)]\n",
        "            distance = 0\n",
        "            max_penalty = TOP_N\n",
        "            for i, true_author in enumerate(true_authors):\n",
        "                if pd.isna(true_author) or true_author == '':\n",
        "                    continue\n",
        "                try:\n",
        "                    pred_pos = pred_authors.index(true_author)\n",
        "                    pos_diff = pred_pos - i\n",
        "                    distance += pos_diff ** 2\n",
        "                except ValueError:\n",
        "                    distance += max_penalty ** 2\n",
        "            distances.append(np.sqrt(distance))\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        norm_dists = scaler.fit_transform(np.array(distances).reshape(-1, 1)).flatten()\n",
        "        merged_df[f'norm_euclidean@{TOP_N}'] = norm_dists\n",
        "\n",
        "        # Ringkasan metrik\n",
        "        summary.append({\n",
        "            'Top-N': TOP_N,\n",
        "            'Mean_Recall_Existence': merged_df[f'recall_of_existence@{TOP_N}'].mean(),\n",
        "            'Recall_Pos_1_Ordered': recall_pos_mean[1],\n",
        "            'Recall_Pos_2_Ordered': recall_pos_mean[2],\n",
        "            'Recall_Pos_3_Ordered': recall_pos_mean[3],\n",
        "            'Avg_Normalized_Euclidean': np.mean(norm_dists)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(summary)\n",
        "\n",
        "result_df = evaluate_ordered_recommendation_cosine(similarity_cosine_df, true_label_df)\n",
        "print(result_df)\n",
        "# result_df.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/cosine/14_baru/result_df.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiyEGakTZn8P"
      },
      "source": [
        "## per proposal baru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZoVKOZGDJFx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def evaluate_per_proposal_cosine(rank_all_df, true_label_df, top_n=3):\n",
        "    # Filter dan urutkan\n",
        "    top_n_df = rank_all_df[rank_all_df[\"rank\"] <= top_n]\n",
        "    rec_df = top_n_df.sort_values(by=[\"id_proposal\", \"rank\", \"similarity_score_akhir\"], ascending=[True, True, False])\n",
        "    rec_pivot = rec_df.pivot(index=\"id_proposal\", columns=\"rank\", values=\"dosen\").reset_index()\n",
        "    rec_pivot.columns.name = None\n",
        "    rec_pivot.columns = [\"id_proposal\"] + [f\"rec_{i}\" for i in range(1, len(rec_pivot.columns))]\n",
        "\n",
        "    # Gabung dengan label kebenaran\n",
        "    merged_df = pd.merge(\n",
        "        rec_pivot,\n",
        "        true_label_df.rename(columns={\"proposal_id\": \"id_proposal\"}),\n",
        "        on=\"id_proposal\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    # Recall of existence (abaikan urutan)\n",
        "    def recall_of_existence(row):\n",
        "        true_set = {row.get(\"author1\"), row.get(\"author2\"), row.get(\"author3\")}\n",
        "        pred_set = {row.get(f\"rec_{i}\") for i in range(1, top_n + 1) if row.get(f\"rec_{i}\")}\n",
        "        return len(true_set.intersection(pred_set)) / 3\n",
        "\n",
        "    merged_df[f'recall_of_existence@{top_n}'] = merged_df.apply(recall_of_existence, axis=1)\n",
        "\n",
        "    # Recall berdasarkan posisi (urutan harus sama)\n",
        "    for pos in [1, 2, 3]:\n",
        "        merged_df[f'recall_pos{pos}_ordered@{top_n}'] = merged_df.apply(\n",
        "            lambda row: int(\n",
        "                pd.notna(row.get(f'author{pos}')) and\n",
        "                pd.notna(row.get(f'rec_{pos}')) and\n",
        "                row.get(f'author{pos}') == row.get(f'rec_{pos}')\n",
        "            ) if pos <= top_n else 0,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    # Euclidean distance penalti posisi\n",
        "    distances = []\n",
        "    for _, row in merged_df.iterrows():\n",
        "        true_authors = [row.get(f'author{i}') for i in [1, 2, 3]]\n",
        "        pred_authors = [row.get(f'rec_{i}', None) for i in range(1, top_n + 1)]\n",
        "        distance = 0\n",
        "        max_penalty = top_n\n",
        "        for i, true_author in enumerate(true_authors):\n",
        "            if pd.isna(true_author) or true_author == '':\n",
        "                continue\n",
        "            try:\n",
        "                pred_pos = pred_authors.index(true_author)\n",
        "                pos_diff = pred_pos - i\n",
        "                distance += pos_diff ** 2\n",
        "            except ValueError:\n",
        "                distance += max_penalty ** 2\n",
        "        distances.append(np.sqrt(distance))\n",
        "\n",
        "    # Normalisasi jarak\n",
        "    scaler = MinMaxScaler()\n",
        "    norm_dists = scaler.fit_transform(np.array(distances).reshape(-1, 1)).flatten()\n",
        "    merged_df[f'norm_euclidean@{top_n}'] = norm_dists\n",
        "\n",
        "    # Ambil kolom evaluasi\n",
        "    result_df = merged_df[[\"id_proposal\",\n",
        "                           f'recall_of_existence@{top_n}',\n",
        "                           f'recall_pos1_ordered@{top_n}',\n",
        "                           f'recall_pos2_ordered@{top_n}',\n",
        "                           f'recall_pos3_ordered@{top_n}',\n",
        "                           f'norm_euclidean@{top_n}']].copy()\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "eval_per_proposal_cosine_3 = evaluate_per_proposal_cosine(similarity_cosine_df, true_label_df, top_n=3)\n",
        "eval_per_proposal_cosine_5 = evaluate_per_proposal_cosine(similarity_cosine_df, true_label_df, top_n=5)\n",
        "eval_per_proposal_cosine_7 = evaluate_per_proposal_cosine(similarity_cosine_df, true_label_df, top_n=7)\n",
        "eval_per_proposal_cosine_10 = evaluate_per_proposal_cosine(similarity_cosine_df, true_label_df, top_n=10)\n",
        "\n",
        "# eval_per_proposal_cosine_3.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/cosine/14_baru/hasil_eval_3_cosine_topik.csv\", index=False)\n",
        "# eval_per_proposal_cosine_5.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/cosine/14_baru/hasil_eval_5_cosine_topik.csv\", index=False)\n",
        "# eval_per_proposal_cosine_7.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/cosine/14_baru/hasil_eval_7_cosine_topik.csv\", index=False)\n",
        "# eval_per_proposal_cosine_10.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/cosine/14_baru/hasil_eval_10_cosine_topik.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}