{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slsabilaAura/Tugas-Akhir/blob/main/LDA_Topik_30.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aca851f",
        "outputId": "c3312d00-5f69-42fd-b533-8e649de76a0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQIdw0eelk43",
        "outputId": "e5a0d001-bff2-42fd-ef46-3ef3f526bb4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: import file form drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0HTbrGd6ePM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Save the preprocessed DataFrames to new CSV files\n",
        "proposal_df = pd.read_csv('/content/drive/MyDrive/Skripsi3/Dataset/processed_proposalC.csv')\n",
        "expert_df = pd.read_csv('/content/drive/MyDrive/Skripsi3/Dataset/processed_expertC.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpxYRgx768uU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import ast\n",
        "from gensim.models import Phrases\n",
        "from gensim.models.phrases import Phraser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brLAWvZi8CQh"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G42J8DDSOq0f"
      },
      "outputs": [],
      "source": [
        "def convert_to_list(text):\n",
        "    try:\n",
        "        return ast.literal_eval(text) if isinstance(text, str) else text\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "\n",
        "proposal_df[\"processed\"] = proposal_df[\"stemmed\"].apply(convert_to_list)\n",
        "expert_df[\"processed\"] = expert_df[\"stemmed\"].apply(convert_to_list)\n",
        "\n",
        "# Gabungkan semua dokumen untuk membuat satu kamus bersama\n",
        "documents_all = proposal_df[\"processed\"].tolist() + expert_df[\"processed\"].tolist()\n",
        "dictionary_all = Dictionary(documents_all)\n",
        "\n",
        "dictionary_all.filter_extremes(no_below=5, no_above=0.5)\n",
        "\n",
        "proposal_corpus = [dictionary_all.doc2bow(doc) for doc in proposal_df[\"processed\"]]\n",
        "expert_corpus = [dictionary_all.doc2bow(doc) for doc in expert_df[\"processed\"]]\n",
        "\n",
        "corpus_all= proposal_corpus + expert_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx-2C63p6Ma_",
        "outputId": "39760004-5e41-44a2-b31d-87f9502305f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609\n",
            "609\n",
            "3086\n"
          ]
        }
      ],
      "source": [
        "print(len(documents_all))\n",
        "print(len(corpus_all))\n",
        "print(len(dictionary_all))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NeYydlZKA0W3"
      },
      "outputs": [],
      "source": [
        "# # Misalnya kita ambil satu dokumen dari corpus_all\n",
        "# doc_bow = corpus_all[0]\n",
        "\n",
        "# # # Tampilkan representasi kata dari dokumen tersebut\n",
        "# for word_id, freq in doc_bow:\n",
        "#     word = dictionary_all[word_id]  # ambil kata dari kamus berdasarkan indeks\n",
        "#     print(f\"('{word}', {freq})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me5xyf5Z8OwR"
      },
      "source": [
        "# Model LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wYePqiNSxAy5"
      },
      "outputs": [],
      "source": [
        "# import time\n",
        "# import pandas as pd\n",
        "# import pandas as pd\n",
        "# from gensim.models.ldamodel import LdaModel\n",
        "# from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "# # List untuk menyimpan hasil eksperimen\n",
        "# results_lda = []\n",
        "\n",
        "\n",
        "# for num_topics in range(1, 51):\n",
        "#     start_time = time.time()  # Mulai timer\n",
        "\n",
        "#     # Train LDA model\n",
        "#     lda_model_lda = LdaModel(\n",
        "#         corpus=corpus_all,\n",
        "#         id2word=dictionary_all,\n",
        "#         num_topics=num_topics,\n",
        "#         alpha=0.5,   # Bisa diganti dengan nilai tetap\n",
        "#         eta=0.01,     # Bisa diganti dengan nilai tetap\n",
        "#         passes=50,\n",
        "#         random_state=42,\n",
        "#         iterations= 400,\n",
        "#     )\n",
        "\n",
        "#     # Evaluasi dengan coherence score\n",
        "#     coherence_model_lda= CoherenceModel(\n",
        "#         model=lda_model_lda,\n",
        "#         texts=proposal_df['processed'].tolist() + expert_df['processed'].tolist(),\n",
        "#         dictionary=dictionary_all,\n",
        "#         coherence='c_v'\n",
        "#     )\n",
        "#     coherence_score_lda = coherence_model_lda.get_coherence()\n",
        "\n",
        "#     # Evaluasi Topic Diversity\n",
        "#     top_words_per_topic_lda = [lda_model_lda.show_topic(topic_id, topn=20) for topic_id in range(num_topics)]\n",
        "#     unique_words_lda = set([word for topic in top_words_per_topic_lda for word, _ in topic])\n",
        "#     total_words_lda = num_topics * 20  # 10 kata per topik\n",
        "#     topic_diversity_lda = len(unique_words_lda) / total_words_lda if total_words_lda > 0 else 0\n",
        "\n",
        "#     # Hitung waktu eksekusi\n",
        "#     time_taken_lda = time.time() - start_time\n",
        "\n",
        "#     # Simpan hasil dalam list\n",
        "#     results_lda.append([num_topics, coherence_score_lda, topic_diversity_lda, time_taken_lda])\n",
        "#     print(f\"Topics: {num_topics} | Coherence: {coherence_score_lda:.4f} | Diversity: {topic_diversity_lda:.4f} | Time: {time_taken_lda:.2f} sec\")\n",
        "\n",
        "# # Simpan ke DataFrame\n",
        "# results_looping_df = pd.DataFrame(results_lda, columns=['num_topics', 'coherence_score_lda', 'topic_diversity_lda', 'time_taken_lda'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6DYahm2FTpy"
      },
      "outputs": [],
      "source": [
        "# results_looping_df.to_csv('/content/drive/MyDrive/Skripsi4/topik/dictionary/looping_baru,3koma3.csv', index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJDKKZUd8Kqz"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "num_topics = 30  # atau optimalisasi via coherence\n",
        "\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus_all,\n",
        "    id2word=dictionary_all,\n",
        "    num_topics=num_topics,\n",
        "    passes=50,\n",
        "    random_state=42,\n",
        "    iterations= 400,\n",
        "    alpha=0.5,\n",
        "    eta=0.01\n",
        "\n",
        ")\n",
        "\n",
        "# coherence_model_lda = CoherenceModel(model=lda_model, texts=proposal_df['processed'].tolist() + expert_df['processed'].tolist(), dictionary=dictionary_all, coherence='c_v')\n",
        "# coherence_score = coherence_model_lda.get_coherence()\n",
        "# print(f\"Coherence Score: {coherence_score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan dictionary\n",
        "lda_model.save(\"lda_model_30.model\")\n",
        "dictionary_all.save(\"dictionary_lda.dict\")"
      ],
      "metadata": {
        "id": "FXYawUQCDrEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FQqm9YGikDW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "\n",
        "for topic_id in range(num_topics):\n",
        "    top_words = lda_model.show_topic(topic_id, topn=25)\n",
        "    words = [word for word, prob in top_words]\n",
        "    probs = [round(prob, 4) for word, prob in top_words]\n",
        "\n",
        "    data.append({\n",
        "        'Topik': topic_id + 1,\n",
        "        'Kata Kunci': ', '.join(words),\n",
        "        'Probabilitas': ', '.join(map(str, probs))\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Tampilkan hasil\n",
        "# print(df)\n",
        "\n",
        "# Simpan ke CSV jika diinginkan\n",
        "df.to_csv('/content/drive/MyDrive/Skripsi4/dictionary/topik_kata_dan_probabilitas_30.csv', index=False, encoding='utf-8-sig')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hon3nGsY2PfQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fQXjgHk-H7HU"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# for topic_id in range(num_topics):\n",
        "#     plt.figure(figsize=(8, 4))\n",
        "#     terms = lda_model.show_topic(topic_id, topn=4)\n",
        "#     words = [w for w, _ in terms]\n",
        "#     scores = [s for _, s in terms]\n",
        "\n",
        "#     plt.barh(words, scores)\n",
        "#     plt.gca().invert_yaxis()\n",
        "#     plt.title(f\"Topik {topic_id}\")\n",
        "#     plt.xlabel(\"Probabilitas\")\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5k7jvUSj35x"
      },
      "outputs": [],
      "source": [
        "from gensim.matutils import sparse2full\n",
        "\n",
        "# Fungsi untuk mendapatkan dense topic vector\n",
        "def get_topic_vector(lda_model, dictionary, document, num_topics):\n",
        "    bow = dictionary.doc2bow(document)\n",
        "    topic_dist = lda_model.get_document_topics(bow, minimum_probability=0.00001)\n",
        "    return sparse2full(topic_dist, num_topics)\n",
        "\n",
        "# Ambil jumlah topik dari model LDA\n",
        "num_topics = lda_model.num_topics\n",
        "\n",
        "# Hitung topik vektor dalam bentuk dense array (untuk expert)\n",
        "expert_df[\"topic_vector\"] = [\n",
        "    get_topic_vector(lda_model, dictionary_all, doc, num_topics)\n",
        "    for doc in expert_df[\"processed\"]\n",
        "]\n",
        "\n",
        "# Hitung topik vektor dalam bentuk dense array (untuk proposal)\n",
        "proposal_df[\"topic_vector\"] = [\n",
        "    get_topic_vector(lda_model, dictionary_all, doc, num_topics)\n",
        "    for doc in proposal_df[\"processed\"]\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WOc8HgLHAzGS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "topic_matrix_expert = np.vstack(expert_df[\"topic_vector\"])\n",
        "topic_df = pd.DataFrame(\n",
        "    topic_matrix_expert,\n",
        "    columns=[f\"topic_{i+1}\" for i in range(lda_model.num_topics)]\n",
        ")\n",
        "\n",
        "topic_df[\"research_id\"] = expert_df[\"research_id\"].values\n",
        "\n",
        "# Set kolom id_dosen sebagai index\n",
        "topic_df.set_index(\"research_id\", inplace=True)\n",
        "\n",
        "# (Opsional) Tampilkan 5 baris pertama untuk verifikasi\n",
        "# print(topic_df.head())\n",
        "\n",
        "# print(topic_df)\n",
        "topic_df.to_csv('/content/drive/MyDrive/Skripsi4/dictionary/vektorExpert_30.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zhzx4doA_MN"
      },
      "outputs": [],
      "source": [
        "topic_matrix_proposal = np.vstack(proposal_df[\"topic_vector\"])\n",
        "topic_proposal_df = pd.DataFrame(topic_matrix_proposal, columns=[f\"topic_{i+1}\" for i in range(lda_model.num_topics)])\n",
        "topic_proposal_df[\"proposal_id\"] = proposal_df[\"proposal_id\"].values\n",
        "\n",
        "# Set kolom id_dosen sebagai index\n",
        "topic_proposal_df.set_index(\"proposal_id\", inplace=True)\n",
        "\n",
        "# print(topic_proposal_df)\n",
        "topic_proposal_df.to_csv('/content/drive/MyDrive/Skripsi4/dictionary/vektorProposal_30.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kktZnb3TBCsE"
      },
      "source": [
        "# Kemiripan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZFE5rntA7F4"
      },
      "outputs": [],
      "source": [
        "proposal_df[\"tahun\"] = proposal_df[\"proposal_year\"].astype(int)\n",
        "expert_df[\"pub_year\"] = expert_df[\"research_pub_year\"].astype(int)\n",
        "\n",
        "\n",
        "def similarity_m_to_d(proposal_vector, expert_vector):\n",
        "    numerator = np.dot(proposal_vector, expert_vector)\n",
        "    denominator = (np.linalg.norm(proposal_vector) + 1e-10)\n",
        "    return numerator / (denominator)\n",
        "\n",
        "\n",
        "def similarity_d_to_m(expert_vector, proposal_vector):\n",
        "    numerator = np.dot(expert_vector, proposal_vector)\n",
        "    denominator = (np.linalg.norm(expert_vector) + 1e-10)\n",
        "    return numerator / (denominator)\n",
        "\n",
        " # denominator = np.linalg.norm(expert_vector)\n",
        "\n",
        "def time_decay(year_prop, year_ex, t=1, gamma=0.1):\n",
        "    decay = 1 - ((year_prop - year_ex) / t) * gamma\n",
        "    return max(decay, 0.0)  # tidak boleh negatif\n",
        "\n",
        "# mapping Dosen dengan ID Dosen\n",
        "\n",
        "dosen_id_map = pd.read_csv(\"/content/drive/MyDrive/Skripsi3/Dataset/mapping.csv\")  # pastikan kolom: expert_id, expert_name\n",
        "dosen_id_map[\"expert_name\"] = dosen_id_map[\"expert_name\"].str.strip().str.lower()\n",
        "\n",
        "\n",
        "def explode_authors_with_weights(df, dosen_id_map):\n",
        "    rows = []\n",
        "\n",
        "    # Normalisasi nama dosen agar cocok\n",
        "    dosen_id_map = dosen_id_map.copy()\n",
        "    dosen_id_map[\"expert_name\"] = dosen_id_map[\"expert_name\"].str.strip().str.lower()\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        authors = row.get(\"authors\", [])\n",
        "        # Bersihkan nama kosong atau NaN\n",
        "        authors = [a for a in authors if isinstance(a, str) and a.strip() != \"\"]\n",
        "\n",
        "        num_authors = len(authors)\n",
        "        for idx, name in enumerate(authors):\n",
        "            name_clean = name.strip().lower()\n",
        "\n",
        "            if num_authors == 1:\n",
        "                weight = 1.0\n",
        "            else:\n",
        "                weight = 0.6 if idx == 0 else 0.4 / (num_authors - 1)\n",
        "\n",
        "            new_row = row.to_dict()\n",
        "            new_row[\"name\"] = name\n",
        "            new_row[\"author_position\"] = idx + 1\n",
        "            new_row[\"num_authors\"] = num_authors\n",
        "            new_row[\"author_weight\"] = round(weight, 4)\n",
        "\n",
        "            matched = dosen_id_map[dosen_id_map[\"expert_name\"] == name_clean]\n",
        "            new_row[\"expert_id\"] = matched[\"expert_id\"].values[0] if not matched.empty else None\n",
        "\n",
        "            rows.append(new_row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# Gabungkan author_1 sampai author_6 jadi list\n",
        "author_cols = [\"author_1\", \"author_2\", \"author_3\", \"author_4\", \"author_5\", \"author_6\"]\n",
        "expert_df[\"authors\"] = expert_df[author_cols].values.tolist()\n",
        "\n",
        "# Hapus duplikat berdasarkan research_id\n",
        "expert_df = expert_df.drop_duplicates(subset=[\"research_id\"]).copy()\n",
        "\n",
        "# Jalankan explode\n",
        "expert_df = explode_authors_with_weights(expert_df, dosen_id_map)\n",
        "\n",
        "# Opsional: hanya simpan baris dengan expert_id valid\n",
        "expert_df = expert_df[expert_df[\"expert_id\"].notna()]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "expert_df.to_csv('/content/drive/MyDrive/Skripsi4/dictionary/exploded.csv', index=False)"
      ],
      "metadata": {
        "id": "daj75VtjI_N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Skripsi4/dictionary/exploded.csv\")\n",
        "\n",
        "import re\n",
        "import ast\n",
        "\n",
        "def fix_vector(text):\n",
        "    if isinstance(text, str):\n",
        "        # Tambahkan koma antara angka yang dipisah spasi\n",
        "        text_fixed = re.sub(r'(?<=\\d)\\s+(?=\\d)', ', ', text.strip())\n",
        "        try:\n",
        "            return ast.literal_eval(text_fixed)\n",
        "        except:\n",
        "            return []\n",
        "    return text\n",
        "\n",
        "df[\"topic_vector\"] = df[\"topic_vector\"].apply(fix_vector)\n",
        "\n",
        "df[\"topic_vector\"].apply(lambda x: len(x)).value_counts()\n",
        "\n",
        "df.to_csv(\"/content/drive/MyDrive/Skripsi4/dictionary/exploded_clean.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "2YP_SVniKqJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pAGdpruELTI"
      },
      "source": [
        "## Directed M->D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BwQYxgFDHL6"
      },
      "outputs": [],
      "source": [
        "def compute_od_m_to_d(proposals_df, experts_df):\n",
        "    all_results = []\n",
        "\n",
        "    for _, proposal in proposals_df.iterrows():\n",
        "        proposal_vector = proposal[\"topic_vector\"]\n",
        "        mahasiswa = proposal[\"student_id\"]\n",
        "        id_proposal = proposal[\"proposal_id\"]\n",
        "        tahun_proposal = proposal[\"tahun\"]\n",
        "\n",
        "        for _, expert in expert_df.iterrows():\n",
        "            expert_vector = expert[\"topic_vector\"]\n",
        "            dosen = expert[\"name\"]\n",
        "            id_dosen = expert[\"expert_id\"]\n",
        "            id_penelitian = expert[\"research_id\"]\n",
        "            tahun_penelitian = expert[\"pub_year\"]\n",
        "            weight = expert.get(\"author_weight\")\n",
        "            position= expert[\"author_position\"]\n",
        "\n",
        "            sim_mahasiswa = similarity_m_to_d(proposal_vector, expert_vector)\n",
        "            score = sim_mahasiswa * weight\n",
        "\n",
        "            all_results.append({\n",
        "                \"id_proposal\": id_proposal,\n",
        "                \"id_penelitian\": id_penelitian,\n",
        "                \"mahasiswa\": mahasiswa,\n",
        "                \"dosen\": dosen,\n",
        "                \"id_dosen\": id_dosen,\n",
        "                \"posisi_author\": position,\n",
        "                \"author_weight\":weight,\n",
        "                \"tahun_proposal\": tahun_proposal,\n",
        "                \"tahun_penelitian\": tahun_penelitian,\n",
        "                \"OD(M→D)\": round(score, 4)\n",
        "            })\n",
        "    df_scores = pd.DataFrame(all_results)\n",
        "    # df_scores = df_scores.loc[df_scores.groupby([\"id_proposal\", \"id_dosen\"])[\"OD(M→D)\"].idxmax()].reset_index(drop=True)\n",
        "    df_scores = df_scores.sort_values(by=[\"id_proposal\", \"OD(M→D)\"], ascending=[True, False])\n",
        "    return df_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tc1RIoXLFNaR"
      },
      "outputs": [],
      "source": [
        "od_m2d_df = compute_od_m_to_d(proposal_df, expert_df)\n",
        "# print(od_m2d_df[[\"id_proposal\", \"mahasiswa\",\"id_penelitian\",\"dosen\",\"id_dosen\",\"posisi_author\",\"author_weight\", \"OD(M→D)\"]].head(30))\n",
        "\n",
        "od_m2d_df.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/30/od_m2d_df_topik.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5FkdQdcHvI2"
      },
      "source": [
        "## TOD M->D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUx5KCOsFSFm"
      },
      "outputs": [],
      "source": [
        "# Hitung TOD(M→D)\n",
        "def compute_tod_m_to_d(od_df, t=1, gamma=0.1, max_year_diff=5):\n",
        "    od_df = od_df.copy()\n",
        "\n",
        "    # Hitung selisih tahun\n",
        "    od_df[\"selisih_tahun\"] = od_df[\"tahun_proposal\"] - od_df[\"tahun_penelitian\"]\n",
        "\n",
        "    # Filter: proposal harus lebih baru dari publikasi, dan selisih maksimal 5 tahun\n",
        "    od_df = od_df[(od_df[\"selisih_tahun\"] >= 0) & (od_df[\"selisih_tahun\"] <= max_year_diff)].copy()\n",
        "\n",
        "    # Hitung time decay factor\n",
        "    od_df[\"time_decay_factor\"] = od_df.apply(\n",
        "        lambda row: time_decay(row[\"tahun_proposal\"], row[\"tahun_penelitian\"], t=t, gamma=gamma),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Hitung TOD(M→D)\n",
        "    od_df[\"TOD(M→D)\"] = (od_df[\"OD(M→D)\"] * od_df[\"time_decay_factor\"]).round(4)\n",
        "\n",
        "    return od_df\n",
        "\n",
        "\n",
        "\n",
        "# # Hitung TOD(M→D) sebelum di sortir\n",
        "# tod_m2d_before_sorted= compute_tod_m_to_d(od_m2d_df, t=1, gamma=0.1)\n",
        "\n",
        "# # Lihat contoh hasil\n",
        "# print(tod_m2d_before_sorted[[\"id_proposal\", \"id_penelitian\",\"dosen\",\"id_dosen\", \"OD(M→D)\", \"tahun_proposal\",\"tahun_penelitian\",\"selisih_tahun\", \"time_decay_factor\", \"TOD(M→D)\"]].head(5))\n",
        "\n",
        "# tod_m2d_before_sorted.to_csv(\"/kaggle/working/tod_m2d_before_sorted.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMgz9n1VHzFg"
      },
      "outputs": [],
      "source": [
        "# Hitung TOD(M→D) Disortir\n",
        "tod_m2d_df= compute_tod_m_to_d(od_m2d_df, t=1, gamma=0.1, max_year_diff=5)\n",
        "\n",
        "tod_m2d_df = (\n",
        "    tod_m2d_df\n",
        "    .loc[tod_m2d_df.groupby([\"id_proposal\", \"id_dosen\"])[\"TOD(M→D)\"].idxmax()]\n",
        "    .reset_index(drop=True)\n",
        "    .sort_values(by=[\"id_proposal\", \"TOD(M→D)\"], ascending=[True, False])\n",
        ")\n",
        "\n",
        "\n",
        "# Lihat contoh hasil\n",
        "\n",
        "# print(tod_m2d_df[[\"id_proposal\",\"mahasiswa\", \"id_penelitian\",\"dosen\",\"id_dosen\", \"OD(M→D)\", \"tahun_proposal\",\"tahun_penelitian\",\"selisih_tahun\", \"time_decay_factor\", \"TOD(M→D)\"]].head(5))\n",
        "\n",
        "# tod_m2d_df.to_csv(\"/kaggle/working/tod_m2d_df.csv\", index=False)\n",
        "tod_m2d_df.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/30/tod_m2d_df_topik.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GGdS8VRaUtG"
      },
      "outputs": [],
      "source": [
        "# dupes_todm_d = tod_m2d_df[tod_m2d_df.duplicated(subset=[\"id_proposal\",\"dosen\", \"TOD(M→D)\"], keep=False)]\n",
        "# print(dupes_todm_d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXJRYVfCIx5Z"
      },
      "source": [
        "# OD D->M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql8l9N2zYCoS"
      },
      "outputs": [],
      "source": [
        "def compute_od_d_to_m(proposals_df, experts_df):\n",
        "    results = []\n",
        "\n",
        "    for _, expert in experts_df.iterrows():\n",
        "        expert_vector = expert[\"topic_vector\"]\n",
        "        dosen = expert[\"name\"]\n",
        "        id_dosen = expert[\"expert_id\"]\n",
        "        id_penelitian = expert[\"research_id\"]\n",
        "        weight = expert[\"author_weight\"]\n",
        "        position = expert[\"author_position\"]\n",
        "        tahun_penelitian = expert[\"pub_year\"]\n",
        "\n",
        "        for _, proposal in proposals_df.iterrows():\n",
        "            proposal_vector = proposal[\"topic_vector\"]\n",
        "            mahasiswa = proposal[\"student_id\"]\n",
        "            id_proposal = proposal[\"proposal_id\"]\n",
        "            tahun_proposal = proposal[\"tahun\"]\n",
        "\n",
        "            # # Tambahkan filter tahun\n",
        "            selisih_tahun = tahun_proposal - tahun_penelitian\n",
        "            if 0 < selisih_tahun <= 5 and tahun_penelitian <= tahun_proposal:\n",
        "               sim_dosen = similarity_d_to_m(expert_vector, proposal_vector)\n",
        "               score = sim_dosen * weight\n",
        "               results.append({\n",
        "                    \"id_proposal\": id_proposal,\n",
        "                    \"id_penelitian\": id_penelitian,\n",
        "                    \"mahasiswa\": mahasiswa,\n",
        "                    \"dosen\": dosen,\n",
        "                    \"id_dosen\": id_dosen,\n",
        "                    \"posisi_author\": position,\n",
        "                    \"author_weight\": weight,\n",
        "                    \"tahun_proposal\": tahun_proposal,\n",
        "                    \"tahun_penelitian\": tahun_penelitian,\n",
        "                    \"OD(D→M)\": round(score, 4),\n",
        "                })\n",
        "\n",
        "    df_scores = pd.DataFrame(results)\n",
        "    df_scores = df_scores.loc[df_scores.groupby([\"id_proposal\",\"id_dosen\"])[\"OD(D→M)\"].idxmax()].reset_index(drop=True)\n",
        "    df_scores = df_scores.sort_values(by=[\"id_dosen\", \"OD(D→M)\"], ascending=[True, False])\n",
        "\n",
        "    return df_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SJKghECMbCs"
      },
      "outputs": [],
      "source": [
        "#Setelah di Sorted\n",
        "# od_d2m_df = (\n",
        "#     od_d2m_before_sorted\n",
        "#     .sort_values(by=[\"id_proposal\", \"OD(D→M)\"], ascending=[True, False])\n",
        "#     # .drop_duplicates(subset=[\"id_proposal\"], keep=\"first\")  #nnti comment ini\n",
        "#     .reset_index(drop=True)\n",
        "# )\n",
        "\n",
        "# Filter nilai di atas 0.3\n",
        "# od_d2m_df = od_d2m_df.loc[od_d2m_df.groupby([\"id_proposal\", \"id_dosen\"])[\"OD(D→M)\"].idxmax()].reset_index(drop=True).sort_values(by=[\"id_proposal\", \"OD(D→M)\"], ascending=[True, False])\n",
        "\n",
        "od_d2m_df = compute_od_d_to_m(proposal_df, expert_df)\n",
        "# print(od_d2m_df[[\"id_proposal\", \"mahasiswa\",\"id_penelitian\", \"dosen\", \"id_dosen\",\"posisi_author\",\"author_weight\",\"OD(D→M)\"]].head(5))\n",
        "\n",
        "# od_d2m_df.to_csv(\"/kaggle/working/od_d2m_df.csv\", index=False)\n",
        "od_d2m_df.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/30/od_d2m_df_topik.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLs6cYz8NWTv"
      },
      "source": [
        "# Overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QaOzHpwzMwpf"
      },
      "outputs": [],
      "source": [
        "# def combine_overlap_scores(df_m2d, df_d2m):\n",
        "#     # Ambil Top-N dari masing-masing arah\n",
        "#     top_m2d = df_m2d.groupby(\"id_proposal\")[[\"id_proposal\",\"mahasiswa\", \"id_dosen\", \"dosen\", \"TOD(M→D)\"]]\n",
        "#     top_d2m = df_d2m.groupby(\"id_proposal\")[[\"id_proposal\",\"mahasiswa\", \"id_dosen\", \"dosen\", \"OD(D→M)\"]]\n",
        "\n",
        "#     # Outer join untuk semua pasangan top-n\n",
        "#     merged = pd.merge(top_m2d, top_d2m, on=[\"id_proposal\", \"id_dosen\"], how=\"outer\")\n",
        "\n",
        "#     # Tambahkan kolom nama dosen jika hilang (dari M→D arah saja)\n",
        "#     if \"dosen_x\" in merged.columns:\n",
        "#         merged[\"dosen\"] = merged[\"dosen_x\"].combine_first(merged.get(\"dosen_y\"))\n",
        "#     elif \"dosen\" not in merged.columns:\n",
        "#         merged[\"dosen\"] = None\n",
        "\n",
        "#     if \"mahasiswa_x\" in merged.columns:\n",
        "#         merged[\"mahasiswa\"] = merged[\"mahasiswa_x\"].combine_first(merged.get(\"mahasiswa_y\"))\n",
        "#     elif \"mahasiswa\" not in merged.columns:\n",
        "#         merged[\"mahasiswa\"] = None\n",
        "\n",
        "\n",
        "#     # Ganti NaN skor dengan 0 agar bisa dihitung rata-ratanya\n",
        "#     merged[\"TOD(M→D)\"] = merged[\"TOD(M→D)\"].fillna(0)\n",
        "#     merged[\"OD(D→M)\"] = merged[\"OD(D→M)\"].fillna(0)\n",
        "\n",
        "#     # Tandai overlap jika dosen muncul di kedua arah\n",
        "#     merged[\"overlap\"] = (merged[\"TOD(M→D)\"] > 0) & (merged[\"OD(D→M)\"] > 0)\n",
        "\n",
        "#     # Hitung skor rata-rata (hanya jika overlap)\n",
        "#     merged[\"skor_rata2\"] = merged.apply(\n",
        "#         lambda row: (row[\"TOD(M→D)\"] + row[\"OD(D→M)\"]) / 2 if row[\"overlap\"] else 0, axis=1\n",
        "#     )\n",
        "\n",
        "#     # Ambil skor tertinggi per proposal dan dosen\n",
        "#     final_scores = merged.sort_values(by=[\"id_proposal\", \"skor_rata2\"], ascending=[True, False])\n",
        "#     final_scores = final_scores[[\"id_proposal\", \"mahasiswa\",\"dosen\", \"id_dosen\", \"TOD(M→D)\",\"OD(D→M)\", \"skor_rata2\", \"overlap\"]]\n",
        "\n",
        "#     return final_scores\n",
        "\n",
        "# df_final = combine_overlap_scores(tod_m2d_df, od_d2m_df)\n",
        "# # df_final = combine_overlap_scores(tod_m2d_before_sorted, od_d2m_before_sorted, top_n=10)\n",
        "\n",
        "# # idx = df_final.groupby([\"id_proposal\", \"id_dosen\"])[\"skor_rata2\"].idxmax()\n",
        "# # df_final = df_final.loc[idx]\n",
        "\n",
        "# # print(df_final.head(5))\n",
        "\n",
        "# df_final.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/30/overlap_directed_topik.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCttVVnEsjV7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def combine_overlap_scores(df_m2d, df_d2m):\n",
        "    # Ambil semua kolom yang dibutuhkan dari masing-masing arah\n",
        "    top_m2d = df_m2d[[\"id_proposal\", \"mahasiswa\", \"id_dosen\", \"dosen\", \"TOD(M→D)\"]]\n",
        "    top_d2m = df_d2m[[\"id_proposal\", \"mahasiswa\", \"id_dosen\", \"dosen\", \"OD(D→M)\"]]\n",
        "\n",
        "    # Gabungkan kedua dataframe berdasarkan id_proposal dan id_dosen\n",
        "    merged = pd.merge(top_m2d, top_d2m, on=[\"id_proposal\", \"id_dosen\"], how=\"outer\", suffixes=('_m2d', '_d2m'))\n",
        "\n",
        "    # Gabungkan kolom nama mahasiswa dan dosen (dari salah satu sisi)\n",
        "    merged[\"mahasiswa\"] = merged[\"mahasiswa_m2d\"].combine_first(merged[\"mahasiswa_d2m\"])\n",
        "    merged[\"dosen\"] = merged[\"dosen_m2d\"].combine_first(merged[\"dosen_d2m\"])\n",
        "\n",
        "    # Ganti NaN skor dengan 0\n",
        "    merged[\"TOD(M→D)\"] = merged[\"TOD(M→D)\"].fillna(0)\n",
        "    merged[\"OD(D→M)\"] = merged[\"OD(D→M)\"].fillna(0)\n",
        "\n",
        "    # Tandai overlap jika skor dari dua arah ada (lebih dari 0)\n",
        "    merged[\"overlap\"] = (merged[\"TOD(M→D)\"] > 0) & (merged[\"OD(D→M)\"] > 0)\n",
        "\n",
        "    # Hitung skor rata-rata hanya jika overlap, jika tidak maka 0\n",
        "    merged[\"skor_rata2\"] = merged.apply(\n",
        "        lambda row: (row[\"TOD(M→D)\"] + row[\"OD(D→M)\"]) / 2 if row[\"overlap\"] else 0, axis=1\n",
        "    )\n",
        "\n",
        "    # Urutkan berdasarkan skor rata-rata tertinggi untuk setiap proposal\n",
        "    final_scores = merged.sort_values(by=[\"id_proposal\", \"skor_rata2\"], ascending=[True, False])\n",
        "\n",
        "    # Pilih kolom akhir yang relevan\n",
        "    final_scores = final_scores[[\n",
        "        \"id_proposal\", \"mahasiswa\", \"dosen\", \"id_dosen\",\n",
        "        \"TOD(M→D)\", \"OD(D→M)\", \"skor_rata2\", \"overlap\"\n",
        "    ]]\n",
        "\n",
        "    return final_scores\n",
        "\n",
        "# Contoh penggunaan:\n",
        "df_final = combine_overlap_scores(tod_m2d_df, od_d2m_df)\n",
        "df_final.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/30/overlap_directed_topik.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65wj7aCgNlf3"
      },
      "source": [
        "# Ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsIetvPuNaPV"
      },
      "outputs": [],
      "source": [
        "def combine_overlap_scores_with_ranking(df_m2d, df_d2m):\n",
        "    # Ambil semua skor dari kedua arah\n",
        "    m2d = df_m2d[[\"id_proposal\", \"id_dosen\",\"mahasiswa\", \"dosen\", \"TOD(M→D)\"]]\n",
        "    d2m = df_d2m[[\"id_proposal\",\"id_dosen\",\"mahasiswa\",\"dosen\", \"OD(D→M)\"]]\n",
        "\n",
        "    # Outer join agar semua kombinasi muncul\n",
        "    merged = pd.merge(m2d, d2m, on=[\"id_proposal\", \"id_dosen\"], how=\"outer\")\n",
        "\n",
        "    # Tambahkan kolom nama dosen jika hilang (dari M→D arah saja)\n",
        "    if \"dosen_x\" in merged.columns:\n",
        "        merged[\"dosen\"] = merged[\"dosen_x\"].combine_first(merged.get(\"dosen_y\"))\n",
        "    elif \"dosen\" not in merged.columns:\n",
        "        merged[\"dosen\"] = None\n",
        "\n",
        "    if \"mahasiswa_x\" in merged.columns:\n",
        "        merged[\"mahasiswa\"] = merged[\"mahasiswa_x\"].combine_first(merged.get(\"mahasiswa_y\"))\n",
        "    elif \"mahasiswa\" not in merged.columns:\n",
        "        merged[\"mahasiswa\"] = None\n",
        "\n",
        "    # Isi nilai NaN dengan 0 untuk penggabungan skor\n",
        "    merged[\"TOD(M→D)\"] = merged[\"TOD(M→D)\"].fillna(0)\n",
        "    merged[\"OD(D→M)\"] = merged[\"OD(D→M)\"].fillna(0)\n",
        "\n",
        "    # Overlap = muncul di kedua arah\n",
        "    merged[\"overlap\"] = (merged[\"TOD(M→D)\"] > 0) & (merged[\"OD(D→M)\"] > 0)\n",
        "\n",
        "    # Skor rata-rata jika overlap\n",
        "    merged[\"skor_rata2\"] = merged.apply(\n",
        "        lambda row: (row[\"TOD(M→D)\"] + row[\"OD(D→M)\"]) / 2 if row[\"overlap\"] else 0, axis=1\n",
        "    )\n",
        "\n",
        "       # Hitung ranking per proposal berdasarkan skor rata-rata (tanpa groupby + agg)\n",
        "    merged[\"rank\"] = merged.groupby(\"id_proposal\")[\"skor_rata2\"]\\\n",
        "                           .rank(ascending=False, method=\"dense\")\\\n",
        "                           .astype(int)\n",
        "\n",
        "    # Ambil kolom yang diinginkan dan urutkan\n",
        "    result = merged.sort_values(by=[\"id_proposal\", \"rank\"])[\n",
        "        [\"id_proposal\",\"mahasiswa\", \"dosen\", \"id_dosen\", \"TOD(M→D)\", \"OD(D→M)\", \"skor_rata2\", \"overlap\", \"rank\"]\n",
        "    ]\n",
        "\n",
        "\n",
        "    return result.sort_values(by=[\"id_proposal\", \"rank\"])[\n",
        "        [\"id_proposal\",\"mahasiswa\", \"dosen\", \"id_dosen\", \"TOD(M→D)\", \"OD(D→M)\", \"skor_rata2\", \"overlap\", \"rank\"]\n",
        "    ]\n",
        "\n",
        "\n",
        "df_peringkat = combine_overlap_scores_with_ranking(tod_m2d_df, od_d2m_df)\n",
        "# Filter hanya yang overlap == True\n",
        "df_overlap_true = df_peringkat[df_peringkat[\"overlap\"] == True]\n",
        "# print(df_overlap_true.head(20))\n",
        "\n",
        "df_overlap_true.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/30/rank_overlap_true_kata.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CyWBkN1Nohe"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# 1. Inisialisasi count untuk rank 1\n",
        "rank1_count_directed = defaultdict(int)\n",
        "final_assignment_directed = []\n",
        "\n",
        "# 2. Tetapkan rank 1 dengan batas 15 kali per dosen\n",
        "for pid in df_overlap_true[\"id_proposal\"].unique():\n",
        "    candidates = df_overlap_true[df_overlap_true[\"id_proposal\"] == pid]\n",
        "    candidates = candidates.sort_values(by=\"skor_rata2\", ascending=False)\n",
        "\n",
        "    assigned_rank1 = False\n",
        "    for _, row in candidates.iterrows():\n",
        "        dosen_id = row[\"id_dosen\"]\n",
        "        if rank1_count_directed[dosen_id] < 15:\n",
        "            rank1_count_directed[dosen_id] += 1\n",
        "            row_data = row.to_dict()\n",
        "            row_data[\"rank\"] = 1\n",
        "            row_data[\"beban\"] = rank1_count_directed[dosen_id]\n",
        "            final_assignment_directed.append(row_data)\n",
        "            assigned_rank1 = True\n",
        "            break\n",
        "\n",
        "    if not assigned_rank1:\n",
        "        row = candidates.iloc[0].to_dict()\n",
        "        dosen_id = row[\"id_dosen\"]\n",
        "        rank1_count_directed[dosen_id] += 1\n",
        "        row[\"rank\"] = 1\n",
        "        row[\"beban\"] = rank1_count_directed[dosen_id]\n",
        "        final_assignment_directed.append(row)\n",
        "\n",
        "# 3. Buat dataframe dari rank 1\n",
        "rank1_directed = pd.DataFrame(final_assignment_directed)\n",
        "\n",
        "# 4. Tambahkan rank 2–17 berdasarkan similarity, excl. dosen yang sudah dipakai di rank 1 untuk proposal yang sama\n",
        "other_ranks = []\n",
        "\n",
        "for pid in df_overlap_true[\"id_proposal\"].unique():\n",
        "    # Dapatkan dosen yang sudah dipakai sebagai rank 1\n",
        "    used_dosen = rank1_directed[rank1_directed[\"id_proposal\"] == pid][\"id_dosen\"].tolist()\n",
        "\n",
        "    # Ambil kandidat lain untuk proposal ini\n",
        "    candidates = df_overlap_true[(df_overlap_true[\"id_proposal\"] == pid) & (~df_overlap_true[\"id_dosen\"].isin(used_dosen))]\n",
        "    candidates = candidates.sort_values(by=\"skor_rata2\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    for idx, (_, row) in enumerate(candidates.iterrows(), start=2):\n",
        "        if idx > 17:\n",
        "            break\n",
        "        row_data = row.to_dict()\n",
        "        row_data[\"rank\"] = idx\n",
        "        row_data[\"beban\"] = rank1_count_directed[row_data[\"id_dosen\"]]  # Beban hanya dihitung dari rank 1\n",
        "        other_ranks.append(row_data)\n",
        "\n",
        "# 5. Gabungkan rank1 dan other ranks\n",
        "df_ranked_filtered = pd.concat([rank1_directed, pd.DataFrame(other_ranks)], ignore_index=True)\n",
        "df_ranked_filtered = df_ranked_filtered.sort_values(by=[\"id_proposal\", \"rank\"])\n",
        "\n",
        "# 🔟 Filter hanya Top 10 dosen per proposal\n",
        "df_ranked_filtered = df_ranked_filtered[df_ranked_filtered[\"rank\"] <= 17]\n",
        "\n",
        "# Print the DataFrame\n",
        "# print(df_ranked_filtered)\n",
        "\n",
        "# df_ranked_filtered.to_csv(\"/kaggle/working/df_ranked_filtered_coba.csv\", index=False)\n",
        "df_ranked_filtered.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/30/beban_directed_topik.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY5LgnXwOAoi"
      },
      "source": [
        "# Evaluasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWMQJ-jIOM6q"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# Load the true labels DataFrame\n",
        "true_label_df = pd.read_csv(\"/content/drive/MyDrive/Skripsi3/Dataset/true_labels.csv\")\n",
        "\n",
        "# Gabungkan label benar menjadi list\n",
        "# true_label_df[\"true_dosens\"] = true_label_df.apply(lambda row: [\n",
        "#     row[\"author1\"], row[\"author2\"], row[\"author3\"]\n",
        "# ], axis=1)\n",
        "\n",
        "\n",
        "# Ubah kolom author, author2, author3 menjadi lowercase\n",
        "true_label_df[\"author1\"] = true_label_df[\"examiner_1\"].astype(str).str.strip()\n",
        "true_label_df[\"author2\"] = true_label_df[\"examiner_2\"].astype(str).str.strip()\n",
        "true_label_df[\"author3\"] = true_label_df[\"examiner_3\"].astype(str).str.strip()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISliac_wWuZb"
      },
      "source": [
        "coba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skG7yYjbKUlO"
      },
      "source": [
        "## Evaluasi Baru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks2XbNJ-Wt_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1012a550-de75-4710-dd42-6e76b0fddb69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Top-N  Mean_Recall_Existence  Recall_Pos_1_Ordered  Recall_Pos_2_Ordered  \\\n",
            "0      3               0.340376              0.380282              0.056338   \n",
            "1      5               0.511737              0.380282              0.056338   \n",
            "2      7               0.636150              0.380282              0.056338   \n",
            "3     10               0.781690              0.380282              0.056338   \n",
            "\n",
            "   Recall_Pos_3_Ordered  Avg_Normalized_Euclidean  \n",
            "0              0.091549                  0.822388  \n",
            "1              0.091549                  0.710031  \n",
            "2              0.091549                  0.625719  \n",
            "3              0.091549                  0.555407  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Contoh struktur dummy data untuk demonstrasi perbaikan fungsi (tidak dijalankan secara nyata di sini)\n",
        "# similarity_cosine_df = pd.read_csv(...)  # Format: id_proposal, rank, similarity_score_akhir, dosen\n",
        "# true_label_df = pd.read_csv(...)  # Format: proposal_id, author1, author2, author3\n",
        "\n",
        "def evaluate_ordered_recommendation_directed(rank_all_df, true_label_df, top_ns=[3, 5, 7, 10]):\n",
        "    summary = []\n",
        "\n",
        "    for TOP_N in top_ns:\n",
        "        # Filter Top-N dan urutkan\n",
        "        top_n_df = rank_all_df[rank_all_df[\"rank\"] <= TOP_N]\n",
        "        rec_df = top_n_df.sort_values(by=[\"id_proposal\", \"rank\", \"skor_rata2\"], ascending=[True, True, False])\n",
        "        rec_df = rec_df.drop_duplicates(subset=[\"id_proposal\", \"rank\",\"skor_rata2\"])\n",
        "        rec_pivot = rec_df.pivot(index=\"id_proposal\", columns=\"rank\", values=\"dosen\").reset_index()\n",
        "        rec_pivot.columns.name = None\n",
        "        rec_pivot.columns = [\"id_proposal\"] + [f\"rec_{i}\" for i in range(1, len(rec_pivot.columns))]\n",
        "\n",
        "        # Gabungkan dengan ground truth\n",
        "        merged_df = pd.merge(\n",
        "            rec_pivot,\n",
        "            true_label_df.rename(columns={\"proposal_id\": \"id_proposal\"}),\n",
        "            on=\"id_proposal\",\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Recall keberadaan (tidak memperhatikan urutan)\n",
        "        def recall_of_existence(row):\n",
        "            true_set = {row.get(\"author1\"), row.get(\"author2\"), row.get(\"author3\")}\n",
        "            pred_set = {row.get(f\"rec_{i}\") for i in range(1, TOP_N + 1) if row.get(f\"rec_{i}\")}\n",
        "            return len(true_set.intersection(pred_set)) / 3\n",
        "\n",
        "        merged_df[f'recall_of_existence@{TOP_N}'] = merged_df.apply(recall_of_existence, axis=1)\n",
        "\n",
        "        # Recall per posisi dengan urutan diperhatikan (rec_i harus sama dengan author_i)\n",
        "        recall_pos = {1: [], 2: [], 3: []}\n",
        "        for _, row in merged_df.iterrows():\n",
        "            for pos in [1, 2, 3]:\n",
        "                examiner = row.get(f'author{pos}')\n",
        "                rec = row.get(f'rec_{pos}') if pos <= TOP_N else None\n",
        "                hit = int(pd.notna(examiner) and pd.notna(rec) and examiner == rec)\n",
        "                recall_pos[pos].append(hit)\n",
        "\n",
        "        # Tambahkan recall ke DataFrame\n",
        "        for pos in [1, 2, 3]:\n",
        "            merged_df[f'recall_pos{pos}_ordered@{TOP_N}'] = recall_pos[pos]\n",
        "\n",
        "        recall_pos_mean = {pos: np.mean(recall_pos[pos]) for pos in [1, 2, 3]}\n",
        "\n",
        "        # Euclidean distance antar posisi (penalti posisi meleset)\n",
        "        distances = []\n",
        "        for _, row in merged_df.iterrows():\n",
        "            true_authors = [row.get(f'author{i}') for i in [1, 2, 3]]\n",
        "            pred_authors = [row.get(f'rec_{i}', None) for i in range(1, TOP_N + 1)]\n",
        "            distance = 0\n",
        "            max_penalty = TOP_N\n",
        "            for i, true_author in enumerate(true_authors):\n",
        "                if pd.isna(true_author) or true_author == '':\n",
        "                    continue\n",
        "                try:\n",
        "                    pred_pos = pred_authors.index(true_author)\n",
        "                    pos_diff = pred_pos - i\n",
        "                    distance += pos_diff ** 2\n",
        "                except ValueError:\n",
        "                    distance += max_penalty ** 2\n",
        "            distances.append(np.sqrt(distance))\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        norm_dists = scaler.fit_transform(np.array(distances).reshape(-1, 1)).flatten()\n",
        "        merged_df[f'norm_euclidean@{TOP_N}'] = norm_dists\n",
        "\n",
        "        # Ringkasan metrik\n",
        "        summary.append({\n",
        "            'Top-N': TOP_N,\n",
        "            'Mean_Recall_Existence': merged_df[f'recall_of_existence@{TOP_N}'].mean(),\n",
        "            'Recall_Pos_1_Ordered': recall_pos_mean[1],\n",
        "            'Recall_Pos_2_Ordered': recall_pos_mean[2],\n",
        "            'Recall_Pos_3_Ordered': recall_pos_mean[3],\n",
        "            'Avg_Normalized_Euclidean': np.mean(norm_dists)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(summary)\n",
        "\n",
        "result_df_directed = evaluate_ordered_recommendation_directed(df_overlap_true, true_label_df)\n",
        "# result_df_directed = evaluate_ordered_recommendation_directed(df_ranked_filtered, true_label_df)\n",
        "print(result_df_directed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YrQO7Qbasqb"
      },
      "source": [
        "## Evaluasi per proposal Baru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7h0LFgnasJe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def evaluate_per_proposal_directed(rank_all_df, true_label_df, top_n=3):\n",
        "    # Filter dan urutkan\n",
        "    top_n_df = rank_all_df[rank_all_df[\"rank\"] <= top_n]\n",
        "    rec_df = top_n_df.sort_values(by=[\"id_proposal\", \"rank\", \"skor_rata2\"], ascending=[True, True, False])\n",
        "    rec_df = rec_df.drop_duplicates(subset=[\"id_proposal\", \"rank\",\"skor_rata2\"])\n",
        "    rec_pivot = rec_df.pivot(index=\"id_proposal\", columns=\"rank\", values=\"dosen\").reset_index()\n",
        "    rec_pivot.columns.name = None\n",
        "    rec_pivot.columns = [\"id_proposal\"] + [f\"rec_{i}\" for i in range(1, len(rec_pivot.columns))]\n",
        "\n",
        "    # Gabung dengan label kebenaran\n",
        "    merged_df = pd.merge(\n",
        "        rec_pivot,\n",
        "        true_label_df.rename(columns={\"proposal_id\": \"id_proposal\"}),\n",
        "        on=\"id_proposal\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    # Recall of existence (abaikan urutan)\n",
        "    def recall_of_existence(row):\n",
        "        true_set = {row.get(\"author1\"), row.get(\"author2\"), row.get(\"author3\")}\n",
        "        pred_set = {row.get(f\"rec_{i}\") for i in range(1, top_n + 1) if row.get(f\"rec_{i}\")}\n",
        "        return len(true_set.intersection(pred_set)) / 3\n",
        "\n",
        "    merged_df[f'recall_of_existence@{top_n}'] = merged_df.apply(recall_of_existence, axis=1)\n",
        "\n",
        "    # Recall berdasarkan posisi (urutan harus sama)\n",
        "    for pos in [1, 2, 3]:\n",
        "        merged_df[f'recall_pos{pos}_ordered@{top_n}'] = merged_df.apply(\n",
        "            lambda row: int(\n",
        "                pd.notna(row.get(f'author{pos}')) and\n",
        "                pd.notna(row.get(f'rec_{pos}')) and\n",
        "                row.get(f'author{pos}') == row.get(f'rec_{pos}')\n",
        "            ) if pos <= top_n else 0,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    # Euclidean distance penalti posisi\n",
        "    distances = []\n",
        "    for _, row in merged_df.iterrows():\n",
        "        true_authors = [row.get(f'author{i}') for i in [1, 2, 3]]\n",
        "        pred_authors = [row.get(f'rec_{i}', None) for i in range(1, top_n + 1)]\n",
        "        distance = 0\n",
        "        max_penalty = top_n\n",
        "        for i, true_author in enumerate(true_authors):\n",
        "            if pd.isna(true_author) or true_author == '':\n",
        "                continue\n",
        "            try:\n",
        "                pred_pos = pred_authors.index(true_author)\n",
        "                pos_diff = pred_pos - i\n",
        "                distance += pos_diff ** 2\n",
        "            except ValueError:\n",
        "                distance += max_penalty ** 2\n",
        "        distances.append(np.sqrt(distance))\n",
        "\n",
        "    # Normalisasi jarak\n",
        "    scaler = MinMaxScaler()\n",
        "    norm_dists = scaler.fit_transform(np.array(distances).reshape(-1, 1)).flatten()\n",
        "    merged_df[f'norm_euclidean@{top_n}'] = norm_dists\n",
        "\n",
        "    # Ambil kolom evaluasi\n",
        "    result_df = merged_df[[\"id_proposal\",\n",
        "                           f'recall_of_existence@{top_n}',\n",
        "                           f'recall_pos1_ordered@{top_n}',\n",
        "                           f'recall_pos2_ordered@{top_n}',\n",
        "                           f'recall_pos3_ordered@{top_n}',\n",
        "                           f'norm_euclidean@{top_n}']].copy()\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "eval_per_proposal_directed_3 = evaluate_per_proposal_directed(df_overlap_true, true_label_df, top_n=3)\n",
        "eval_per_proposal_directed_5 = evaluate_per_proposal_directed(df_overlap_true, true_label_df, top_n=5)\n",
        "eval_per_proposal_directed_7 = evaluate_per_proposal_directed(df_overlap_true, true_label_df, top_n=7)\n",
        "eval_per_proposal_directed_10 = evaluate_per_proposal_directed(df_overlap_true, true_label_df, top_n=10)\n",
        "\n",
        "eval_per_proposal_directed_3.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/30/hasil_eval_3_directed.csv\", index=False)\n",
        "eval_per_proposal_directed_5.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/30/hasil_eval_5_directed.csv\", index=False)\n",
        "eval_per_proposal_directed_7.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/30/hasil_eval_7_directed.csv\", index=False)\n",
        "eval_per_proposal_directed_10.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/directed/30/hasil_eval_10_directed.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc463fO8YsFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8afff6ff-9771-467e-9944-eb1b1ed57bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_proposal mahasiswa                   dosen id_dosen  TOD(M→D)  \\\n",
            "54          P101      S101         Haryono Setiadi      D14    0.0237   \n",
            "57          P101      S101  Brilyan Hendrasuryawan      D17    0.0237   \n",
            "111         P104      S104      Dewi Wisnu Wardani       D4    0.0876   \n",
            "115         P104      S104            Esti Suryani       D8    0.0876   \n",
            "104         P104      S104       Hasan Dwi Cahyono      D13    0.0510   \n",
            "105         P104      S104         Haryono Setiadi      D14    0.0510   \n",
            "344         P118      S118         Bambang Harjito       D1    0.0842   \n",
            "346         P118      S118                 Winarno      D11    0.0842   \n",
            "1145         P35       S35         Bambang Harjito       D1    0.0844   \n",
            "1147         P35       S35                 Winarno      D11    0.0844   \n",
            "1177         P37       S37         Bambang Harjito       D1    0.0841   \n",
            "1179         P37       S37                 Winarno      D11    0.0841   \n",
            "1275         P42       S42                 Winarno      D11    0.0484   \n",
            "1281         P42       S42                 Wiranto       D2    0.0484   \n",
            "1515         P56       S56                 Winarno      D11    0.0192   \n",
            "1521         P56       S56                 Wiranto       D2    0.0192   \n",
            "1670         P64       S64      Dewi Wisnu Wardani       D4    0.0979   \n",
            "1674         P64       S64            Esti Suryani       D8    0.0979   \n",
            "\n",
            "      OD(D→M)  skor_rata2  overlap  rank  \n",
            "54     0.0669     0.04530     True    10  \n",
            "57     0.0669     0.04530     True    10  \n",
            "111    0.1859     0.13675     True     8  \n",
            "115    0.1859     0.13675     True     8  \n",
            "104    0.1143     0.08265     True    10  \n",
            "105    0.1143     0.08265     True    10  \n",
            "344    0.1882     0.13620     True    10  \n",
            "346    0.1882     0.13620     True    10  \n",
            "1145   0.1777     0.13105     True    10  \n",
            "1147   0.1777     0.13105     True    10  \n",
            "1177   0.1957     0.13990     True    10  \n",
            "1179   0.1957     0.13990     True    10  \n",
            "1275   0.0785     0.06345     True     8  \n",
            "1281   0.0785     0.06345     True     8  \n",
            "1515   0.0442     0.03170     True    10  \n",
            "1521   0.0442     0.03170     True    10  \n",
            "1670   0.1486     0.12325     True     9  \n",
            "1674   0.1486     0.12325     True     9  \n"
          ]
        }
      ],
      "source": [
        "dupes = df_overlap_true[df_overlap_true.duplicated(subset=[\"id_proposal\", \"rank\"], keep=False)]\n",
        "print(dupes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vzt-RHMSsRw"
      },
      "source": [
        "# Cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t6VT0-ROwjb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a987e045-b41a-4db2-9f32-27a52492640a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.02116158 0.19097795 0.1399585  ... 0.02030642 0.02484113 0.05370035]\n",
            " [0.02116158 0.19097795 0.1399585  ... 0.02030642 0.02484113 0.05370035]\n",
            " [0.01516028 0.02125837 0.02516963 ... 0.06561315 0.05264645 0.05729351]\n",
            " ...\n",
            " [0.14608201 0.02545677 0.09075104 ... 0.03309006 0.03286479 0.05853668]\n",
            " [0.20352237 0.02324828 0.07729209 ... 0.03353393 0.0428044  0.05218332]\n",
            " [0.11509452 0.01096047 0.0180661  ... 0.02063077 0.02465363 0.02465597]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Konversi kolom topic_vector menjadi 2D array\n",
        "expert_vectors = np.vstack(expert_df[\"topic_vector\"].values)\n",
        "proposal_vectors = np.vstack(proposal_df[\"topic_vector\"].values)\n",
        "\n",
        "# Hitung similarity\n",
        "similarity_matrix = cosine_similarity(expert_vectors, proposal_vectors)\n",
        "\n",
        "print(similarity_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd3oic5wS5KM"
      },
      "outputs": [],
      "source": [
        "similarity_cosine_df = []\n",
        "\n",
        "for i, (_, mahasiswa) in enumerate(proposal_df.iterrows()):\n",
        "    for j, (_, dosen) in enumerate(expert_df.iterrows()):\n",
        "        weight = dosen.get(\"author_weight\")\n",
        "        similarity_cosine_df.append({\n",
        "            \"id_proposal\": mahasiswa[\"proposal_id\"],\n",
        "            \"mahasiswa\": mahasiswa[\"student_id\"],\n",
        "            \"id_dosen\": dosen[\"expert_id\"],\n",
        "            \"id_penelitian\": dosen[\"research_id\"],\n",
        "            \"tahun_proposal\": mahasiswa[\"tahun\"],\n",
        "            \"tahun_penelitian\": dosen[\"pub_year\"],\n",
        "            \"selisih_tahun\": mahasiswa[\"tahun\"] - dosen[\"pub_year\"],\n",
        "            \"dosen\": dosen[\"name\"],\n",
        "           \"author_position\": dosen[\"author_position\"],\n",
        "            \"weight\": weight,\n",
        "            \"similarity_score\" : similarity_matrix[j, i]\n",
        "\n",
        "\n",
        "        })\n",
        "\n",
        "similarity_cosine_df = pd.DataFrame(similarity_cosine_df)\n",
        "\n",
        "# Filter sesuai kondisi\n",
        "similarity_cosine_df = similarity_cosine_df[\n",
        "    (similarity_cosine_df[\"tahun_proposal\"] > similarity_cosine_df[\"tahun_penelitian\"]) &\n",
        "    (similarity_cosine_df[\"selisih_tahun\"] <= 5)\n",
        "].copy()\n",
        "\n",
        "\n",
        "# Tampilkan hasil\n",
        "# print(similarity_cosine_df[[\"id_proposal\",\"mahasiswa\",\"id_penelitian\", \"id_dosen\",\"selisih_tahun\", \"dosen\",\"weight\", \"similarity_score\" ,\"similarity_score_akhir\",]])\n",
        "# similarity_cosine_df.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/cosine/14/hasil_cosine_topik.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGX5xEtNUpcw"
      },
      "source": [
        "# Pemeringkatan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7shoY9CES9Tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e842df-9e28-4f82-80bf-ba2b4c3644db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id_proposal mahasiswa  tahun_proposal id_penelitian  tahun_penelitian  \\\n",
            "137            P1        S1            2021           R83              2016   \n",
            "138            P1        S1            2021           R83              2016   \n",
            "163            P1        S1            2021           R97              2018   \n",
            "164            P1        S1            2021           R97              2018   \n",
            "84             P1        S1            2021           R48              2018   \n",
            "...           ...       ...             ...           ...               ...   \n",
            "74550         P99       S99            2024           R42              2023   \n",
            "74815         P99       S99            2024          R202              2023   \n",
            "74816         P99       S99            2024          R202              2023   \n",
            "74685         P99       S99            2024          R116              2021   \n",
            "74686         P99       S99            2024          R116              2021   \n",
            "\n",
            "      id_dosen              dosen  selisih_tahun  weight  similarity_score  \\\n",
            "137         D8       Esti Suryani              5     0.6          0.765764   \n",
            "138         D5         Abdul Aziz              5     0.4          0.765764   \n",
            "163         D2            Wiranto              3     0.6          0.754174   \n",
            "164         D8       Esti Suryani              3     0.4          0.754174   \n",
            "84          D8       Esti Suryani              3     0.4          0.657577   \n",
            "...        ...                ...            ...     ...               ...   \n",
            "74550      D11            Winarno              1     0.4          0.563516   \n",
            "74815      D11            Winarno              1     0.6          0.558069   \n",
            "74816      D13  Hasan Dwi Cahyono              1     0.4          0.558069   \n",
            "74685      D12            Wiharto              3     0.6          0.416476   \n",
            "74686       D8       Esti Suryani              3     0.2          0.416476   \n",
            "\n",
            "       rank  \n",
            "137       1  \n",
            "138       2  \n",
            "163       3  \n",
            "164       4  \n",
            "84        5  \n",
            "...     ...  \n",
            "74550    13  \n",
            "74815    14  \n",
            "74816    15  \n",
            "74685    16  \n",
            "74686    17  \n",
            "\n",
            "[2414 rows x 11 columns]\n"
          ]
        }
      ],
      "source": [
        "# Ambil baris dengan similarity tertinggi untuk kombinasi unik id_proposal dan id_dosen\n",
        "idx = similarity_cosine_df.groupby([\"id_proposal\", \"id_dosen\"])[\"similarity_score\"].idxmax()\n",
        "similarity_cosine = similarity_cosine_df.loc[idx]\n",
        "\n",
        "# Ranking ulang berdasarkan proposal\n",
        "similarity_cosine_df[\"rank\"] = similarity_cosine_df.groupby(\"id_proposal\")[\"similarity_score\"] \\\n",
        "                                     .rank(method=\"first\", ascending=False).astype(int)\n",
        "\n",
        "# Urutkan\n",
        "similarity_cosine_df = similarity_cosine_df.sort_values([\"id_proposal\", \"rank\"])\n",
        "similarity_cosine_df = similarity_cosine_df[similarity_cosine_df[\"rank\"] <= 17]\n",
        "\n",
        "\n",
        "# Tampilkan\n",
        "print(similarity_cosine_df[[\"id_proposal\", \"mahasiswa\",\"tahun_proposal\",\"id_penelitian\", \"tahun_penelitian\", \"id_dosen\", \"dosen\",\"selisih_tahun\", \"weight\",\"similarity_score\",\"rank\"]])\n",
        "# similarity_cosine_df.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/14/cosine/similarity_cosine_kata_baru_setelah filter.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EFjJdmrVV34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d999a99-a7a4-432e-813f-7e540c2d00e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_proposal mahasiswa id_dosen              dosen  similarity_score  \\\n",
            "0             P1        S1       D8       Esti Suryani          0.765764   \n",
            "142           P1        S1       D5         Abdul Aziz          0.765764   \n",
            "143           P1        S1       D2            Wiranto          0.754174   \n",
            "144           P1        S1       D1    Bambang Harjito          0.657577   \n",
            "145           P1        S1       D3        Umi Salamah          0.579853   \n",
            "...          ...       ...      ...                ...               ...   \n",
            "2009         P99       S99      D11            Winarno          0.563516   \n",
            "2010         P99       S99      D11            Winarno          0.558069   \n",
            "2011         P99       S99      D13  Hasan Dwi Cahyono          0.558069   \n",
            "2012         P99       S99      D12            Wiharto          0.416476   \n",
            "2013         P99       S99       D8       Esti Suryani          0.416476   \n",
            "\n",
            "      rank  beban  \n",
            "0        1      1  \n",
            "142      2      5  \n",
            "143      3     13  \n",
            "144      4      6  \n",
            "145      5      6  \n",
            "...    ...    ...  \n",
            "2009    13     14  \n",
            "2010    14     14  \n",
            "2011    15      2  \n",
            "2012    16     14  \n",
            "2013    17     15  \n",
            "\n",
            "[2014 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# 1. Inisialisasi count untuk rank 1\n",
        "rank1_count = defaultdict(int)\n",
        "final_assignments = []\n",
        "\n",
        "# 2. Tetapkan rank 1 dengan batas 15 kali per dosen\n",
        "for pid in similarity_cosine_df[\"id_proposal\"].unique():\n",
        "    candidates = similarity_cosine_df[similarity_cosine_df[\"id_proposal\"] == pid]\n",
        "    candidates = candidates.sort_values(by=\"similarity_score\", ascending=False)\n",
        "\n",
        "    assigned_rank1 = False\n",
        "    for _, row in candidates.iterrows():\n",
        "        dosen_id = row[\"id_dosen\"]\n",
        "        if rank1_count[dosen_id] < 15:\n",
        "            rank1_count[dosen_id] += 1\n",
        "            row_data = row.to_dict()\n",
        "            row_data[\"rank\"] = 1\n",
        "            row_data[\"beban\"] = rank1_count[dosen_id]\n",
        "            final_assignments.append(row_data)\n",
        "            assigned_rank1 = True\n",
        "            break\n",
        "\n",
        "    if not assigned_rank1:\n",
        "        row = candidates.iloc[0].to_dict()\n",
        "        dosen_id = row[\"id_dosen\"]\n",
        "        rank1_count[dosen_id] += 1\n",
        "        row[\"rank\"] = 1\n",
        "        row[\"beban\"] = rank1_count[dosen_id]\n",
        "        final_assignments.append(row)\n",
        "\n",
        "# 3. Buat dataframe dari rank 1\n",
        "rank1_df = pd.DataFrame(final_assignments)\n",
        "\n",
        "# 4. Tambahkan rank 2–17 berdasarkan similarity, excl. dosen yang sudah dipakai di rank 1 untuk proposal yang sama\n",
        "other_ranks = []\n",
        "\n",
        "for pid in similarity_cosine_df[\"id_proposal\"].unique():\n",
        "    # Dapatkan dosen yang sudah dipakai sebagai rank 1\n",
        "    used_dosen = rank1_df[rank1_df[\"id_proposal\"] == pid][\"id_dosen\"].tolist()\n",
        "\n",
        "    # Ambil kandidat lain untuk proposal ini\n",
        "    candidates = similarity_cosine_df[(similarity_cosine_df[\"id_proposal\"] == pid) & (~similarity_cosine_df[\"id_dosen\"].isin(used_dosen))]\n",
        "    candidates = candidates.sort_values(by=\"similarity_score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    for idx, (_, row) in enumerate(candidates.iterrows(), start=2):\n",
        "        if idx > 17:\n",
        "            break\n",
        "        row_data = row.to_dict()\n",
        "        row_data[\"rank\"] = idx\n",
        "        row_data[\"beban\"] = rank1_count[row_data[\"id_dosen\"]]  # Beban hanya dihitung dari rank 1\n",
        "        other_ranks.append(row_data)\n",
        "\n",
        "# 5. Gabungkan rank1 dan other ranks\n",
        "rank_all_df = pd.concat([rank1_df, pd.DataFrame(other_ranks)], ignore_index=True)\n",
        "rank_all_df = rank_all_df.sort_values(by=[\"id_proposal\", \"rank\"])\n",
        "\n",
        "# 6. Tampilkan hasil\n",
        "print(rank_all_df[[\"id_proposal\", \"mahasiswa\", \"id_dosen\", \"dosen\", \"similarity_score\", \"rank\", \"beban\"]])\n",
        "rank_all_df.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/cosine/30/beban_cosine_topik.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDpaTPt7VlSU"
      },
      "source": [
        "# Evaluasi Cosine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ11PrKrWfQX"
      },
      "source": [
        "coba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-1fxi4gKcPw"
      },
      "source": [
        "## Evaluasi Baru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyEdj1o1Szkf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862cea74-36d0-4cce-de69-0b9804e362eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Top-N  Mean_Recall_Existence  Recall_Pos_1_Ordered  Recall_Pos_2_Ordered  \\\n",
            "0      3               0.244131              0.309859              0.021127   \n",
            "1      5               0.380282              0.309859              0.021127   \n",
            "2      7               0.488263              0.309859              0.021127   \n",
            "3     10               0.551643              0.309859              0.021127   \n",
            "\n",
            "   Recall_Pos_3_Ordered  Avg_Normalized_Euclidean  \n",
            "0              0.070423                  0.710102  \n",
            "1              0.070423                  0.686285  \n",
            "2              0.070423                  0.647736  \n",
            "3              0.070423                  0.616253  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Contoh struktur dummy data untuk demonstrasi perbaikan fungsi (tidak dijalankan secara nyata di sini)\n",
        "# similarity_cosine_df = pd.read_csv(...)  # Format: id_proposal, rank, similarity_score_akhir, dosen\n",
        "# true_label_df = pd.read_csv(...)  # Format: proposal_id, author1, author2, author3\n",
        "\n",
        "def evaluate_ordered_recommendation_cosine(rank_all_df, true_label_df, top_ns=[3, 5, 7, 10]):\n",
        "    summary = []\n",
        "\n",
        "    for TOP_N in top_ns:\n",
        "        # Filter Top-N dan urutkan\n",
        "        top_n_df = rank_all_df[rank_all_df[\"rank\"] <= TOP_N]\n",
        "        rec_df = top_n_df.sort_values(by=[\"id_proposal\", \"rank\", \"similarity_score\"], ascending=[True, True, False])\n",
        "        rec_pivot = rec_df.pivot(index=\"id_proposal\", columns=\"rank\", values=\"dosen\").reset_index()\n",
        "        rec_pivot.columns.name = None\n",
        "        rec_pivot.columns = [\"id_proposal\"] + [f\"rec_{i}\" for i in range(1, len(rec_pivot.columns))]\n",
        "\n",
        "        # Gabungkan dengan ground truth\n",
        "        merged_df = pd.merge(\n",
        "            rec_pivot,\n",
        "            true_label_df.rename(columns={\"proposal_id\": \"id_proposal\"}),\n",
        "            on=\"id_proposal\",\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Recall keberadaan (tidak memperhatikan urutan)\n",
        "        def recall_of_existence(row):\n",
        "            true_set = {row.get(\"author1\"), row.get(\"author2\"), row.get(\"author3\")}\n",
        "            pred_set = {row.get(f\"rec_{i}\") for i in range(1, TOP_N + 1) if row.get(f\"rec_{i}\")}\n",
        "            return len(true_set.intersection(pred_set)) / 3\n",
        "\n",
        "        merged_df[f'recall_of_existence@{TOP_N}'] = merged_df.apply(recall_of_existence, axis=1)\n",
        "\n",
        "        # Recall per posisi dengan urutan diperhatikan (rec_i harus sama dengan author_i)\n",
        "        recall_pos = {1: [], 2: [], 3: []}\n",
        "        for _, row in merged_df.iterrows():\n",
        "            for pos in [1, 2, 3]:\n",
        "                examiner = row.get(f'author{pos}')\n",
        "                rec = row.get(f'rec_{pos}') if pos <= TOP_N else None\n",
        "                hit = int(pd.notna(examiner) and pd.notna(rec) and examiner == rec)\n",
        "                recall_pos[pos].append(hit)\n",
        "\n",
        "        # Tambahkan recall ke DataFrame\n",
        "        for pos in [1, 2, 3]:\n",
        "            merged_df[f'recall_pos{pos}_ordered@{TOP_N}'] = recall_pos[pos]\n",
        "\n",
        "        recall_pos_mean = {pos: np.mean(recall_pos[pos]) for pos in [1, 2, 3]}\n",
        "\n",
        "        # Euclidean distance antar posisi (penalti posisi meleset)\n",
        "        distances = []\n",
        "        for _, row in merged_df.iterrows():\n",
        "            true_authors = [row.get(f'author{i}') for i in [1, 2, 3]]\n",
        "            pred_authors = [row.get(f'rec_{i}', None) for i in range(1, TOP_N + 1)]\n",
        "            distance = 0\n",
        "            max_penalty = TOP_N\n",
        "            for i, true_author in enumerate(true_authors):\n",
        "                if pd.isna(true_author) or true_author == '':\n",
        "                    continue\n",
        "                try:\n",
        "                    pred_pos = pred_authors.index(true_author)\n",
        "                    pos_diff = pred_pos - i\n",
        "                    distance += pos_diff ** 2\n",
        "                except ValueError:\n",
        "                    distance += max_penalty ** 2\n",
        "            distances.append(np.sqrt(distance))\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        norm_dists = scaler.fit_transform(np.array(distances).reshape(-1, 1)).flatten()\n",
        "        merged_df[f'norm_euclidean@{TOP_N}'] = norm_dists\n",
        "\n",
        "        # Ringkasan metrik\n",
        "        summary.append({\n",
        "            'Top-N': TOP_N,\n",
        "            'Mean_Recall_Existence': merged_df[f'recall_of_existence@{TOP_N}'].mean(),\n",
        "            'Recall_Pos_1_Ordered': recall_pos_mean[1],\n",
        "            'Recall_Pos_2_Ordered': recall_pos_mean[2],\n",
        "            'Recall_Pos_3_Ordered': recall_pos_mean[3],\n",
        "            'Avg_Normalized_Euclidean': np.mean(norm_dists)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(summary)\n",
        "\n",
        "result_df = evaluate_ordered_recommendation_cosine(similarity_cosine_df, true_label_df)\n",
        "print(result_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiyEGakTZn8P"
      },
      "source": [
        "## per proposal baru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZoVKOZGDJFx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def evaluate_per_proposal_cosine(rank_all_df, true_label_df, top_n=3):\n",
        "    # Filter dan urutkan\n",
        "    top_n_df = rank_all_df[rank_all_df[\"rank\"] <= top_n]\n",
        "    rec_df = top_n_df.sort_values(by=[\"id_proposal\", \"rank\", \"similarity_score\"], ascending=[True, True, False])\n",
        "    rec_pivot = rec_df.pivot(index=\"id_proposal\", columns=\"rank\", values=\"dosen\").reset_index()\n",
        "    rec_pivot.columns.name = None\n",
        "    rec_pivot.columns = [\"id_proposal\"] + [f\"rec_{i}\" for i in range(1, len(rec_pivot.columns))]\n",
        "\n",
        "    # Gabung dengan label kebenaran\n",
        "    merged_df = pd.merge(\n",
        "        rec_pivot,\n",
        "        true_label_df.rename(columns={\"proposal_id\": \"id_proposal\"}),\n",
        "        on=\"id_proposal\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    # Recall of existence (abaikan urutan)\n",
        "    def recall_of_existence(row):\n",
        "        true_set = {row.get(\"author1\"), row.get(\"author2\"), row.get(\"author3\")}\n",
        "        pred_set = {row.get(f\"rec_{i}\") for i in range(1, top_n + 1) if row.get(f\"rec_{i}\")}\n",
        "        return len(true_set.intersection(pred_set)) / 3\n",
        "\n",
        "    merged_df[f'recall_of_existence@{top_n}'] = merged_df.apply(recall_of_existence, axis=1)\n",
        "\n",
        "    # Recall berdasarkan posisi (urutan harus sama)\n",
        "    for pos in [1, 2, 3]:\n",
        "        merged_df[f'recall_pos{pos}_ordered@{top_n}'] = merged_df.apply(\n",
        "            lambda row: int(\n",
        "                pd.notna(row.get(f'author{pos}')) and\n",
        "                pd.notna(row.get(f'rec_{pos}')) and\n",
        "                row.get(f'author{pos}') == row.get(f'rec_{pos}')\n",
        "            ) if pos <= top_n else 0,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    # Euclidean distance penalti posisi\n",
        "    distances = []\n",
        "    for _, row in merged_df.iterrows():\n",
        "        true_authors = [row.get(f'author{i}') for i in [1, 2, 3]]\n",
        "        pred_authors = [row.get(f'rec_{i}', None) for i in range(1, top_n + 1)]\n",
        "        distance = 0\n",
        "        max_penalty = top_n\n",
        "        for i, true_author in enumerate(true_authors):\n",
        "            if pd.isna(true_author) or true_author == '':\n",
        "                continue\n",
        "            try:\n",
        "                pred_pos = pred_authors.index(true_author)\n",
        "                pos_diff = pred_pos - i\n",
        "                distance += pos_diff ** 2\n",
        "            except ValueError:\n",
        "                distance += max_penalty ** 2\n",
        "        distances.append(np.sqrt(distance))\n",
        "\n",
        "    # Normalisasi jarak\n",
        "    scaler = MinMaxScaler()\n",
        "    norm_dists = scaler.fit_transform(np.array(distances).reshape(-1, 1)).flatten()\n",
        "    merged_df[f'norm_euclidean@{top_n}'] = norm_dists\n",
        "\n",
        "    # Ambil kolom evaluasi\n",
        "    result_df = merged_df[[\"id_proposal\",\n",
        "                           f'recall_of_existence@{top_n}',\n",
        "                           f'recall_pos1_ordered@{top_n}',\n",
        "                           f'recall_pos2_ordered@{top_n}',\n",
        "                           f'recall_pos3_ordered@{top_n}',\n",
        "                           f'norm_euclidean@{top_n}']].copy()\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "eval_per_proposal_cosine_3 = evaluate_per_proposal_cosine(similarity_cosine_df, true_label_df, top_n=3)\n",
        "eval_per_proposal_cosine_5 = evaluate_per_proposal_cosine(similarity_cosine_df, true_label_df, top_n=5)\n",
        "eval_per_proposal_cosine_7 = evaluate_per_proposal_cosine(similarity_cosine_df, true_label_df, top_n=7)\n",
        "eval_per_proposal_cosine_10 = evaluate_per_proposal_cosine(similarity_cosine_df, true_label_df, top_n=10)\n",
        "\n",
        "# eval_per_proposal_cosine_3.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/cosine/30/hasil_eval_3_cosine_topik.csv\", index=False)\n",
        "# eval_per_proposal_cosine_5.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/cosine/30/hasil_eval_5_cosine_topik.csv\", index=False)\n",
        "# eval_per_proposal_cosine_7.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/cosine/30/hasil_eval_7_cosine_topik.csv\", index=False)\n",
        "# eval_per_proposal_cosine_10.to_csv(\"/content/drive/MyDrive/Skripsi4/topik/cosine/30/hasil_eval_10_cosine_topik.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9pAGdpruELTI",
        "O5FkdQdcHvI2",
        "rXJRYVfCIx5Z"
      ],
      "authorship_tag": "ABX9TyMSOjNZHqHucfTcIdTikvN1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}