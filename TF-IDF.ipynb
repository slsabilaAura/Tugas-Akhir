{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slsabilaAura/Tugas-Akhir/blob/main/TF-IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpcD1p73jQRw",
        "outputId": "3b736f14-a7a6-44db-a012-3369621357dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: import file form drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9t0dbzGjWSI",
        "outputId": "516c14b5-0693-41e0-9a25-3248377f53ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Save the preprocessed DataFrames to new CSV files\n",
        "# Save the preprocessed DataFrames to new CSV files\n",
        "proposal_df = pd.read_csv('/content/drive/MyDrive/Skripsi3/Dataset/processed_proposalC.csv')\n",
        "expert_df = pd.read_csv('/content/drive/MyDrive/Skripsi3/Dataset/processed_expertC.csv')\n"
      ],
      "metadata": {
        "id": "kSy3xlzojYty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.corpora import Dictionary\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim import corpora, models\n",
        "from gensim.matutils import corpus2dense\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.matutils import corpus2dense\n",
        "from gensim.matutils import sparse2full\n",
        "import pandas as pd\n",
        "import ast"
      ],
      "metadata": {
        "id": "U8euBGprjZo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus dan Dictionary"
      ],
      "metadata": {
        "id": "dh5qIA_vTAR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_list(text):\n",
        "    try:\n",
        "        return ast.literal_eval(text) if isinstance(text, str) else text\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "\n",
        "proposal_df[\"processed\"] = proposal_df[\"stemmed\"].apply(convert_to_list)\n",
        "expert_df[\"processed\"] = expert_df[\"stemmed\"].apply(convert_to_list)\n",
        "\n",
        "proposal_df[\"processed\"] = proposal_df[\"processed\"].apply(lambda x: x[:30])\n",
        "expert_df[\"processed\"] = expert_df[\"processed\"].apply(lambda x: x[:30])\n",
        "\n",
        "# Gabungkan semua dokumen untuk membuat satu kamus bersama\n",
        "documents_all = proposal_df[\"processed\"].tolist() + expert_df[\"processed\"].tolist()\n",
        "dictionary_all = Dictionary(documents_all)\n",
        "\n",
        "\n",
        "dictionary_all.filter_extremes(no_below=6, no_above=0.5)\n",
        "\n",
        "# Buat corpus (Bag of Words)\n",
        "corpus_combined = [dictionary_all.doc2bow(text) for text in documents_all]\n",
        "\n",
        "# Buat model TF-IDF\n",
        "tfidf_model = models.TfidfModel(corpus_combined)\n",
        "\n",
        "# Transformasikan corpus ke TF-IDF\n",
        "tfidf_corpus_combined = tfidf_model[corpus_combined]\n",
        "\n",
        "n_proposals = len(proposal_df)\n",
        "n_experts = len(expert_df)\n",
        "\n",
        "tfidf_proposal = tfidf_corpus_combined[:n_proposals]\n",
        "tfidf_expert = tfidf_corpus_combined[n_proposals:]\n",
        "\n",
        "# Buat matriks dense (dokumen x vocab_size)\n",
        "proposal_dense = corpus2dense(tfidf_proposal, num_terms=len(dictionary_all)).T\n",
        "expert_dense = corpus2dense(tfidf_expert, num_terms=len(dictionary_all)).T"
      ],
      "metadata": {
        "id": "BeBu3-oGjcxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ambil daftar dokumen dari tfidf_corpus_combined\n",
        "tfidf_data = []\n",
        "\n",
        "for doc_index, doc in enumerate(tfidf_corpus_combined):\n",
        "    for term_id, weight in doc:\n",
        "        tfidf_data.append({\n",
        "            \"doc_id\": doc_index,\n",
        "            \"term\": dictionary_all[term_id],\n",
        "            \"term_id\": term_id,\n",
        "            \"tfidf\": weight\n",
        "        })\n",
        "\n",
        "# Ubah ke DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_data)\n",
        "\n",
        "doc1_tfidf = tfidf_df[tfidf_df[\"doc_id\"] == 1]\n",
        "print(doc1_tfidf.sort_values(by=\"tfidf\", ascending=False))  # Urut dari tertinggi\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51A2tN9-xKQr",
        "outputId": "911c8dce-890d-4fd3-9f7b-bd1b39ec8fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    doc_id      term  term_id     tfidf\n",
            "14       1     cepat       14  0.456942\n",
            "23       1     pikir       23  0.404016\n",
            "24       1      pola       24  0.335068\n",
            "22       1      maju       22  0.323745\n",
            "25       1    sangat       25  0.211175\n",
            "28       1  sekarang       28  0.208981\n",
            "16       1      desa       16  0.208981\n",
            "27       1    segala       27  0.208981\n",
            "17       1       era       17  0.170655\n",
            "30       1     tidak       30  0.164351\n",
            "19       1     jalan       19  0.159287\n",
            "26       1     sebar       26  0.159287\n",
            "21       1      luas       21  0.152314\n",
            "15       1    dampak       15  0.152314\n",
            "18       1     hanya       18  0.144428\n",
            "13       1    bidang       13  0.142653\n",
            "29       1    sosial       29  0.142653\n",
            "12       1      baik       12  0.113074\n",
            "20       1   kembang       20  0.087503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Directed M->D"
      ],
      "metadata": {
        "id": "A1DZqu5vm-1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# tfidf_df_proposal_contoh = pd.DataFrame(proposal_dense, columns=[f\"term_{i}\" for i in range(proposal_dense.shape[1])])\n",
        "# print(tfidf_df_proposal_contoh.head())\n",
        "terms = [dictionary_all[i] for i in range(len(dictionary_all))]\n",
        "tfidf_df_proposal_contoh = pd.DataFrame(proposal_dense, columns=terms)\n",
        "# print(tfidf_df_proposal_contoh)\n"
      ],
      "metadata": {
        "id": "-LkAUeKl0b7Y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konversi ke DataFrame\n",
        "proposal_df_tfidf = pd.DataFrame(proposal_dense)\n",
        "expert_df_tfidf = pd.DataFrame(expert_dense)\n",
        "\n",
        "# Simpan ke CSV\n",
        "proposal_df_tfidf.to_csv('/content/drive/MyDrive/Skripsi4/kata/hasil_TFIDF_proposal_lama.csv', index=False)\n",
        "expert_df_tfidf.to_csv('/content/drive/MyDrive/Skripsi4/kata/hasil_TFIDF_expert_lama.csv', index=False)"
      ],
      "metadata": {
        "id": "RSGvrFIOcS1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proposal_df[\"tahun\"] = proposal_df[\"proposal_year\"].astype(int)\n",
        "expert_df[\"pub_year\"] = expert_df[\"research_pub_year\"].astype(int)\n",
        "\n",
        "\n",
        "def similarity_m_to_d(proposal_vector, expert_vector):\n",
        "    numerator = np.dot(proposal_vector, expert_vector)\n",
        "    denominator = (np.linalg.norm(proposal_vector) + 1e-10)\n",
        "    return numerator / (denominator)\n",
        "\n",
        "\n",
        "def similarity_d_to_m(expert_vector, proposal_vector):\n",
        "    numerator = np.dot(expert_vector, proposal_vector)\n",
        "    denominator = (np.linalg.norm(expert_vector) + 1e-10)\n",
        "    return numerator / (denominator)\n",
        "\n",
        " # denominator = np.linalg.norm(expert_vector)\n",
        "\n",
        "def time_decay(year_prop, year_ex, t=1, gamma=0.1):\n",
        "    decay = 1 - ((year_prop - year_ex) / t) * gamma\n",
        "    return max(decay, 0.0)  # tidak boleh negatif\n",
        "\n",
        "# mapping Dosen dengan ID Dosen\n",
        "\n",
        "dosen_id_map = pd.read_csv(\"/content/drive/MyDrive/Skripsi3/Dataset/mapping.csv\")  # pastikan kolom: expert_id, expert_name\n",
        "dosen_id_map[\"expert_name\"] = dosen_id_map[\"expert_name\"].str.strip().str.lower()\n",
        "\n",
        "\n",
        "def attach_dosen_id(df, dosen_id_map):\n",
        "\n",
        "    df = df.copy()\n",
        "    df[\"dosen\"] = df[\"dosen\"].str.strip().str.lower()  # normalisasi agar cocok\n",
        "    dosen_id_map = dosen_id_map.copy()\n",
        "    dosen_id_map[\"dosen\"] = dosen_id_map[\"dosen\"].str.strip().str.lower()\n",
        "\n",
        "    merged_df = df.merge(dosen_id_map, on=\"dosen\", how=\"left\")\n",
        "    return merged_df\n",
        "\n",
        "\n",
        "\n",
        "def explode_authors_with_weights(df, dosen_id_map):\n",
        "    rows = []\n",
        "\n",
        "    # Normalisasi nama dosen agar cocok\n",
        "    dosen_id_map = dosen_id_map.copy()\n",
        "    dosen_id_map[\"expert_name\"] = dosen_id_map[\"expert_name\"].str.strip().str.lower()\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        authors = row.get(\"authors\", [])\n",
        "        # Bersihkan nama kosong atau NaN\n",
        "        authors = [a for a in authors if isinstance(a, str) and a.strip() != \"\"]\n",
        "\n",
        "        num_authors = len(authors)\n",
        "        for idx, name in enumerate(authors):\n",
        "            name_clean = name.strip().lower()\n",
        "\n",
        "            if num_authors == 1:\n",
        "                weight = 1.0\n",
        "            else:\n",
        "                weight = 0.6 if idx == 0 else 0.4 / (num_authors - 1)\n",
        "\n",
        "            new_row = row.to_dict()\n",
        "            new_row[\"name\"] = name\n",
        "            new_row[\"author_position\"] = idx + 1\n",
        "            new_row[\"num_authors\"] = num_authors\n",
        "            new_row[\"author_weight\"] = round(weight, 4)\n",
        "\n",
        "            matched = dosen_id_map[dosen_id_map[\"expert_name\"] == name_clean]\n",
        "            new_row[\"expert_id\"] = matched[\"expert_id\"].values[0] if not matched.empty else None\n",
        "\n",
        "            rows.append(new_row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# Gabungkan author_1 sampai author_6 jadi list\n",
        "author_cols = [\"author_1\", \"author_2\", \"author_3\", \"author_4\", \"author_5\", \"author_6\"]\n",
        "expert_df[\"authors\"] = expert_df[author_cols].values.tolist()\n",
        "\n",
        "# Hapus duplikat berdasarkan research_id\n",
        "expert_df = expert_df.drop_duplicates(subset=[\"research_id\"]).copy()\n",
        "\n",
        "# Jalankan explode\n",
        "expert_df = explode_authors_with_weights(expert_df, dosen_id_map)\n",
        "\n",
        "# Opsional: hanya simpan baris dengan expert_id valid\n",
        "expert_df = expert_df[expert_df[\"expert_id\"].notna()]\n",
        "\n",
        "\n",
        "\n",
        "expert_df.to_csv(\"/content/drive/MyDrive/Skripsi4/dictionary/expert_df_id_dosen.csv\", index=False)"
      ],
      "metadata": {
        "id": "tDDkfvOAmfwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(proposal_df))\n",
        "print(len(proposal_dense))\n",
        "print(len(expert_df))\n",
        "print(len(expert_dense))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W34OvQSLAOcm",
        "outputId": "d1e902c6-7166-4c9f-d8b7-e7c945fbaed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "142\n",
            "142\n",
            "760\n",
            "467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OD M->D"
      ],
      "metadata": {
        "id": "9H2x1IidnImf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_od_m_to_d(tfidf_proposal, tfidf_expert, proposal_df, expert_df):\n",
        "    records = []\n",
        "\n",
        "    for idx_prop, prop_row in proposal_df.iterrows():\n",
        "        prop_id = prop_row[\"proposal_id\"]\n",
        "        prop_vector = tfidf_proposal[idx_prop]\n",
        "        prop_year = prop_row[\"tahun\"]\n",
        "        mahasiswa = prop_row[\"student_id\"]\n",
        "\n",
        "        for idx_expert, exp_row in expert_df.iterrows():\n",
        "            exp_id_dosen = exp_row[\"expert_id\"]\n",
        "            author_position = exp_row[\"author_position\"]\n",
        "            author_weight = exp_row[\"author_weight\"]\n",
        "            pub_year = exp_row[\"pub_year\"]\n",
        "            penelitian_id = exp_row[\"research_id\"]\n",
        "            dosen = exp_row[\"name\"]\n",
        "            exp_vector = tfidf_expert[idx_expert]\n",
        "\n",
        "            sim_m2d = similarity_m_to_d(prop_vector, exp_vector)\n",
        "            od_score = sim_m2d * author_weight\n",
        "\n",
        "            records.append({\n",
        "                \"id_proposal\": prop_id,\n",
        "                \"mahasiswa\": mahasiswa,\n",
        "                \"penelitian_id\": penelitian_id,\n",
        "                \"id_dosen\": exp_id_dosen,\n",
        "                \"dosen\": dosen,\n",
        "                \"author_position\": author_position,\n",
        "                \"tahun_proposal\": prop_year,\n",
        "                \"tahun_penelitian\": pub_year,\n",
        "                \"author_weight\": author_weight,\n",
        "                \"OD(M→D)\": round(od_score, 4)\n",
        "            })\n",
        "\n",
        "    df_scores = pd.DataFrame(records)\n",
        "\n",
        "    # Ambil OD tertinggi untuk tiap pasangan proposal-dosen\n",
        "    df_scores['id_proposal'] = df_scores['id_proposal'].astype(str)\n",
        "    # df_scores = df_scores.loc[df_scores.groupby([\"id_proposal\", \"id_dosen\"])[\"OD(M→D)\"].idxmax()]\n",
        "    df_scores = df_scores[df_scores[\"OD(M→D)\"] > 0.02]\n",
        "    df_scores = df_scores.reset_index(drop=True)\n",
        "\n",
        "    # Urutkan\n",
        "    df_scores = df_scores.sort_values(by=[\"id_proposal\", \"OD(M→D)\"], ascending=[True, False])\n",
        "\n",
        "    return df_scores\n",
        "\n",
        "# Hitung ulang\n",
        "df_od = compute_od_m_to_d(\n",
        "    proposal_dense,\n",
        "    expert_dense,\n",
        "    proposal_df,\n",
        "    expert_df\n",
        ")\n",
        "df_od = df_od[df_od[\"OD(M→D)\"] > 0.02]\n",
        "\n",
        "print(df_od[[\"id_proposal\", \"mahasiswa\", \"id_dosen\", \"dosen\", \"penelitian_id\", \"author_position\", \"author_weight\",\"OD(M→D)\"]].head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Lihat hasil\n",
        "# print(df_od[[\"id_proposal\",\"mahasiswa\", \"id_dosen\", \"dosen\", \"author_position\", \"penelitian_id\", \"tahun_proposal\",\"tahun_penelitian\", \"OD(M→D)\"]].head(5))\n",
        "df_od.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/directed/od_m2d_kata.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n25Qyv8WnCg_",
        "outputId": "50fad483-1f02-455b-9bae-26cca563cc52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    id_proposal mahasiswa id_dosen            dosen penelitian_id  \\\n",
            "105          P1        S1       D1  Bambang Harjito          R456   \n",
            "77           P1        S1       D1  Bambang Harjito          R345   \n",
            "5            P1        S1       D1  Bambang Harjito           R32   \n",
            "106          P1        S1      D15    Heri Prasetyo          R456   \n",
            "43           P1        S1      D15    Heri Prasetyo          R186   \n",
            "\n",
            "     author_position  author_weight  OD(M→D)  \n",
            "105                1            0.6   0.2402  \n",
            "77                 1            0.6   0.1818  \n",
            "5                  1            1.0   0.1683  \n",
            "106                2            0.4   0.1601  \n",
            "43                 1            1.0   0.1326  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TOD M->D"
      ],
      "metadata": {
        "id": "qU6ML4QXosPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tod_m2d(od_df, t=1, gamma=0.1, min_year_diff=0, max_year_diff=5):\n",
        "    od_df = od_df.copy()\n",
        "\n",
        "    # Hitung selisih tahun\n",
        "    od_df[\"selisih_tahun\"] = od_df[\"tahun_proposal\"] - od_df[\"tahun_penelitian\"]\n",
        "\n",
        "    # Filter: selisih tahun antara 3 dan 5 tahun\n",
        "    od_df = od_df[\n",
        "        (od_df[\"selisih_tahun\"] >= min_year_diff) &\n",
        "        (od_df[\"selisih_tahun\"] <= max_year_diff)\n",
        "    ].copy()\n",
        "\n",
        "    # Hitung time decay factor\n",
        "    od_df[\"time_decay_factor\"] = od_df.apply(\n",
        "        lambda row: time_decay(row[\"tahun_proposal\"], row[\"tahun_penelitian\"], t=t, gamma=gamma),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Hitung TOD(M→D)\n",
        "    od_df[\"TOD(M→D)\"] = (od_df[\"OD(M→D)\"] * od_df[\"time_decay_factor\"]).round(4)\n",
        "\n",
        "    return od_df\n",
        "\n",
        "# Panggil fungsi dengan batas selisih tahun antara 3–5\n",
        "tod_m2d_df = compute_tod_m2d(df_od, t=1, gamma=0.1, min_year_diff=0, max_year_diff=5)\n",
        "# Filter nilai TOD(M→D) > 0.04\n",
        "# tod_m2d_df = tod_m2d_df[tod_m2d_df[\"TOD(M→D)\"] > 0.06]\n",
        "\n",
        "# Urutkan berdasarkan skor rekomendasi\n",
        "tod_m2d_df = (\n",
        "    tod_m2d_df\n",
        "    .loc[tod_m2d_df.groupby([\"id_proposal\", \"id_dosen\"])[\"TOD(M→D)\"].idxmax()]\n",
        "    .reset_index(drop=True)\n",
        "    .sort_values(by=[\"id_proposal\", \"TOD(M→D)\"], ascending=[True, False])\n",
        ")\n",
        "\n",
        "# Tampilkan kolom penting\n",
        "print(tod_m2d_df[[\n",
        "    \"id_proposal\", \"mahasiswa\", \"penelitian_id\", \"dosen\", \"id_dosen\",\n",
        "    \"author_position\", \"author_weight\", \"OD(M→D)\", \"tahun_penelitian\",\n",
        "    \"tahun_proposal\", \"selisih_tahun\", \"time_decay_factor\", \"TOD(M→D)\"\n",
        "]].head(5))\n",
        "\n",
        "# Simpan ke CSV\n",
        "tod_m2d_df.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/directed/tod_m2d_df_kata.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grGd9iKYcZ4u",
        "outputId": "9a101455-17e2-4714-b604-e9ceb7ea703a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  id_proposal mahasiswa penelitian_id               dosen id_dosen  \\\n",
            "5          P1        S1          R186       Heri Prasetyo      D15   \n",
            "0          P1        S1           R48     Bambang Harjito       D1   \n",
            "3          P1        S1          R291             Wiharto      D12   \n",
            "9          P1        S1           R71  Dewi Wisnu Wardani       D4   \n",
            "8          P1        S1          R264         Umi Salamah       D3   \n",
            "\n",
            "   author_position  author_weight  OD(M→D)  tahun_penelitian  tahun_proposal  \\\n",
            "5                1            1.0   0.1326              2020            2021   \n",
            "0                1            0.6   0.0911              2018            2021   \n",
            "3                1            1.0   0.0891              2017            2021   \n",
            "9                1            1.0   0.0501              2021            2021   \n",
            "8                1            1.0   0.0896              2016            2021   \n",
            "\n",
            "   selisih_tahun  time_decay_factor  TOD(M→D)  \n",
            "5              1                0.9    0.1193  \n",
            "0              3                0.7    0.0638  \n",
            "3              4                0.6    0.0535  \n",
            "9              0                1.0    0.0501  \n",
            "8              5                0.5    0.0448  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_tod_m2d(od_df, t=1, gamma=0.1, max_year_diff=5):\n",
        "#     od_df = od_df.copy()\n",
        "\n",
        "#     # Hitung selisih tahun\n",
        "#     od_df[\"selisih_tahun\"] = od_df[\"tahun_proposal\"] - od_df[\"tahun_penelitian\"]\n",
        "\n",
        "#     # Filter: proposal harus lebih baru dari publikasi, dan selisih maksimal 5 tahun\n",
        "#     od_df = od_df[(od_df[\"selisih_tahun\"] > 0) & (od_df[\"selisih_tahun\"] <= max_year_diff)].copy()\n",
        "\n",
        "#     # Hitung time decay factor\n",
        "#     od_df[\"time_decay_factor\"] = od_df.apply(\n",
        "#         lambda row: time_decay(row[\"tahun_proposal\"], row[\"tahun_penelitian\"], t=t, gamma=gamma),\n",
        "#         axis=1\n",
        "#     )\n",
        "\n",
        "#     # Hitung TOD(M→D)\n",
        "#     od_df[\"TOD(M→D)\"] = (od_df[\"OD(M→D)\"] * od_df[\"time_decay_factor\"]).round(4)\n",
        "\n",
        "#     return od_df\n",
        "\n",
        "# tod_m2d_df= compute_tod_m2d(df_od, t=1, gamma=0.1, max_year_diff=5)\n",
        "\n",
        "# tod_m2d_df = (\n",
        "#     tod_m2d_df\n",
        "#     # .loc[tod_m2d_df.groupby([\"id_proposal\", \"id_dosen\"])[\"TOD(M→D)\"].idxmax()]\n",
        "#     # .reset_index(drop=True)\n",
        "#     .sort_values(by=[\"id_proposal\", \"TOD(M→D)\"], ascending=[True, False])\n",
        "# )\n",
        "\n",
        "\n",
        "# print(tod_m2d_df[[\"id_proposal\", \"mahasiswa\", \"penelitian_id\",\"dosen\",\"id_dosen\",\"author_position\", \"author_weight\", \"OD(M→D)\",\"tahun_penelitian\",\"tahun_proposal\", \"selisih_tahun\", \"time_decay_factor\", \"TOD(M→D)\"]].head(5))\n",
        "\n",
        "# tod_m2d_df.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/directed/tod_m2d_df_kata.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "1NYc1qB2orl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Directed D→M"
      ],
      "metadata": {
        "id": "Eo2pqgJBpN57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_od_d_to_m(tfidf_expert, tfidf_proposal, proposal_df, expert_df):\n",
        "    records = []\n",
        "\n",
        "\n",
        "    for idx_expert, exp_row in expert_df.iterrows():\n",
        "        exp_id_dosen = exp_row[\"expert_id\"]\n",
        "        author_position = exp_row[\"author_position\"]\n",
        "        author_weight = exp_row[\"author_weight\"]\n",
        "        pub_year = exp_row[\"pub_year\"]\n",
        "        penelitian_id = exp_row[\"research_id\"]\n",
        "        dosen = exp_row[\"name\"]\n",
        "        exp_vector = tfidf_expert[idx_expert]\n",
        "\n",
        "        for idx_prop, prop_row in proposal_df.iterrows():\n",
        "            prop_id = prop_row[\"proposal_id\"]\n",
        "            prop_vector = tfidf_proposal[idx_prop]\n",
        "            prop_year = prop_row[\"tahun\"]\n",
        "            mahasiswa=prop_row[\"student_id\"]\n",
        "\n",
        "            selisih_tahun = prop_year - pub_year\n",
        "            if 0 <= selisih_tahun <= 5 & pub_year <= prop_year:\n",
        "              sim_d2m = similarity_d_to_m(exp_vector, prop_vector)\n",
        "              od_score = sim_d2m * author_weight\n",
        "\n",
        "              records.append({\n",
        "                  \"id_proposal\": prop_id,\n",
        "                  \"penelitian_id\": penelitian_id,\n",
        "                  \"id_dosen\": exp_id_dosen,\n",
        "                  'mahasiswa': mahasiswa,\n",
        "                  \"dosen\": dosen,\n",
        "                  \"author_position\": author_position,\n",
        "                  \"author_weight\": author_weight,\n",
        "                  \"tahun_penelitian\": pub_year,\n",
        "                  \"tahun_proposal\": prop_year,\n",
        "                  \"OD(D→M)\": round(od_score, 4)\n",
        "              })\n",
        "\n",
        "    df_scores = pd.DataFrame(records)\n",
        "    df_scores = df_scores.loc[df_scores.groupby([\"id_proposal\", \"id_dosen\"])[\"OD(D→M)\"].idxmax()].reset_index(drop=True)\n",
        "    df_scores['id_proposal'] = df_scores['id_proposal'].astype(str)\n",
        "    df_scores = df_scores.sort_values(by=[\"id_dosen\", \"OD(D→M)\"], ascending=[True, False])\n",
        "    return df_scores\n",
        "\n",
        "df_od_d2m = compute_od_d_to_m(\n",
        "    expert_dense,\n",
        "    proposal_dense,\n",
        "    proposal_df, expert_df\n",
        ")\n",
        "\n",
        "# df_od_d2m = df_od_d2m[df_od_d2m[\"OD(D→M)\"] > 0.05]\n",
        "\n",
        "\n",
        "print(df_od_d2m[[\"id_proposal\",\"mahasiswa\", \"id_dosen\", \"dosen\", \"author_position\", \"author_weight\", \"penelitian_id\",\n",
        "                  \"tahun_proposal\", \"tahun_penelitian\", \"OD(D→M)\"]].head(5))\n",
        "\n",
        "df_od_d2m.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/directed/df_od_d2m_kata.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpApO12TpQyT",
        "outputId": "5a0ffa94-afda-4f0c-fdda-509707927d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_proposal mahasiswa id_dosen            dosen  author_position  \\\n",
            "264         P114      S114       D1  Bambang Harjito                1   \n",
            "602         P134      S134       D1  Bambang Harjito                1   \n",
            "248         P113      S113       D1  Bambang Harjito                1   \n",
            "1280         P46       S46       D1  Bambang Harjito                1   \n",
            "1934         P82       S82       D1  Bambang Harjito                1   \n",
            "\n",
            "      author_weight penelitian_id  tahun_proposal  tahun_penelitian  OD(D→M)  \n",
            "264             1.0           R54            2023              2021   0.3182  \n",
            "602             1.0           R54            2021              2021   0.3008  \n",
            "248             1.0          R363            2023              2023   0.2119  \n",
            "1280            1.0          R391            2023              2023   0.2089  \n",
            "1934            1.0          R363            2024              2023   0.2085  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rekomendasi"
      ],
      "metadata": {
        "id": "FdkW0z-LpyIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_overlap_scores(df_m2d, df_d2m, top_n=17):\n",
        "    # Ambil Top-N dari masing-masing arah\n",
        "    top_m2d = df_m2d.groupby(\"id_proposal\").head(top_n)[[\"id_proposal\", \"id_dosen\",\"mahasiswa\", \"dosen\", \"TOD(M→D)\"]]\n",
        "    top_d2m = df_d2m.groupby(\"id_proposal\").head(top_n)[[\"id_proposal\", \"id_dosen\",\"mahasiswa\", \"dosen\",\"OD(D→M)\"]]\n",
        "\n",
        "    # Outer join untuk semua pasangan top-n\n",
        "    merged = pd.merge(top_m2d, top_d2m, on=[\"id_proposal\", \"id_dosen\"], how=\"outer\")\n",
        "\n",
        "    # Tambahkan kolom nama dosen jika hilang (dari M→D arah saja)\n",
        "    if \"dosen_x\" in merged.columns:\n",
        "        merged[\"dosen\"] = merged[\"dosen_x\"].combine_first(merged.get(\"dosen_y\"))\n",
        "    elif \"dosen\" not in merged.columns:\n",
        "        merged[\"dosen\"] = None\n",
        "\n",
        "    if \"mahasiswa_x\" in merged.columns:\n",
        "        merged[\"mahasiswa\"] = merged[\"mahasiswa_x\"].combine_first(merged.get(\"mahasiswa_y\"))\n",
        "    elif \"mahasiswa\" not in merged.columns:\n",
        "        merged[\"mahasiswa\"] = None\n",
        "\n",
        "    # Ganti NaN skor dengan 0 agar bisa dihitung rata-ratanya\n",
        "    merged[\"TOD(M→D)\"] = merged[\"TOD(M→D)\"].fillna(0)\n",
        "    merged[\"OD(D→M)\"] = merged[\"OD(D→M)\"].fillna(0)\n",
        "\n",
        "    # Tandai overlap jika dosen muncul di kedua arah\n",
        "    merged[\"overlap\"] = (merged[\"TOD(M→D)\"] > 0) & (merged[\"OD(D→M)\"] > 0)\n",
        "\n",
        "    # Hitung skor rata-rata (hanya jika overlap)\n",
        "    merged[\"skor_rata2\"] = merged.apply(\n",
        "        lambda row: (row[\"TOD(M→D)\"] + row[\"OD(D→M)\"]) / 2 if row[\"overlap\"] else 0, axis=1\n",
        "    )\n",
        "\n",
        "    # Ambil skor tertinggi per proposal dan dosen\n",
        "    final_scores = merged[[\"id_proposal\",\"mahasiswa\", \"dosen\", \"id_dosen\", \"TOD(M→D)\",\"OD(D→M)\", \"skor_rata2\", \"overlap\"]]\n",
        "    final_scores = final_scores.sort_values(by=[\"id_proposal\", \"skor_rata2\"], ascending=[True, False])\n",
        "\n",
        "    return final_scores\n",
        "\n",
        "df_final = combine_overlap_scores(tod_m2d_df, df_od_d2m, top_n=17)\n",
        "print(df_final.head(5))\n",
        "\n",
        "df_final.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/directed/overlap_kombinasi_kata.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzOzfPOvp0Ym",
        "outputId": "3ff5f315-1c2d-40e8-fc5f-f75e0f15f9c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  id_proposal mahasiswa               dosen id_dosen  TOD(M→D)  OD(D→M)  \\\n",
            "5          P1        S1       Heri Prasetyo      D15    0.1193   0.1326   \n",
            "0          P1        S1     Bambang Harjito       D1    0.0638   0.0386   \n",
            "9          P1        S1  Dewi Wisnu Wardani       D4    0.0501   0.0501   \n",
            "8          P1        S1         Umi Salamah       D3    0.0448   0.0292   \n",
            "3          P1        S1             Wiharto      D12    0.0535   0.0190   \n",
            "\n",
            "   skor_rata2  overlap  \n",
            "5     0.12595     True  \n",
            "0     0.05120     True  \n",
            "9     0.05010     True  \n",
            "8     0.03700     True  \n",
            "3     0.03625     True  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rangking Directed"
      ],
      "metadata": {
        "id": "1uUCgzjip7Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_overlap_scores_with_ranking(df_m2d, df_d2m):\n",
        "    # Ambil semua skor dari kedua arah\n",
        "    m2d = df_m2d[[\"id_proposal\", \"id_dosen\",\"mahasiswa\", \"dosen\", \"TOD(M→D)\"]]\n",
        "    d2m = df_d2m[[\"id_proposal\",\"id_dosen\",\"mahasiswa\",\"dosen\", \"OD(D→M)\"]]\n",
        "\n",
        "    # Outer join agar semua kombinasi muncul\n",
        "    merged = pd.merge(m2d, d2m, on=[\"id_proposal\", \"id_dosen\"], how=\"outer\")\n",
        "\n",
        "    # Tambahkan kolom nama dosen jika hilang (dari M→D arah saja)\n",
        "    if \"dosen_x\" in merged.columns:\n",
        "        merged[\"dosen\"] = merged[\"dosen_x\"].combine_first(merged.get(\"dosen_y\"))\n",
        "    elif \"dosen\" not in merged.columns:\n",
        "        merged[\"dosen\"] = None\n",
        "\n",
        "    if \"mahasiswa_x\" in merged.columns:\n",
        "        merged[\"mahasiswa\"] = merged[\"mahasiswa_x\"].combine_first(merged.get(\"mahasiswa_y\"))\n",
        "    elif \"mahasiswa\" not in merged.columns:\n",
        "        merged[\"mahasiswa\"] = None\n",
        "\n",
        "    # Isi nilai NaN dengan 0 untuk penggabungan skor\n",
        "    merged[\"TOD(M→D)\"] = merged[\"TOD(M→D)\"].fillna(0)\n",
        "    merged[\"OD(D→M)\"] = merged[\"OD(D→M)\"].fillna(0)\n",
        "\n",
        "    # Overlap = muncul di kedua arah\n",
        "    merged[\"overlap\"] = (merged[\"TOD(M→D)\"] > 0) & (merged[\"OD(D→M)\"] > 0)\n",
        "\n",
        "    # Skor rata-rata jika overlap\n",
        "    merged[\"skor_rata2\"] = merged.apply(\n",
        "        lambda row: (row[\"TOD(M→D)\"] + row[\"OD(D→M)\"]) / 2 if row[\"overlap\"] else 0, axis=1\n",
        "    )\n",
        "\n",
        "       # Hitung ranking per proposal berdasarkan skor rata-rata (tanpa groupby + agg)\n",
        "    merged[\"rank\"] = merged.groupby(\"id_proposal\")[\"skor_rata2\"]\\\n",
        "                           .rank(ascending=False, method=\"dense\")\\\n",
        "                           .astype(int)\n",
        "\n",
        "    # Ambil kolom yang diinginkan dan urutkan\n",
        "    result = merged.sort_values(by=[\"id_proposal\", \"rank\"])[\n",
        "        [\"id_proposal\",\"mahasiswa\", \"dosen\", \"id_dosen\", \"TOD(M→D)\", \"OD(D→M)\", \"skor_rata2\", \"overlap\", \"rank\"]\n",
        "    ]\n",
        "\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "df_peringkat = combine_overlap_scores_with_ranking(tod_m2d_df, df_od_d2m)\n",
        "# Filter hanya yang overlap == True\n",
        "df_overlap_true = df_peringkat[df_peringkat[\"overlap\"] == True]\n",
        "print(df_overlap_true.head(20))\n",
        "\n",
        "df_overlap_true.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/directed/rank_overlap_true_kata.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx_S4r_Qp3Ym",
        "outputId": "d45a7763-6616-4721-c949-a57de23dec9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id_proposal mahasiswa               dosen id_dosen  TOD(M→D)  OD(D→M)  \\\n",
            "5           P1        S1       Heri Prasetyo      D15    0.1193   0.1326   \n",
            "0           P1        S1     Bambang Harjito       D1    0.0638   0.0386   \n",
            "9           P1        S1  Dewi Wisnu Wardani       D4    0.0501   0.0501   \n",
            "8           P1        S1         Umi Salamah       D3    0.0448   0.0292   \n",
            "3           P1        S1             Wiharto      D12    0.0535   0.0190   \n",
            "13          P1        S1        Esti Suryani       D8    0.0426   0.0286   \n",
            "2           P1        S1             Winarno      D11    0.0319   0.0354   \n",
            "7           P1        S1             Wiranto       D2    0.0324   0.0324   \n",
            "6           P1        S1     Ardhi Wijayanto      D16    0.0215   0.0179   \n",
            "4           P1        S1     Haryono Setiadi      D14    0.0196   0.0192   \n",
            "14          P1        S1    Sari Widya Sihwi       D9    0.0251   0.0115   \n",
            "23         P10       S10             Wiranto       D2    0.2056   0.2285   \n",
            "15         P10       S10     Bambang Harjito       D1    0.0979   0.0979   \n",
            "21         P10       S10       Heri Prasetyo      D15    0.0742   0.0824   \n",
            "17         P10       S10             Winarno      D11    0.0729   0.0729   \n",
            "18         P10       S10             Wiharto      D12    0.0779   0.0596   \n",
            "16         P10       S10      Afrizal Doewes      D10    0.0677   0.0677   \n",
            "20         P10       S10     Haryono Setiadi      D14    0.0676   0.0676   \n",
            "24         P10       S10         Umi Salamah       D3    0.0496   0.0551   \n",
            "29         P10       S10        Esti Suryani       D8    0.0468   0.0443   \n",
            "\n",
            "    skor_rata2  overlap  rank  \n",
            "5      0.12595     True     1  \n",
            "0      0.05120     True     2  \n",
            "9      0.05010     True     3  \n",
            "8      0.03700     True     4  \n",
            "3      0.03625     True     5  \n",
            "13     0.03560     True     6  \n",
            "2      0.03365     True     7  \n",
            "7      0.03240     True     8  \n",
            "6      0.01970     True     9  \n",
            "4      0.01940     True    10  \n",
            "14     0.01830     True    11  \n",
            "23     0.21705     True     1  \n",
            "15     0.09790     True     2  \n",
            "21     0.07830     True     3  \n",
            "17     0.07290     True     4  \n",
            "18     0.06875     True     5  \n",
            "16     0.06770     True     6  \n",
            "20     0.06760     True     7  \n",
            "24     0.05235     True     8  \n",
            "29     0.04555     True     9  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beban Kerja"
      ],
      "metadata": {
        "id": "ydn7OewJqDJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# 1. Inisialisasi count untuk rank 1\n",
        "rank1_count_directed = defaultdict(int)\n",
        "final_assignment_directed = []\n",
        "\n",
        "# 2. Tetapkan rank 1 dengan batas 15 kali per dosen\n",
        "for pid in df_overlap_true[\"id_proposal\"].unique():\n",
        "    candidates = df_overlap_true[df_overlap_true[\"id_proposal\"] == pid]\n",
        "    candidates = candidates.sort_values(by=\"skor_rata2\", ascending=False)\n",
        "\n",
        "    assigned_rank1 = False\n",
        "    for _, row in candidates.iterrows():\n",
        "        dosen_id = row[\"id_dosen\"]\n",
        "        if rank1_count_directed[dosen_id] < 15:\n",
        "            rank1_count_directed[dosen_id] += 1\n",
        "            row_data = row.to_dict()\n",
        "            row_data[\"rank\"] = 1\n",
        "            row_data[\"beban\"] = rank1_count_directed[dosen_id]\n",
        "            final_assignment_directed.append(row_data)\n",
        "            assigned_rank1 = True\n",
        "            break\n",
        "\n",
        "    if not assigned_rank1:\n",
        "        row = candidates.iloc[0].to_dict()\n",
        "        dosen_id = row[\"id_dosen\"]\n",
        "        rank1_count_directed[dosen_id] += 1\n",
        "        row[\"rank\"] = 1\n",
        "        row[\"beban\"] = rank1_count_directed[dosen_id]\n",
        "        final_assignment_directed.append(row)\n",
        "\n",
        "# 3. Buat dataframe dari rank 1\n",
        "rank1_directed = pd.DataFrame(final_assignment_directed)\n",
        "\n",
        "# 4. Tambahkan rank 2–17 berdasarkan similarity, excl. dosen yang sudah dipakai di rank 1 untuk proposal yang sama\n",
        "other_ranks = []\n",
        "\n",
        "for pid in df_overlap_true[\"id_proposal\"].unique():\n",
        "    # Dapatkan dosen yang sudah dipakai sebagai rank 1\n",
        "    used_dosen = rank1_directed[rank1_directed[\"id_proposal\"] == pid][\"id_dosen\"].tolist()\n",
        "\n",
        "    # Ambil kandidat lain untuk proposal ini\n",
        "    candidates = df_overlap_true[(df_overlap_true[\"id_proposal\"] == pid) & (~df_overlap_true[\"id_dosen\"].isin(used_dosen))]\n",
        "    candidates = candidates.sort_values(by=\"skor_rata2\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    for idx, (_, row) in enumerate(candidates.iterrows(), start=2):\n",
        "        if idx > 17:\n",
        "            break\n",
        "        row_data = row.to_dict()\n",
        "        row_data[\"rank\"] = idx\n",
        "        row_data[\"beban\"] = rank1_count_directed[row_data[\"id_dosen\"]]  # Beban hanya dihitung dari rank 1\n",
        "        other_ranks.append(row_data)\n",
        "\n",
        "# 5. Gabungkan rank1 dan other ranks\n",
        "final_df = pd.concat([rank1_directed, pd.DataFrame(other_ranks)], ignore_index=True)\n",
        "final_df = final_df.sort_values(by=[\"id_proposal\", \"rank\"])\n",
        "\n",
        "# 🔟 Filter hanya Top 10 dosen per proposal\n",
        "final_df = final_df[final_df[\"rank\"] <= 17]\n",
        "\n",
        "\n",
        "\n",
        "# 6. Tampilkan hasil\n",
        "print(final_df[[\"id_proposal\", \"mahasiswa\", \"id_dosen\", \"dosen\", \"skor_rata2\", \"rank\", \"beban\"]])\n",
        "\n",
        "final_df.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/directed/beban_kerja_kata.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnF-h0ZLYuEp",
        "outputId": "5eafa229-30cd-4440-aac0-60f07bbf49f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_proposal mahasiswa id_dosen               dosen  skor_rata2  rank  \\\n",
            "0             P1        S1      D15       Heri Prasetyo     0.12595     1   \n",
            "142           P1        S1       D1     Bambang Harjito     0.05120     2   \n",
            "143           P1        S1       D4  Dewi Wisnu Wardani     0.05010     3   \n",
            "144           P1        S1       D3         Umi Salamah     0.03700     4   \n",
            "145           P1        S1      D12             Wiharto     0.03625     5   \n",
            "...          ...       ...      ...                 ...         ...   ...   \n",
            "2153         P99       S99       D7      Wisnu Widiarto     0.06005    13   \n",
            "2154         P99       S99      D11             Winarno     0.05770    14   \n",
            "2155         P99       S99      D13   Hasan Dwi Cahyono     0.04550    15   \n",
            "2156         P99       S99       D5          Abdul Aziz     0.03510    16   \n",
            "2157         P99       S99       D6       Ristu Saptono     0.02135    17   \n",
            "\n",
            "      beban  \n",
            "0         1  \n",
            "142      12  \n",
            "143      15  \n",
            "144       9  \n",
            "145      15  \n",
            "...     ...  \n",
            "2153      7  \n",
            "2154     13  \n",
            "2155      8  \n",
            "2156      3  \n",
            "2157      1  \n",
            "\n",
            "[2158 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluasi Directed"
      ],
      "metadata": {
        "id": "-zGycLpzqLty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# Load the true labels DataFrame\n",
        "true_label_df = pd.read_csv(\"/content/drive/MyDrive/Skripsi3/Dataset/true_labels.csv\")\n",
        "\n",
        "\n",
        "# Ubah kolom author, author2, author3 menjadi lowercase\n",
        "true_label_df[\"author1\"] = true_label_df[\"examiner_1\"].astype(str).str.strip()\n",
        "true_label_df[\"author2\"] = true_label_df[\"examiner_2\"].astype(str).str.strip()\n",
        "true_label_df[\"author3\"] = true_label_df[\"examiner_3\"].astype(str).str.strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "eZyklzkzq3eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluasi Baru"
      ],
      "metadata": {
        "id": "iDNkVsorTYsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Contoh struktur dummy data untuk demonstrasi perbaikan fungsi (tidak dijalankan secara nyata di sini)\n",
        "# similarity_cosine_df = pd.read_csv(...)  # Format: id_proposal, rank, similarity_score_akhir, dosen\n",
        "# true_label_df = pd.read_csv(...)  # Format: proposal_id, author1, author2, author3\n",
        "\n",
        "def evaluate_ordered_recommendation_cosine(rank_all_df, true_label_df, top_ns=[3, 5, 7, 10]):\n",
        "    summary = []\n",
        "\n",
        "    for TOP_N in top_ns:\n",
        "        # Filter Top-N dan urutkan\n",
        "        top_n_df = rank_all_df[rank_all_df[\"rank\"] <= TOP_N]\n",
        "        rec_df = top_n_df.sort_values(by=[\"id_proposal\", \"rank\", \"skor_rata2\"], ascending=[True, True, False])\n",
        "        rec_df = rec_df.drop_duplicates(subset=[\"id_proposal\", \"rank\",\"skor_rata2\"])\n",
        "        rec_pivot = rec_df.pivot(index=\"id_proposal\", columns=\"rank\", values=\"dosen\").reset_index()\n",
        "        rec_pivot.columns.name = None\n",
        "        rec_pivot.columns = [\"id_proposal\"] + [f\"rec_{i}\" for i in range(1, len(rec_pivot.columns))]\n",
        "\n",
        "        # Gabungkan dengan ground truth\n",
        "        merged_df = pd.merge(\n",
        "            rec_pivot,\n",
        "            true_label_df.rename(columns={\"proposal_id\": \"id_proposal\"}),\n",
        "            on=\"id_proposal\",\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Recall keberadaan (tidak memperhatikan urutan)\n",
        "        def recall_of_existence(row):\n",
        "            true_set = {row.get(\"author1\"), row.get(\"author2\"), row.get(\"author3\")}\n",
        "            pred_set = {row.get(f\"rec_{i}\") for i in range(1, TOP_N + 1) if row.get(f\"rec_{i}\")}\n",
        "            return len(true_set.intersection(pred_set)) / 3\n",
        "\n",
        "        merged_df[f'recall_of_existence@{TOP_N}'] = merged_df.apply(recall_of_existence, axis=1)\n",
        "\n",
        "        # Recall per posisi dengan urutan diperhatikan (rec_i harus sama dengan author_i)\n",
        "        recall_pos = {1: [], 2: [], 3: []}\n",
        "        for _, row in merged_df.iterrows():\n",
        "            for pos in [1, 2, 3]:\n",
        "                examiner = row.get(f'author{pos}')\n",
        "                rec = row.get(f'rec_{pos}') if pos <= TOP_N else None\n",
        "                hit = int(pd.notna(examiner) and pd.notna(rec) and examiner == rec)\n",
        "                recall_pos[pos].append(hit)\n",
        "\n",
        "        # Tambahkan recall ke DataFrame\n",
        "        for pos in [1, 2, 3]:\n",
        "            merged_df[f'recall_pos{pos}_ordered@{TOP_N}'] = recall_pos[pos]\n",
        "\n",
        "        recall_pos_mean = {pos: np.mean(recall_pos[pos]) for pos in [1, 2, 3]}\n",
        "\n",
        "        # Euclidean distance antar posisi (penalti posisi meleset)\n",
        "        distances = []\n",
        "        for _, row in merged_df.iterrows():\n",
        "            true_authors = [row.get(f'author{i}') for i in [1, 2, 3]]\n",
        "            pred_authors = [row.get(f'rec_{i}', None) for i in range(1, TOP_N + 1)]\n",
        "            distance = 0\n",
        "            max_penalty = TOP_N\n",
        "            for i, true_author in enumerate(true_authors):\n",
        "                if pd.isna(true_author) or true_author == '':\n",
        "                    continue\n",
        "                try:\n",
        "                    pred_pos = pred_authors.index(true_author)\n",
        "                    pos_diff = pred_pos - i\n",
        "                    distance += pos_diff ** 2\n",
        "                except ValueError:\n",
        "                    distance += max_penalty ** 2\n",
        "            distances.append(np.sqrt(distance))\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        norm_dists = scaler.fit_transform(np.array(distances).reshape(-1, 1)).flatten()\n",
        "        merged_df[f'norm_euclidean@{TOP_N}'] = norm_dists\n",
        "\n",
        "        # Ringkasan metrik\n",
        "        summary.append({\n",
        "            'Top-N': TOP_N,\n",
        "            'Mean_Recall_Existence': merged_df[f'recall_of_existence@{TOP_N}'].mean(),\n",
        "            'Recall_Pos_1_Ordered': recall_pos_mean[1],\n",
        "            'Recall_Pos_2_Ordered': recall_pos_mean[2],\n",
        "            'Recall_Pos_3_Ordered': recall_pos_mean[3],\n",
        "            'Avg_Normalized_Euclidean': np.mean(norm_dists)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(summary)\n",
        "\n",
        "result_df = evaluate_ordered_recommendation_cosine(final_df, true_label_df)\n",
        "print(result_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQIiCgD_X8am",
        "outputId": "d9cb2168-948e-4fd3-85fc-3fafe50e1c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Top-N  Mean_Recall_Existence  Recall_Pos_1_Ordered  Recall_Pos_2_Ordered  \\\n",
            "0      3               0.316901              0.246479              0.091549   \n",
            "1      5               0.455399              0.246479              0.091549   \n",
            "2      7               0.577465              0.246479              0.091549   \n",
            "3     10               0.708920              0.246479              0.091549   \n",
            "\n",
            "   Recall_Pos_3_Ordered  Avg_Normalized_Euclidean  \n",
            "0              0.077465                  0.769400  \n",
            "1              0.077465                  0.708720  \n",
            "2              0.077465                  0.638305  \n",
            "3              0.077465                  0.549607  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def evaluate_per_proposal_directed(rank_all_df, true_label_df, top_n=3):\n",
        "    # Filter dan urutkan\n",
        "    top_n_df = rank_all_df[rank_all_df[\"rank\"] <= top_n]\n",
        "    rec_df = top_n_df.sort_values(by=[\"id_proposal\", \"rank\", \"skor_rata2\"], ascending=[True, True, False])\n",
        "    rec_df = rec_df.drop_duplicates(subset=[\"id_proposal\", \"rank\",\"skor_rata2\"])\n",
        "    rec_pivot = rec_df.pivot(index=\"id_proposal\", columns=\"rank\", values=\"dosen\").reset_index()\n",
        "    rec_pivot.columns.name = None\n",
        "    rec_pivot.columns = [\"id_proposal\"] + [f\"rec_{i}\" for i in range(1, len(rec_pivot.columns))]\n",
        "\n",
        "    # Gabung dengan label kebenaran\n",
        "    merged_df = pd.merge(\n",
        "        rec_pivot,\n",
        "        true_label_df.rename(columns={\"proposal_id\": \"id_proposal\"}),\n",
        "        on=\"id_proposal\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    # Recall of existence (abaikan urutan)\n",
        "    def recall_of_existence(row):\n",
        "        true_set = {row.get(\"author1\"), row.get(\"author2\"), row.get(\"author3\")}\n",
        "        pred_set = {row.get(f\"rec_{i}\") for i in range(1, top_n + 1) if row.get(f\"rec_{i}\")}\n",
        "        return len(true_set.intersection(pred_set)) / 3\n",
        "\n",
        "    merged_df[f'recall_of_existence@{top_n}'] = merged_df.apply(recall_of_existence, axis=1)\n",
        "\n",
        "    # Recall berdasarkan posisi (urutan harus sama)\n",
        "    for pos in [1, 2, 3]:\n",
        "        merged_df[f'recall_pos{pos}_ordered@{top_n}'] = merged_df.apply(\n",
        "            lambda row: int(\n",
        "                pd.notna(row.get(f'author{pos}')) and\n",
        "                pd.notna(row.get(f'rec_{pos}')) and\n",
        "                row.get(f'author{pos}') == row.get(f'rec_{pos}')\n",
        "            ) if pos <= top_n else 0,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    # Euclidean distance penalti posisi\n",
        "    distances = []\n",
        "    for _, row in merged_df.iterrows():\n",
        "        true_authors = [row.get(f'author{i}') for i in [1, 2, 3]]\n",
        "        pred_authors = [row.get(f'rec_{i}', None) for i in range(1, top_n + 1)]\n",
        "        distance = 0\n",
        "        max_penalty = top_n\n",
        "        for i, true_author in enumerate(true_authors):\n",
        "            if pd.isna(true_author) or true_author == '':\n",
        "                continue\n",
        "            try:\n",
        "                pred_pos = pred_authors.index(true_author)\n",
        "                pos_diff = pred_pos - i\n",
        "                distance += pos_diff ** 2\n",
        "            except ValueError:\n",
        "                distance += max_penalty ** 2\n",
        "        distances.append(np.sqrt(distance))\n",
        "\n",
        "    # Normalisasi jarak\n",
        "    scaler = MinMaxScaler()\n",
        "    norm_dists = scaler.fit_transform(np.array(distances).reshape(-1, 1)).flatten()\n",
        "    merged_df[f'norm_euclidean@{top_n}'] = norm_dists\n",
        "\n",
        "    # Ambil kolom evaluasi\n",
        "    result_df = merged_df[[\"id_proposal\",\n",
        "                           f'recall_of_existence@{top_n}',\n",
        "                           f'recall_pos1_ordered@{top_n}',\n",
        "                           f'recall_pos2_ordered@{top_n}',\n",
        "                           f'recall_pos3_ordered@{top_n}',\n",
        "                           f'norm_euclidean@{top_n}']].copy()\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "eval_per_proposal_directed_3 = evaluate_per_proposal_directed(final_df, true_label_df, top_n=3)\n",
        "eval_per_proposal_directed_5 = evaluate_per_proposal_directed(final_df, true_label_df, top_n=5)\n",
        "eval_per_proposal_directed_7 = evaluate_per_proposal_directed(final_df, true_label_df, top_n=7)\n",
        "eval_per_proposal_directed_10 = evaluate_per_proposal_directed(final_df, true_label_df, top_n=10)\n",
        "\n",
        "eval_per_proposal_directed_3.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/directed/hasil_eval_3_kata.csv\", index=False)\n",
        "eval_per_proposal_directed_5.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/directed/hasil_eval_5_kata.csv\", index=False)\n",
        "eval_per_proposal_directed_7.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/directed/hasil_eval_7_kata.csv\", index=False)\n",
        "eval_per_proposal_directed_10.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/directed/hasil_eval_10_kata.csv\", index=False)"
      ],
      "metadata": {
        "id": "emD6Ny3gcKi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def recall_urutan_k_at_top_k(pred_df, true_label_df, posisi=1, k=3):\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "\n",
        "#     for _, row in true_label_df.iterrows():\n",
        "#         proposal_id = row[\"proposal_id\"]\n",
        "#         author_column = f\"author{posisi}\" if posisi > 0 else \"author\"\n",
        "#         true_author = row.get(author_column)\n",
        "\n",
        "#         if pd.isna(true_author):\n",
        "#             continue\n",
        "\n",
        "#         # Ambil top-k prediksi\n",
        "#         top_k_pred = (\n",
        "#             pred_df[pred_df[\"id_proposal\"] == proposal_id]\n",
        "#             .sort_values(by=\"skor_rata2\", ascending=False)\n",
        "#             .head(k)\n",
        "#         )\n",
        "\n",
        "#         # Pastikan posisi yang dimaksud ada dalam top-k\n",
        "#         if posisi <= len(top_k_pred):\n",
        "#             predicted_author = top_k_pred.iloc[posisi - 1][\"dosen\"]\n",
        "#             if predicted_author == true_author:\n",
        "#                 correct += 1\n",
        "#             total += 1  # Hanya dihitung jika posisi valid dalam top-k\n",
        "\n",
        "#     recall_at_k = correct / total if total > 0 else 0\n",
        "#     return recall_at_k\n",
        "\n",
        "# for k in [1,2, 3, 5, 7]:\n",
        "#     recall_rank1 = recall_urutan_k_at_top_k(final_df, true_label_df, posisi=1, k=k)\n",
        "#     recall_rank2 = recall_urutan_k_at_top_k(final_df, true_label_df, posisi=2, k=k)\n",
        "#     recall_rank3 = recall_urutan_k_at_top_k(final_df, true_label_df, posisi=3, k=k)\n",
        "\n",
        "#     print(f\"\\nRecall@{k}:\")\n",
        "#     print(f\"  Posisi 1 (author):  {recall_rank1:.4f}\")\n",
        "#     print(f\"  Posisi 2 (author2): {recall_rank2:.4f}\")\n",
        "#     print(f\"  Posisi 3 (author3): {recall_rank3:.4f}\")"
      ],
      "metadata": {
        "id": "Edtx4RxXTbVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# # Fungsi evaluasi rekomendasi dosen penguji\n",
        "# def evaluate_ordered_recommendation_word(df_peringkat, true_label_df, top_ns=[3, 5, 7, 10]):\n",
        "#     summary_word = []\n",
        "\n",
        "#     for TOP_N in top_ns:\n",
        "#         # Ambil Top-N dosen per proposal\n",
        "#         top_n_df = df_peringkat[df_peringkat[\"rank\"] <= TOP_N]\n",
        "\n",
        "#         # Ubah menjadi format pivot untuk gabung dengan true_label_df\n",
        "#         rec_df = top_n_df.sort_values(by=[\"id_proposal\", \"rank\", \"skor_rata2\"], ascending=[True, True, False])\n",
        "#         rec_df = rec_df.drop_duplicates(subset=[\"id_proposal\", \"rank\"])\n",
        "#         rec_pivot = rec_df.pivot(index=\"id_proposal\", columns=\"rank\", values=\"dosen\").reset_index()\n",
        "#         rec_pivot.columns.name = None\n",
        "#         rec_pivot.columns = [\"id_proposal\"] + [f\"rec_{i}\" for i in range(1, len(rec_pivot.columns))]\n",
        "\n",
        "#         # Gabungkan dengan label ground truth\n",
        "#         merged_df = pd.merge(\n",
        "#             rec_pivot,\n",
        "#             true_label_df.rename(columns={\"proposal_id\": \"id_proposal\"}),\n",
        "#             on=\"id_proposal\",\n",
        "#             how=\"left\"\n",
        "#         )\n",
        "\n",
        "#         # Recall keberadaan (tanpa urutan)\n",
        "#         def recall_of_existence(row):\n",
        "#             true_set = {row.get(\"author1\"), row.get(\"author2\"), row.get(\"author3\")}\n",
        "#             pred_set = {row.get(f\"rec_{i}\") for i in range(1, TOP_N+1) if row.get(f\"rec_{i}\")}\n",
        "#             return len(true_set.intersection(pred_set)) / 3\n",
        "\n",
        "#         merged_df[f'recall_of_existence@{TOP_N}'] = merged_df.apply(recall_of_existence, axis=1)\n",
        "\n",
        "#         # Recall posisi urut (ordered position recall)\n",
        "#         recall_pos = {1: [], 2: [], 3: []}\n",
        "#         for _, row in merged_df.iterrows():\n",
        "#             gt = [row.get(f'author{i}') for i in range(1, 4)]\n",
        "#             pred = [row.get(f'rec_{i}') for i in range(1, TOP_N + 1)]\n",
        "\n",
        "#             # Posisi kemunculan tiap GT di pred\n",
        "#             positions = {}\n",
        "#             for i, g in enumerate(gt):\n",
        "#                 try:\n",
        "#                     positions[i + 1] = pred.index(g)\n",
        "#                 except ValueError:\n",
        "#                     positions[i + 1] = None\n",
        "\n",
        "#             # Cek urutan valid (misalnya: pos(author1) < pos(author2) < pos(author3))\n",
        "#             valid_order = True\n",
        "#             for i in range(1, 3):\n",
        "#                 if positions.get(i) is not None and positions.get(i + 1) is not None:\n",
        "#                     if positions[i] >= positions[i + 1]:\n",
        "#                         valid_order = False\n",
        "#                         break\n",
        "\n",
        "#             for pos in [1, 2, 3]:\n",
        "#                 hit = int(positions.get(pos) is not None and valid_order)\n",
        "#                 recall_pos[pos].append(hit)\n",
        "\n",
        "#         for pos in [1, 2, 3]:\n",
        "#             merged_df[f'recall_pos{pos}_ordered@{TOP_N}'] = recall_pos[pos]\n",
        "\n",
        "#         recall_pos_mean = {\n",
        "#             pos: np.mean(recall_pos[pos]) if recall_pos[pos] else None\n",
        "#             for pos in [1, 2, 3]\n",
        "#         }\n",
        "\n",
        "#         # Euclidean distance antar posisi\n",
        "#         distances = []\n",
        "#         for _, row in merged_df.iterrows():\n",
        "#             true_authors = [row.get(f'author{i}') for i in [1, 2, 3]]\n",
        "#             pred_authors = [row.get(f'rec_{i}', None) for i in range(1, TOP_N + 1)]\n",
        "#             distance = 0\n",
        "#             max_penalty = TOP_N\n",
        "#             for i, true_author in enumerate(true_authors):\n",
        "#                 if pd.isna(true_author) or true_author == '':\n",
        "#                     continue\n",
        "#                 try:\n",
        "#                     pred_pos = pred_authors.index(true_author)\n",
        "#                     pos_diff = pred_pos - i\n",
        "#                     distance += pos_diff ** 2\n",
        "#                 except ValueError:\n",
        "#                     distance += max_penalty ** 2\n",
        "#             distances.append(np.sqrt(distance))\n",
        "\n",
        "#         scaler = MinMaxScaler()\n",
        "#         norm_dists = scaler.fit_transform(np.array(distances).reshape(-1, 1)).flatten()\n",
        "#         merged_df[f'norm_euclidean@{TOP_N}'] = norm_dists\n",
        "#         avg_dist = np.mean(norm_dists)\n",
        "#         avg_recall_exist = merged_df[f'recall_of_existence@{TOP_N}'].mean()\n",
        "\n",
        "#         summary_word.append({\n",
        "#             'Top-N': TOP_N,\n",
        "#             'Mean_Recall_Existence': avg_recall_exist,\n",
        "#             'Recall_Pos_1_Ordered': recall_pos_mean[1],\n",
        "#             'Recall_Pos_2_Ordered': recall_pos_mean[2],\n",
        "#             'Recall_Pos_3_Ordered': recall_pos_mean[3],\n",
        "#             'Avg_Normalized_Euclidean': avg_dist\n",
        "#         })\n",
        "\n",
        "#     return pd.DataFrame(summary_word)\n",
        "\n",
        "\n",
        "# result_eval_word = evaluate_ordered_recommendation_word(final_df, true_label_df)\n",
        "# print(result_eval_word)\n",
        "\n"
      ],
      "metadata": {
        "id": "LjEcpusvqOK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hasil per proposal"
      ],
      "metadata": {
        "id": "RHj6xty3aZ28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cosine"
      ],
      "metadata": {
        "id": "dZYcau9mSVeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Hitung similarity\n",
        "similarity_matrix = cosine_similarity(proposal_dense, expert_dense)\n",
        "\n",
        "# Konversi ke DataFrame\n",
        "similarity_df = pd.DataFrame(similarity_matrix)\n",
        "\n",
        "print(similarity_df)\n",
        "\n",
        "# Simpan ke CSV\n",
        "similarity_df.to_csv(\"/content/drive/MyDrive/Skripsi4/dictionary/similarity_MATRIX_kata_.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56339a6-eeb7-428e-e104-66938d24cd2b",
        "id": "EmUFEcqaSVeV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          0         1         2         3         4         5         6    \\\n",
            "0    0.026477  0.026477  0.000000  0.017549  0.017549  0.010021  0.010021   \n",
            "1    0.000000  0.000000  0.000000  0.036989  0.036989  0.027733  0.027733   \n",
            "2    0.007235  0.007235  0.050639  0.034361  0.034361  0.018101  0.018101   \n",
            "3    0.000000  0.000000  0.000000  0.038030  0.038030  0.017538  0.017538   \n",
            "4    0.101273  0.101273  0.008708  0.013364  0.013364  0.029803  0.029803   \n",
            "..        ...       ...       ...       ...       ...       ...       ...   \n",
            "137  0.033836  0.033836  0.000000  0.000000  0.000000  0.014103  0.014103   \n",
            "138  0.023580  0.023580  0.061345  0.031288  0.031288  0.014460  0.014460   \n",
            "139  0.057262  0.057262  0.000000  0.000000  0.000000  0.027204  0.027204   \n",
            "140  0.050860  0.050860  0.000000  0.000000  0.000000  0.074921  0.074921   \n",
            "141  0.062749  0.062749  0.004877  0.008516  0.008516  0.016263  0.016263   \n",
            "\n",
            "          7         8         9    ...       750       751       752  \\\n",
            "0    0.000000  0.000000  0.021520  ...  0.000000  0.028483  0.062481   \n",
            "1    0.023502  0.000000  0.014143  ...  0.062713  0.020473  0.028680   \n",
            "2    0.041957  0.010773  0.000000  ...  0.041245  0.073633  0.030611   \n",
            "3    0.000000  0.000000  0.000000  ...  0.031178  0.021049  0.000000   \n",
            "4    0.003996  0.005795  0.009106  ...  0.037863  0.065611  0.000000   \n",
            "..        ...       ...       ...  ...       ...       ...       ...   \n",
            "137  0.000000  0.000000  0.125494  ...  0.005580  0.068067  0.000000   \n",
            "138  0.013218  0.000000  0.017401  ...  0.010524  0.024193  0.000000   \n",
            "139  0.148466  0.000000  0.012674  ...  0.081058  0.003459  0.014479   \n",
            "140  0.000000  0.000000  0.031120  ...  0.090260  0.048482  0.015707   \n",
            "141  0.014400  0.014576  0.048593  ...  0.005212  0.000000  0.000000   \n",
            "\n",
            "          753       754       755       756       757       758       759  \n",
            "0    0.150696  0.150696  0.000000  0.000000  0.046972  0.000000  0.012703  \n",
            "1    0.087383  0.087383  0.027772  0.027772  0.000000  0.000000  0.000000  \n",
            "2    0.000000  0.000000  0.011303  0.011303  0.000000  0.014092  0.014064  \n",
            "3    0.023115  0.023115  0.054714  0.054714  0.131525  0.122783  0.000000  \n",
            "4    0.054400  0.054400  0.042068  0.042068  0.148452  0.175797  0.003783  \n",
            "..        ...       ...       ...       ...       ...       ...       ...  \n",
            "137  0.000000  0.000000  0.051406  0.051406  0.030146  0.013639  0.026680  \n",
            "138  0.000000  0.000000  0.066672  0.066672  0.018720  0.008469  0.020998  \n",
            "139  0.008628  0.008628  0.102498  0.102498  0.010249  0.000000  0.000000  \n",
            "140  0.004680  0.004680  0.040293  0.040293  0.005559  0.000000  0.000000  \n",
            "141  0.042200  0.042200  0.054199  0.054199  0.000000  0.007831  0.000000  \n",
            "\n",
            "[142 rows x 760 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter sesuai kondisi\n",
        "similarity_cosine_df = []\n",
        "\n",
        "for i, (_, mahasiswa) in enumerate(proposal_df.iterrows()):\n",
        "    for j, (_, dosen) in enumerate(expert_df.iterrows()):\n",
        "        # weight = dosen.get(\"author_weight\", 1.0)\n",
        "        # score_akhir = similarity_matrix[i, j] * weight\n",
        "        # score_akhir = similarity_matrix[i, j]\n",
        "        similarity_cosine_df.append({\n",
        "            \"id_proposal\": mahasiswa[\"proposal_id\"],\n",
        "            \"mahasiswa\": mahasiswa[\"student_id\"],\n",
        "            \"id_dosen\": dosen[\"expert_id\"],\n",
        "            \"id_penelitian\": dosen[\"research_id\"],\n",
        "            \"tahun_proposal\": mahasiswa[\"tahun\"],\n",
        "            \"tahun_penelitian\": dosen[\"pub_year\"],\n",
        "            \"selisih_tahun\": mahasiswa[\"tahun\"] - dosen[\"pub_year\"],\n",
        "            \"dosen\": dosen[\"name\"],\n",
        "           \"author_position\": dosen[\"author_position\"],\n",
        "            # \"weight\": weight,\n",
        "            \"similarity_score\" : similarity_matrix[j, i],\n",
        "            # \"similarity_score_akhir\": score_akhir,\n",
        "        })\n",
        "\n",
        "similarity_cosine_df = pd.DataFrame(similarity_cosine_df)\n",
        "\n",
        "\n",
        "similarity_cosine = similarity_cosine_df[\n",
        "    (similarity_cosine_df[\"tahun_proposal\"] > similarity_cosine_df[\"tahun_penelitian\"]) &\n",
        "    (similarity_cosine_df[\"selisih_tahun\"] > 0) &\n",
        "    (similarity_cosine_df[\"selisih_tahun\"] <= 5)\n",
        "].copy()\n",
        "\n",
        "\n",
        "# similarity_cosine.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/cosine/similarity_cosine_kata_baru_sbelumfilter.csv\", index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Olpur4CrU61h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "50c738d3-a00d-49c2-a86a-b6f8b0fc0e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 142 is out of bounds for axis 0 with size 142",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-3817509871>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m            \u001b[0;34m\"author_position\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdosen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"author_position\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# \"weight\": weight,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m\"similarity_score\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;31m# \"similarity_score_akhir\": score_akhir,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         })\n",
            "\u001b[0;31mIndexError\u001b[0m: index 142 is out of bounds for axis 0 with size 142"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ambil baris dengan similarity tertinggi untuk kombinasi unik id_proposal dan id_dosen\n",
        "idx = similarity_cosine.groupby([\"id_proposal\", \"id_dosen\"])[\"similarity_score\"].idxmax()\n",
        "similarity_cosine = similarity_cosine.loc[idx]\n",
        "\n",
        "# Ranking ulang berdasarkan proposal\n",
        "similarity_cosine[\"rank\"] = similarity_cosine.groupby(\"id_proposal\")[\"similarity_score\"] \\\n",
        "                                     .rank(method=\"first\", ascending=False).astype(int)\n",
        "\n",
        "# Urutkan\n",
        "similarity_cosine = similarity_cosine.sort_values([\"id_proposal\", \"rank\"])\n",
        "similarity_cosine = similarity_cosine[similarity_cosine[\"rank\"] <= 17]\n",
        "\n",
        "\n",
        "# Tampilkan\n",
        "print(similarity_cosine[[\"id_proposal\", \"mahasiswa\",\"tahun_proposal\",\"id_penelitian\", \"tahun_penelitian\", \"id_dosen\", \"dosen\",\"selisih_tahun\", \"similarity_score\",\"rank\"]])\n",
        "# similarity_cosine.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/cosine/similarity_cosine_kata_baru_setelah filter.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2854a0aa-e687-4161-9cd2-8fff52edfa9b",
        "id": "ecYp_9r1SVeW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id_proposal mahasiswa  tahun_proposal id_penelitian  tahun_penelitian  \\\n",
            "83             P1        S1            2021           R48              2018   \n",
            "84             P1        S1            2021           R48              2018   \n",
            "305            P1        S1            2021          R186              2020   \n",
            "434            P1        S1            2021          R264              2016   \n",
            "470            P1        S1            2021          R291              2017   \n",
            "...           ...       ...             ...           ...               ...   \n",
            "75004         P99       S99            2024          R335              2022   \n",
            "74980         P99       S99            2024          R316              2020   \n",
            "75082         P99       S99            2024          R377              2023   \n",
            "74888         P99       S99            2024          R247              2019   \n",
            "74574         P99       S99            2024           R55              2021   \n",
            "\n",
            "      id_dosen              dosen  selisih_tahun  similarity_score  rank  \n",
            "83          D1    Bambang Harjito              3          0.151904     1  \n",
            "84          D8       Esti Suryani              3          0.151904     2  \n",
            "305        D15      Heri Prasetyo              1          0.132554     3  \n",
            "434         D3        Umi Salamah              5          0.089575     4  \n",
            "470        D12            Wiharto              4          0.089078     5  \n",
            "...        ...                ...            ...               ...   ...  \n",
            "75004      D10     Afrizal Doewes              2          0.116629    12  \n",
            "74980       D2            Wiranto              4          0.109794    13  \n",
            "75082      D13  Hasan Dwi Cahyono              1          0.088952    14  \n",
            "74888       D6      Ristu Saptono              5          0.079144    15  \n",
            "74574       D5         Abdul Aziz              3          0.068880    16  \n",
            "\n",
            "[2248 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# 1. Inisialisasi count untuk rank 1\n",
        "rank1_count = defaultdict(int)\n",
        "final_assignments = []\n",
        "\n",
        "# 2. Tetapkan rank 1 dengan batas 15 kali per dosen\n",
        "for pid in similarity_cosine[\"id_proposal\"].unique():\n",
        "    candidates = similarity_cosine[similarity_cosine[\"id_proposal\"] == pid]\n",
        "    candidates = candidates.sort_values(by=\"similarity_score\", ascending=False)\n",
        "\n",
        "    assigned_rank1 = False\n",
        "    for _, row in candidates.iterrows():\n",
        "        dosen_id = row[\"id_dosen\"]\n",
        "        if rank1_count[dosen_id] < 15:\n",
        "            rank1_count[dosen_id] += 1\n",
        "            row_data = row.to_dict()\n",
        "            row_data[\"rank\"] = 1\n",
        "            row_data[\"beban\"] = rank1_count[dosen_id]\n",
        "            final_assignments.append(row_data)\n",
        "            assigned_rank1 = True\n",
        "            break\n",
        "\n",
        "    if not assigned_rank1:\n",
        "        row = candidates.iloc[0].to_dict()\n",
        "        dosen_id = row[\"id_dosen\"]\n",
        "        rank1_count[dosen_id] += 1\n",
        "        row[\"rank\"] = 1\n",
        "        row[\"beban\"] = rank1_count[dosen_id]\n",
        "        final_assignments.append(row)\n",
        "\n",
        "# 3. Buat dataframe dari rank 1\n",
        "rank1_df = pd.DataFrame(final_assignments)\n",
        "\n",
        "# 4. Tambahkan rank 2–17 berdasarkan similarity, excl. dosen yang sudah dipakai di rank 1 untuk proposal yang sama\n",
        "other_ranks = []\n",
        "\n",
        "for pid in similarity_cosine[\"id_proposal\"].unique():\n",
        "    # Dapatkan dosen yang sudah dipakai sebagai rank 1\n",
        "    used_dosen = rank1_df[rank1_df[\"id_proposal\"] == pid][\"id_dosen\"].tolist()\n",
        "\n",
        "    # Ambil kandidat lain untuk proposal ini\n",
        "    candidates = similarity_cosine[(similarity_cosine[\"id_proposal\"] == pid) & (~similarity_cosine[\"id_dosen\"].isin(used_dosen))]\n",
        "    candidates = candidates.sort_values(by=\"similarity_score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    for idx, (_, row) in enumerate(candidates.iterrows(), start=2):\n",
        "        if idx > 17:\n",
        "            break\n",
        "        row_data = row.to_dict()\n",
        "        row_data[\"rank\"] = idx\n",
        "        row_data[\"beban\"] = rank1_count[row_data[\"id_dosen\"]]  # Beban hanya dihitung dari rank 1\n",
        "        other_ranks.append(row_data)\n",
        "\n",
        "# 5. Gabungkan rank1 dan other ranks\n",
        "rank_df = pd.concat([rank1_df, pd.DataFrame(other_ranks)], ignore_index=True)\n",
        "rank_df = rank_df.sort_values(by=[\"id_proposal\", \"rank\"])\n",
        "\n",
        "# 🔟 Filter hanya Top 10 dosen per proposal\n",
        "rank_all_df = rank_df[rank_df[\"rank\"] <= 17]\n",
        "\n",
        "\n",
        "\n",
        "# 6. Tampilkan hasil\n",
        "print(rank_all_df[[\"id_proposal\", \"mahasiswa\", \"id_dosen\", \"dosen\", \"similarity_score\", \"rank\", \"beban\"]])\n",
        "# rank_all_df.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/cosine/beban_kata.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecd960d-2f87-4e99-bc19-f6176fd7fb6d",
        "id": "p9ZrO_tRSVeW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_proposal mahasiswa id_dosen              dosen  similarity_score  \\\n",
            "0             P1        S1       D1    Bambang Harjito          0.151904   \n",
            "142           P1        S1       D8       Esti Suryani          0.151904   \n",
            "143           P1        S1      D15      Heri Prasetyo          0.132554   \n",
            "144           P1        S1       D3        Umi Salamah          0.089575   \n",
            "145           P1        S1      D12            Wiharto          0.089078   \n",
            "...          ...       ...      ...                ...               ...   \n",
            "2243         P99       S99      D10     Afrizal Doewes          0.116629   \n",
            "2244         P99       S99       D2            Wiranto          0.109794   \n",
            "2245         P99       S99      D13  Hasan Dwi Cahyono          0.088952   \n",
            "2246         P99       S99       D6      Ristu Saptono          0.079144   \n",
            "2247         P99       S99       D5         Abdul Aziz          0.068880   \n",
            "\n",
            "      rank  beban  \n",
            "0        1      1  \n",
            "142      2      4  \n",
            "143      3     14  \n",
            "144      4      4  \n",
            "145      5     15  \n",
            "...    ...    ...  \n",
            "2243    12      9  \n",
            "2244    13      8  \n",
            "2245    14      4  \n",
            "2246    15      4  \n",
            "2247    16      8  \n",
            "\n",
            "[2248 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluasi Cosine"
      ],
      "metadata": {
        "id": "8w_Mwi6DDb3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "coba"
      ],
      "metadata": {
        "id": "BI86uhsrX4dO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluasi Baru"
      ],
      "metadata": {
        "id": "AURg-JbKTew-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Contoh struktur dummy data untuk demonstrasi perbaikan fungsi (tidak dijalankan secara nyata di sini)\n",
        "# similarity_cosine_df = pd.read_csv(...)  # Format: id_proposal, rank, similarity_score_akhir, dosen\n",
        "# true_label_df = pd.read_csv(...)  # Format: proposal_id, author1, author2, author3\n",
        "\n",
        "def evaluate_ordered_recommendation_cosine(rank_all_df, true_label_df, top_ns=[3, 5, 7, 10]):\n",
        "    summary = []\n",
        "\n",
        "    for TOP_N in top_ns:\n",
        "        # Filter Top-N dan urutkan\n",
        "        top_n_df = rank_all_df[rank_all_df[\"rank\"] <= TOP_N]\n",
        "        rec_df = top_n_df.sort_values(by=[\"id_proposal\", \"rank\", \"similarity_score\"], ascending=[True, True, False])\n",
        "        rec_pivot = rec_df.pivot(index=\"id_proposal\", columns=\"rank\", values=\"dosen\").reset_index()\n",
        "        rec_pivot.columns.name = None\n",
        "        rec_pivot.columns = [\"id_proposal\"] + [f\"rec_{i}\" for i in range(1, len(rec_pivot.columns))]\n",
        "\n",
        "        # Gabungkan dengan ground truth\n",
        "        merged_df = pd.merge(\n",
        "            rec_pivot,\n",
        "            true_label_df.rename(columns={\"proposal_id\": \"id_proposal\"}),\n",
        "            on=\"id_proposal\",\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        # Recall keberadaan (tidak memperhatikan urutan)\n",
        "        def recall_of_existence(row):\n",
        "            true_set = {row.get(\"author1\"), row.get(\"author2\"), row.get(\"author3\")}\n",
        "            pred_set = {row.get(f\"rec_{i}\") for i in range(1, TOP_N + 1) if row.get(f\"rec_{i}\")}\n",
        "            return len(true_set.intersection(pred_set)) / 3\n",
        "\n",
        "        merged_df[f'recall_of_existence@{TOP_N}'] = merged_df.apply(recall_of_existence, axis=1)\n",
        "\n",
        "        # Recall per posisi dengan urutan diperhatikan (rec_i harus sama dengan author_i)\n",
        "        recall_pos = {1: [], 2: [], 3: []}\n",
        "        for _, row in merged_df.iterrows():\n",
        "            for pos in [1, 2, 3]:\n",
        "                examiner = row.get(f'author{pos}')\n",
        "                rec = row.get(f'rec_{pos}') if pos <= TOP_N else None\n",
        "                hit = int(pd.notna(examiner) and pd.notna(rec) and examiner == rec)\n",
        "                recall_pos[pos].append(hit)\n",
        "\n",
        "        # Tambahkan recall ke DataFrame\n",
        "        for pos in [1, 2, 3]:\n",
        "            merged_df[f'recall_pos{pos}_ordered@{TOP_N}'] = recall_pos[pos]\n",
        "\n",
        "        recall_pos_mean = {pos: np.mean(recall_pos[pos]) for pos in [1, 2, 3]}\n",
        "\n",
        "        # Euclidean distance antar posisi (penalti posisi meleset)\n",
        "        distances = []\n",
        "        for _, row in merged_df.iterrows():\n",
        "            true_authors = [row.get(f'author{i}') for i in [1, 2, 3]]\n",
        "            pred_authors = [row.get(f'rec_{i}', None) for i in range(1, TOP_N + 1)]\n",
        "            distance = 0\n",
        "            max_penalty = TOP_N\n",
        "            for i, true_author in enumerate(true_authors):\n",
        "                if pd.isna(true_author) or true_author == '':\n",
        "                    continue\n",
        "                try:\n",
        "                    pred_pos = pred_authors.index(true_author)\n",
        "                    pos_diff = pred_pos - i\n",
        "                    distance += pos_diff ** 2\n",
        "                except ValueError:\n",
        "                    distance += max_penalty ** 2\n",
        "            distances.append(np.sqrt(distance))\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        norm_dists = scaler.fit_transform(np.array(distances).reshape(-1, 1)).flatten()\n",
        "        merged_df[f'norm_euclidean@{TOP_N}'] = norm_dists\n",
        "\n",
        "        # Ringkasan metrik\n",
        "        summary.append({\n",
        "            'Top-N': TOP_N,\n",
        "            'Mean_Recall_Existence': merged_df[f'recall_of_existence@{TOP_N}'].mean(),\n",
        "            'Recall_Pos_1_Ordered': recall_pos_mean[1],\n",
        "            'Recall_Pos_2_Ordered': recall_pos_mean[2],\n",
        "            'Recall_Pos_3_Ordered': recall_pos_mean[3],\n",
        "            'Avg_Normalized_Euclidean': np.mean(norm_dists)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(summary)\n",
        "\n",
        "result_df = evaluate_ordered_recommendation_cosine(rank_all_df, true_label_df)\n",
        "print(result_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKx0VSImXucE",
        "outputId": "e58ecad4-c061-411d-a579-8aabc149b7ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Top-N  Mean_Recall_Existence  Recall_Pos_1_Ordered  Recall_Pos_2_Ordered  \\\n",
            "0      3               0.269953               0.15493              0.084507   \n",
            "1      5               0.401408               0.15493              0.084507   \n",
            "2      7               0.542254               0.15493              0.084507   \n",
            "3     10               0.697183               0.15493              0.084507   \n",
            "\n",
            "   Recall_Pos_3_Ordered  Avg_Normalized_Euclidean  \n",
            "0              0.077465                  0.860955  \n",
            "1              0.077465                  0.775638  \n",
            "2              0.077465                  0.700958  \n",
            "3              0.077465                  0.601830  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def evaluate_per_proposal_cosine(rank_all_df, true_label_df, top_n=3):\n",
        "    # Filter dan urutkan\n",
        "    top_n_df = rank_all_df[rank_all_df[\"rank\"] <= top_n]\n",
        "    rec_df = top_n_df.sort_values(by=[\"id_proposal\", \"rank\", \"similarity_score\"], ascending=[True, True, False])\n",
        "    rec_pivot = rec_df.pivot(index=\"id_proposal\", columns=\"rank\", values=\"dosen\").reset_index()\n",
        "    rec_pivot.columns.name = None\n",
        "    rec_pivot.columns = [\"id_proposal\"] + [f\"rec_{i}\" for i in range(1, len(rec_pivot.columns))]\n",
        "\n",
        "    # Gabung dengan label kebenaran\n",
        "    merged_df = pd.merge(\n",
        "        rec_pivot,\n",
        "        true_label_df.rename(columns={\"proposal_id\": \"id_proposal\"}),\n",
        "        on=\"id_proposal\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    # Recall of existence (abaikan urutan)\n",
        "    def recall_of_existence(row):\n",
        "        true_set = {row.get(\"author1\"), row.get(\"author2\"), row.get(\"author3\")}\n",
        "        pred_set = {row.get(f\"rec_{i}\") for i in range(1, top_n + 1) if row.get(f\"rec_{i}\")}\n",
        "        return len(true_set.intersection(pred_set)) / 3\n",
        "\n",
        "    merged_df[f'recall_of_existence@{top_n}'] = merged_df.apply(recall_of_existence, axis=1)\n",
        "\n",
        "    # Recall berdasarkan posisi (urutan harus sama)\n",
        "    for pos in [1, 2, 3]:\n",
        "        merged_df[f'recall_pos{pos}_ordered@{top_n}'] = merged_df.apply(\n",
        "            lambda row: int(\n",
        "                pd.notna(row.get(f'author{pos}')) and\n",
        "                pd.notna(row.get(f'rec_{pos}')) and\n",
        "                row.get(f'author{pos}') == row.get(f'rec_{pos}')\n",
        "            ) if pos <= top_n else 0,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    # Euclidean distance penalti posisi\n",
        "    distances = []\n",
        "    for _, row in merged_df.iterrows():\n",
        "        true_authors = [row.get(f'author{i}') for i in [1, 2, 3]]\n",
        "        pred_authors = [row.get(f'rec_{i}', None) for i in range(1, top_n + 1)]\n",
        "        distance = 0\n",
        "        max_penalty = top_n\n",
        "        for i, true_author in enumerate(true_authors):\n",
        "            if pd.isna(true_author) or true_author == '':\n",
        "                continue\n",
        "            try:\n",
        "                pred_pos = pred_authors.index(true_author)\n",
        "                pos_diff = pred_pos - i\n",
        "                distance += pos_diff ** 2\n",
        "            except ValueError:\n",
        "                distance += max_penalty ** 2\n",
        "        distances.append(np.sqrt(distance))\n",
        "\n",
        "    # Normalisasi jarak\n",
        "    scaler = MinMaxScaler()\n",
        "    norm_dists = scaler.fit_transform(np.array(distances).reshape(-1, 1)).flatten()\n",
        "    merged_df[f'norm_euclidean@{top_n}'] = norm_dists\n",
        "\n",
        "    # Ambil kolom evaluasi\n",
        "    result_df = merged_df[[\"id_proposal\",\n",
        "                           f'recall_of_existence@{top_n}',\n",
        "                           f'recall_pos1_ordered@{top_n}',\n",
        "                           f'recall_pos2_ordered@{top_n}',\n",
        "                           f'recall_pos3_ordered@{top_n}',\n",
        "                           f'norm_euclidean@{top_n}']].copy()\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "eval_per_proposal_cosine_3 = evaluate_per_proposal_cosine(rank_all_df, true_label_df, top_n=3)\n",
        "eval_per_proposal_cosine_5 = evaluate_per_proposal_cosine(rank_all_df, true_label_df, top_n=5)\n",
        "eval_per_proposal_cosine_7 = evaluate_per_proposal_cosine(rank_all_df, true_label_df, top_n=7)\n",
        "eval_per_proposal_cosine_10 = evaluate_per_proposal_cosine(rank_all_df, true_label_df, top_n=10)\n",
        "\n",
        "# eval_per_proposal_cosine_3.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/cosine/hasil_eval_3_kata.csv\", index=False)\n",
        "# eval_per_proposal_cosine_5.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/cosine/hasil_eval_5_kata.csv\", index=False)\n",
        "# eval_per_proposal_cosine_7.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/cosine/hasil_eval_7_kata.csv\", index=False)\n",
        "# eval_per_proposal_cosine_10.to_csv(\"/content/drive/MyDrive/Skripsi4/kata/cosine/hasil_eval_10_kata.csv\", index=False)"
      ],
      "metadata": {
        "id": "h9feziiLcYP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluasi per proposal"
      ],
      "metadata": {
        "id": "W57bxnMAcblg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6m3vnW3Xu6Ra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}